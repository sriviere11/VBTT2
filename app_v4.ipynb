{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#functions to deal with creating and extracting features\n",
    "\n",
    "#import bamboolib as bam\n",
    "\n",
    "# import bamboolib as bam\n",
    "# import bamboolib as bam\n",
    "from datetime import datetime, timedelta\n",
    "#from VBTT2_IO.IO import  read_config_file\n",
    "\n",
    "# import bamboolib as bam\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from yahoo_fin import stock_info as si\n",
    "\n",
    "\n",
    "def get_yf_dataframe(data, nbdays):\n",
    "    yesterday = datetime.now() - timedelta(1)  # we want data up to yesterday\n",
    "    start_date = yesterday - timedelta(days=nbdays)  # we run the model using data for nbdays\n",
    "    df_res = pd.DataFrame()\n",
    "    print(\"data in module get_yf_dataframe\", data)\n",
    "    print(\"nbdays in  in module get_yf_dataframe\", nbdays)\n",
    "    for ticker in data:\n",
    "        df_tmp = si.get_data(ticker, start_date, yesterday)\n",
    "        df_res[ticker] = df_tmp['close']\n",
    "    return df_res\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(ticker, additional_data, days):\n",
    "    #this function return a matrix of features augmented with fix data for number of days\n",
    "    print (\"preprocing module ticker\",ticker)\n",
    "    print(\"preprocessing module additional data\",additional_data)\n",
    "    print(\"preprocessing module days\", days)\n",
    "    tickers_in_sector_extended = np.concatenate((ticker, additional_data), axis=None)\n",
    "    tickers_in_sector_extended = tickers_in_sector_extended.tolist()\n",
    "    matrix_features_sector = get_yf_dataframe(tickers_in_sector_extended, days)\n",
    "    # import pandas as pd; import numpy as np\n",
    "    # matrix_features_sector = matrix_features_sector.reset_index()\n",
    "    matrix_features_sector = matrix_features_sector.reindex(sorted(matrix_features_sector.columns), axis=1)\n",
    "\n",
    "    ###################################\n",
    "    ##### saving and reading features - can help increase processing time\n",
    "    ##### to evaluate on future version\n",
    "    ####################################\n",
    "    # matrix_features_sector.to_csv(\"matrix_features_sector.csv\")\n",
    "    # matrix_features_sector=pd.read_csv(\"matrix_features_sector.csv\")\n",
    "\n",
    "    return matrix_features_sector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_train_test_set(ticker, features, lags,additional_data,days,nb_predict_days):\n",
    "    # creating train and test set\n",
    "    yesterday, start_date, train_date_start, train_date_last, test_date_start, test_date_last, days = initialize_data(\n",
    "        days,\n",
    "        lags,\n",
    "        nb_predict_days)\n",
    "\n",
    "    df = features[[ticker] + additional_data]\n",
    "    df_lagged = df.copy()\n",
    "    for window in range(1, lags + 1):\n",
    "        shifted = df.shift(window)\n",
    "        shifted.columns = [x + \"_lag\" + str(window) for x in df.columns]\n",
    "\n",
    "        df_lagged = pd.concat((df_lagged, shifted), axis=1)\n",
    "    #df_lagged = df_lagged.fillna(method='ffill')\n",
    "    df_lagged= df_lagged.interpolate(method='linear')\n",
    "    df_lagged = df_lagged.dropna()\n",
    "    #df_lagged = df_lagged.reindex(sorted(df_lagged.columns), axis=1)\n",
    "\n",
    "    # print(df_lagged)\n",
    "    # df_lagged[ticker+\"_2labels\"]=np.floor(df_lagged[ticker]/df_lagged[ticker+\"_lag1\"]).astype(int)\n",
    "\n",
    "    # train_set\n",
    "    df_filtered = df_lagged.loc[:train_date_last]\n",
    "    # X_train=df_filtered.drop(columns=[ticker, ticker+\"_2labels\"])\n",
    "    X_train = df_filtered.drop(columns=[ticker])\n",
    "    # y_train=df_filtered[ticker+\"_2labels\"]\n",
    "    y_train = df_filtered[ticker]\n",
    "\n",
    "    # test set\n",
    "    df_filtered = df_lagged.loc[test_date_start:test_date_last]\n",
    "    # X_test=df_filtered.drop(columns=[ticker, ticker+\"_2labels\"])\n",
    "    X_test = df_filtered.drop(columns=[ticker])\n",
    "    # y_test=df_filtered[ticker+\"_2labels\"]\n",
    "    y_test = df_filtered[ticker]\n",
    "\n",
    "    # we convert to numpy array\n",
    "    X_train = X_train.to_numpy()\n",
    "    y_train = y_train.to_numpy()\n",
    "    X_test = X_test.to_numpy()\n",
    "    y_test = y_test.to_numpy()\n",
    "    return X_train, y_train, X_test, y_test, df_filtered\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "def initialize_data(days, lags, nb_predict_days):\n",
    "\n",
    "    from datetime import datetime, timedelta\n",
    "\n",
    "\n",
    "\n",
    "    # define main date in models for defining training and test sets dates.\n",
    "    #yesterday = datetime.now() - timedelta(1)\n",
    "    yesterday=datetime.now()\n",
    "    start_date = yesterday - timedelta(days=days)\n",
    "    train_date_start = start_date.strftime(\"%Y-%m-%d\")\n",
    "    train_date_last = yesterday - timedelta(days=nb_predict_days + 1)  # nombre de jours a predire\n",
    "    train_date_last = train_date_last.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "    test_date_start = yesterday - timedelta(days=nb_predict_days)\n",
    "    test_date_start = test_date_start.strftime(\"%Y-%m-%d\")\n",
    "    test_date_last = yesterday.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "    return yesterday, start_date, train_date_start, train_date_last, test_date_start, test_date_last,days\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def create_predict_set(ticker ,features ,lags, nb_predict_days, additional_data):\n",
    "\n",
    "    yesterday, start_date, train_date_start, train_date_last, test_date_start, test_date_last, days = initialize_data(\n",
    "        lags * 2, lags, nb_predict_days)\n",
    "    #  List of features X_train, y_train, X_test,y_test\n",
    "    print(\"features shape in module create_predict_set\")\n",
    "    print(features.shape)\n",
    "\n",
    "    df =features[[ticker ] +additional_data]\n",
    "    print('df=features+additional data in module create_predict_set')\n",
    "    print(df.shape)\n",
    "    \n",
    "\n",
    "    df_lagged =df.copy()\n",
    "    for window in range(1, lags + 1):\n",
    "        shifted = df.shift(window)\n",
    "        shifted.columns = [x + \"_lag\" + str(window) for x in df.columns]\n",
    "\n",
    "        df_lagged = pd.concat((df_lagged, shifted), axis=1)\n",
    "    print('df_lagged of features+additional data before dropna in module create_predict_set')\n",
    "    print(df_lagged.shape)\n",
    "    print(df_lagged)\n",
    "    #df_lagged = df_lagged.fillna(method='ffill')\n",
    "    df_lagged= df_lagged.interpolate(method='linear')\n",
    "    df_lagged.to_csv('df_lagged3.csv')\n",
    "    df_lagged = df_lagged.dropna()\n",
    "    print('df_lagged of features+additional data after dropna and before sort in module create_predict_set')\n",
    "    print(df_lagged.shape)\n",
    "    df_lagged =df_lagged.reindex(sorted(df_lagged.columns), axis=1)\n",
    "    print('df_lagged of features+additional data after dropna in module create_predict_set')\n",
    "    print(df_lagged.shape)\n",
    "    print(df_lagged)\n",
    "    # test set\n",
    "    df_filtered = df_lagged.loc[test_date_start:test_date_last]\n",
    "    \n",
    "    print('df_filtered which is only start to last date of predict  in module create_predict_set')\n",
    "    print(df_filtered.shape)\n",
    "    \n",
    "    X_test =df_filtered.drop(columns=[ticker])\n",
    "    y_test =df_filtered[ticker]\n",
    "\n",
    "    # we convert to numpy array\n",
    "    X_test =X_test.to_numpy()\n",
    "    y_test= y_test.to_numpy()\n",
    "    return X_test ,y_test ,df_filtered\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import bamboolib as bam\n",
    "import os\n",
    "import os.path\n",
    "\n",
    "import numpy as np\n",
    "from yahoo_fin import stock_info as si\n",
    "\n",
    "#from VBTT2_IO.IO import write_list, read_list,delete_then_get_model_from_bucket,read_config_file\n",
    "import time\n",
    "\n",
    "\n",
    "\n",
    "#functions to fetch and process SP500 data\n",
    "\n",
    "def read_create_write_SP500(SP500_tickers,filename_json):\n",
    "    # This create the complete master list of all tickers in SP500, their sectors, their industries\n",
    "    # Check if SP500 json file exist first otherwise process SP500\n",
    "\n",
    "    check=delete_then_get_model_from_bucket(filename_json) #delete local file and copy file from bucket to have fresh one\n",
    "    #file_exists = os.path.exists(filename_json) #no longer required with blob check above\n",
    "\n",
    "    if check==True:\n",
    "        SP500_list = read_list(filename_json)\n",
    "        SP500_list = np.array(SP500_list)  # we need an array\n",
    "    else:\n",
    "        SP500_list = read_write_SP500(SP500_tickers,filename_json)  # this will extract\n",
    "\n",
    "    return SP500_list\n",
    "\n",
    "\n",
    "def get_ticker_sector(ticker):\n",
    "    # version 2\n",
    "    # prerequisites is to import Yahoo_finance.stock_info\n",
    "    # probleme avec cette methode c'est que industry est ligne 18 ou 19 *\n",
    "    # prerequisites 2 is to import pandas as pd; import numpy as np\n",
    "    filename_json=\"SP500.json\"\n",
    "    file_exists = os.path.exists(filename_json)\n",
    "    if file_exists:  # file json exist\n",
    "        SP500_list = read_list(filename_json)\n",
    "        SP500_list = np.array(SP500_list)  # we need an array\n",
    "        #now we should extract the sector which is column 2 for in each item of this 2D array\n",
    "        ticker_sector=SP500_list[:, 1:2][SP500_list[0:, 0:1] == ticker].tolist()\n",
    "\n",
    "        return ticker_sector[0]\n",
    "    else:\n",
    "        df = si.get_company_info(ticker)  # a utiliser pour trouver le secteur\n",
    "        df = df.reset_index()\n",
    "        sector = df.loc[df['Breakdown'].isin(['sector'])]  # from bamboolib to extract sector\n",
    "        sector = sector.iloc[0, 1]  # this is to extract just the value\n",
    "        industry = df.loc[df['Breakdown'].isin(['industry'])]\n",
    "        industry = industry.iloc[0, 1]\n",
    "        ticker_sector = []\n",
    "        ticker_sector.append([ticker, sector, industry])\n",
    "        return ticker_sector[0]\n",
    "\n",
    "\n",
    "def read_write_SP500(tickers_list,filename_json):\n",
    "    # Initialisation of SP500 data - find sector, industry for all tickers in SP500\n",
    "\n",
    "    SP500_list = []\n",
    "    for ticker in tickers_list:\n",
    "        print(\"ticker in module read_write_SP500\",ticker)\n",
    "        SP500_list.append(get_ticker_sector(ticker))\n",
    "        if len(SP500_list) % 30 == 0:\n",
    "            time.sleep(30)\n",
    "            # for some reason processing SP500 one shot is failing. so sleep of 20 seconds for each 50 tickers\n",
    "    SP500_list = np.array(SP500_list)\n",
    "    # Save in a file to reduce processing time next time\n",
    "    write_list(SP500_list.tolist(), filename_json)  # this function work if it is a list\n",
    "    return SP500_list  # this returns the array, not the list\n",
    "\n",
    "\n",
    "def get_all_tickers_sector(sector):\n",
    "    # prerequisites list is an array of ticker, sector and industry\n",
    "    filename_json = \"SP500.json\"\n",
    "    SP500_list = read_list(filename_json)\n",
    "    SP500_list = np.array(SP500_list)  # we need an array\n",
    "\n",
    "\n",
    "    sub_list_tickers = np.array(SP500_list)\n",
    "    fltr = np.asarray([sector])\n",
    "    result = sub_list_tickers[np.in1d(sub_list_tickers[:, 1], fltr)]\n",
    "    return result[:, 0:1]\n",
    "    # on veut extraire les tickers donc toutes les rangees (0:0) et la colonne 0 donc (0:1)\n",
    "\n",
    "\n",
    "def get_all_tickers_industry(industry):\n",
    "    # prerequisites list is an array of ticker, sector and industry\n",
    "    filename_json = \"SP500.json\"\n",
    "    SP500_list = read_list(filename_json)\n",
    "    SP500_list = np.array(SP500_list)  # we need an array\n",
    "\n",
    "    sub_list_tickers = np.array(SP500_list)\n",
    "    fltr = np.asarray([industry])\n",
    "    result = sub_list_tickers[np.in1d(sub_list_tickers[:, 2], fltr)]\n",
    "    return result[:, 0:1]\n",
    "    # on veut extraire les tickers donc toutes les rangees (0:0) et la colonne 0 donc (0:1)\n",
    "\n",
    "\n",
    "def generate_enhanced_data(sector,ticker):\n",
    "    additional_data = read_config_file()[1]\n",
    "    sector_list=get_all_tickers_sector(sector)\n",
    "    additional_data = np.concatenate((sector_list, additional_data), axis=None)\n",
    "    additional_data=additional_data.tolist()\n",
    "    additional_data.remove(ticker)\n",
    "    return additional_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a', 'b', 'c', 'c', 'b', '1', '2']"
      ]
     },
     "execution_count": 350,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "test1=['a','b','c','c','b']\n",
    "additional_data=['1','2','2']\n",
    "additional_data=np.concatenate((test1, additional_data), axis=None)\n",
    "additional_data=additional_data.tolist()\n",
    "additional_data.remove('2')\n",
    "additional_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a', 'b', 'c', 'c', 'b', '1', '2']"
      ]
     },
     "execution_count": 351,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "additional_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: google.api.core in /home/steve/.local/lib/python3.10/site-packages (2.10.2)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.56.2 in /home/steve/.local/lib/python3.10/site-packages (from google.api.core) (1.56.4)\n",
      "Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5 in /home/steve/.local/lib/python3.10/site-packages (from google.api.core) (4.21.9)\n",
      "Requirement already satisfied: google-auth<3.0dev,>=1.25.0 in /home/steve/.local/lib/python3.10/site-packages (from google.api.core) (2.14.0)\n",
      "Requirement already satisfied: requests<3.0.0dev,>=2.18.0 in /usr/lib/python3/dist-packages (from google.api.core) (2.25.1)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /home/steve/.local/lib/python3.10/site-packages (from google-auth<3.0dev,>=1.25.0->google.api.core) (0.2.8)\n",
      "Requirement already satisfied: six>=1.9.0 in /usr/lib/python3/dist-packages (from google-auth<3.0dev,>=1.25.0->google.api.core) (1.16.0)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /home/steve/.local/lib/python3.10/site-packages (from google-auth<3.0dev,>=1.25.0->google.api.core) (5.2.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /home/steve/.local/lib/python3.10/site-packages (from google-auth<3.0dev,>=1.25.0->google.api.core) (4.9)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /home/steve/.local/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth<3.0dev,>=1.25.0->google.api.core) (0.4.8)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install google.api.core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: google-cloud-storage in /home/steve/.local/lib/python3.10/site-packages (2.5.0)\n",
      "Requirement already satisfied: google-auth<3.0dev,>=1.25.0 in /home/steve/.local/lib/python3.10/site-packages (from google-cloud-storage) (2.14.0)\n",
      "Requirement already satisfied: google-cloud-core<3.0dev,>=2.3.0 in /home/steve/.local/lib/python3.10/site-packages (from google-cloud-storage) (2.3.2)\n",
      "Requirement already satisfied: requests<3.0.0dev,>=2.18.0 in /usr/lib/python3/dist-packages (from google-cloud-storage) (2.25.1)\n",
      "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5 in /home/steve/.local/lib/python3.10/site-packages (from google-cloud-storage) (2.10.2)\n",
      "Requirement already satisfied: google-resumable-media>=2.3.2 in /home/steve/.local/lib/python3.10/site-packages (from google-cloud-storage) (2.4.0)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.56.2 in /home/steve/.local/lib/python3.10/site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-storage) (1.56.4)\n",
      "Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5 in /home/steve/.local/lib/python3.10/site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-storage) (4.21.9)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /home/steve/.local/lib/python3.10/site-packages (from google-auth<3.0dev,>=1.25.0->google-cloud-storage) (0.2.8)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /home/steve/.local/lib/python3.10/site-packages (from google-auth<3.0dev,>=1.25.0->google-cloud-storage) (5.2.0)\n",
      "Requirement already satisfied: six>=1.9.0 in /usr/lib/python3/dist-packages (from google-auth<3.0dev,>=1.25.0->google-cloud-storage) (1.16.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /home/steve/.local/lib/python3.10/site-packages (from google-auth<3.0dev,>=1.25.0->google-cloud-storage) (4.9)\n",
      "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /home/steve/.local/lib/python3.10/site-packages (from google-resumable-media>=2.3.2->google-cloud-storage) (1.5.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /home/steve/.local/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth<3.0dev,>=1.25.0->google-cloud-storage) (0.4.8)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install google-cloud-storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.api_core.exceptions import NotFound"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from google.cloud import storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import bamboolib as bam\n",
    "\n",
    "# Input Output functions to save and read files\n",
    "\n",
    "#import bamboolib as bam\n",
    "import json\n",
    "import configparser\n",
    "import logging\n",
    "import joblib\n",
    "\n",
    "import os\n",
    "import os.path\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_list(a_list, filename_json):\n",
    "    print(\"Started writing list data into a json file\")\n",
    "    with open(filename_json, \"w\") as fp:\n",
    "        json.dump(a_list, fp)\n",
    "        print(\"Done writing data into .json file\")\n",
    "    upload_file_to_bucket(filename_json)\n",
    "\n",
    "\n",
    "# Read list to memory\n",
    "def read_list(filename_json):\n",
    "    # for reading also binary mode is important\n",
    "    with open(filename_json, 'rb') as fp:\n",
    "        n_list = json.load(fp)\n",
    "        return n_list\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def read_config_file():\n",
    "    # return element that is in configfile. exemple additional data\n",
    "    config = configparser.ConfigParser()\n",
    "    config.read('config.ini')\n",
    "    version = config['DEFAULT']['version']\n",
    "    additional_data = config['DEFAULT']['additional_data'].split(',')\n",
    "    regressor = config['DEFAULT']['regressor']\n",
    "    model = config['DEFAULT']['model']\n",
    "    root_bucket=config['DEFAULT']['gcloud_root_bucket']\n",
    "\n",
    "    # ^TNX reasury yield is the annual return investors can expect from holding a U.S. government security with a given\n",
    "    # ^GSPC tracks the performance of the stocks of 500 large-cap companies in the US\"\n",
    "    # CL=F crude oil pricesi.get_data(result[0][0])\n",
    "\n",
    "    return version, additional_data,regressor, model, root_bucket\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#file existimport logging\n",
    "\n",
    "\n",
    "def create_bucket(bucket_name):\n",
    "    log = logging.getLogger()\n",
    "\n",
    "    storage_client = storage.Client()\n",
    "    if bucket_name not in [x.name for x in storage_client.list_buckets()]:\n",
    "        bucket = storage_client.create_bucket(bucket_name)\n",
    "\n",
    "        log.info(\"Bucket {} created\".format(bucket.name))\n",
    "    else:\n",
    "        log.info(\"Bucket {} already exists\".format(bucket_name))\n",
    "\n",
    "\n",
    "\n",
    "def get_bucket_name():\n",
    "    root_bucket = read_config_file()[4]\n",
    "    version = read_config_file()[0]\n",
    "    bucket_name=f'{root_bucket}_{version.replace(\".\", \"\")}'\n",
    "    create_bucket(bucket_name)\n",
    "    return bucket_name\n",
    "\n",
    "\n",
    "def upload_file_to_bucket(model_file_name):\n",
    "    bucket_name=get_bucket_name()\n",
    "    log = logging.getLogger()\n",
    "    log.warning(f'uploading {model_file_name} to {bucket_name}')\n",
    "    storage_client = storage.Client()\n",
    "    bucket=storage_client.bucket(bucket_name)\n",
    "    blob = bucket.blob(model_file_name)\n",
    "    blob.upload_from_filename(model_file_name)\n",
    "\n",
    "\n",
    "def delete_blob(blob_name):\n",
    "    log = logging.getLogger()\n",
    "    bucket_name = get_bucket_name()\n",
    "    storage_client = storage.Client()\n",
    "    bucket = storage_client.bucket(bucket_name)\n",
    "    blob = bucket.blob(blob_name)\n",
    "    blob.delete()\n",
    "    log.warning(f'old blob {blob_name} deleted from {bucket_name}\\n')\n",
    "\n",
    "\n",
    "def delete_then_get_model_from_bucket(model_filename):\n",
    "    log = logging.getLogger()\n",
    "    if (os.path.exists(model_filename)):\n",
    "        os.remove(model_filename)\n",
    "        log.warning(f'old file {model_filename} deleted locally\\n')\n",
    "\n",
    "    bucket_name = get_bucket_name()\n",
    "    storage_client = storage.Client()\n",
    "    bucket = storage_client.bucket(bucket_name)\n",
    "    blob=bucket.blob(model_filename)\n",
    "\n",
    "    try:\n",
    "        blob.download_to_filename(model_filename)\n",
    "        log.warning(f'new file   {model_filename} downloaded\\n')\n",
    "        return True #file download ok therefore retrain not required  or fileexists locally\n",
    "\n",
    "    except NotFound as e:\n",
    "        log.warning(f'file {model_filename} not found in bucket\\n')\n",
    "        return False  #retrain required or file does not exist locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "from yahoo_fin import stock_info as si\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Model_Train_Save(ticker_list_for_models, years, lags, additional_data, nb_predict_days):\n",
    "    # Functions for model training and algorythm data and import\n",
    "\n",
    "    # import the regressors\n",
    "    version, additional_data, regressor, MODEL,bucket=read_config_file()\n",
    "   \n",
    "\n",
    "    if MODEL=='DecisionTree()':\n",
    "        from sklearn.tree import DecisionTreeRegressor\n",
    "        MODEL = DecisionTreeRegressor()\n",
    "    elif MODEL=='LinearRegression()':\n",
    "        from sklearn.linear_model import LinearRegression\n",
    "        MODEL = LinearRegression()\n",
    "    elif MODEL=='svm.SVR()':\n",
    "        from sklearn import svm\n",
    "        MODEL = svm.SVR()\n",
    "    elif MODEL==\"DecisionTree(max_depth=5)\":\n",
    "        from sklearn.tree import DecisionTreeRegressor\n",
    "        MODEL = DecisionTreeRegressor(max_depth=5)\n",
    "    elif MODEL==\"Ridge(alpha=1.0)\":\n",
    "        from sklearn.linear_model import Ridge\n",
    "        MODEL=Ridge(alpha=1.0)\n",
    "    elif MODEL==\"Lasso(alpha=1.0)\":\n",
    "        from sklearn.linear_model import Lasso\n",
    "        MODEL=Lasso(alpha=1.0)\n",
    "    elif MODEL==\"xgboost\":\n",
    "        import xgboost as xgb\n",
    "        MODEL= xgb.XGBClassifier()\n",
    "    elif MODEL==\"RandomForestRegressor(n_estimators=100)\":   \n",
    "        from sklearn.ensemble import RandomForestRegressor\n",
    "        MODEL=RandomForestRegressor(n_estimators=100)\n",
    "    else:\n",
    "        #MODEL=='LinearRegression'\n",
    "        from sklearn.linear_model import LinearRegression\n",
    "        MODEL = LinearRegression()\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    #### Initialisation of variables and data\n",
    "    # the timeframe for training and test sets and predict\n",
    "    days = 360 * years  # Nunber of days in the model\n",
    "    yesterday, start_date, train_date_start, train_date_last, test_date_start, test_date_last, days = initialize_data(\n",
    "        days,\n",
    "        lags,\n",
    "        nb_predict_days)\n",
    "    SP500_tickers = si.tickers_sp500()  # get list of tickers\n",
    "\n",
    "    # print(f\"SP500_tickers = {SP500_tickers}\")\n",
    "\n",
    "\n",
    "\n",
    "    # read master list of ticker, sector, industries or if not found, create it and save it#\n",
    "    SP500_list = read_create_write_SP500(SP500_tickers, \"SP500.json\")\n",
    "\n",
    "    ### validation if we have everything\n",
    "    # print(f\"Input ->years {years}\")\n",
    "    # print(f\"Input ->ticker list {ticker_list_for_models}\")\n",
    "    # print(f\"Input->lags {lags}\")\n",
    "    # print(f\"Input ->predict days {nb_predict_days}\")\n",
    "    # print(f\"Period->yesterday {yesterday}\")\n",
    "    # print(f\"period->train_date_start {train_date_start}\")\n",
    "    # print(f\"Period->train_date_last {train_date_last}\")\n",
    "    # print(f\"Period->test_date_start {test_date_start}\")\n",
    "    # print(f\"Period->test_date_last {test_date_last}\")\n",
    "\n",
    "    # print(f\"This is the tickers for our model {ticker_list_for_models}\")\n",
    "    # print(f\"This is the additional data  we add to the tickers for the model {additional_data}\")\n",
    "    # print(f\"VALIDATE - This is the number of training days of the train dataset {days}\")\n",
    "\n",
    "    # Get features\n",
    "    #matrix_features_sector = preprocessing(ticker_list_for_models, additional_data, days)\n",
    "\n",
    "    #### Run models for all tickers selected in input  and predict\n",
    "\n",
    "    predictions = pd.DataFrame()  # to store predictions\n",
    "\n",
    "    for ticker in ticker_list_for_models:\n",
    "        sector = get_ticker_sector(ticker)\n",
    "        additional_data = read_config_file()[1]\n",
    "        additional_data = generate_enhanced_data(sector,ticker)\n",
    "        print(\"in module model_Train_Save, additional_data\")\n",
    "        print(additional_data)\n",
    "\n",
    "        matrix_features_sector = preprocessing(ticker, additional_data, days)\n",
    "\n",
    "        X_train, y_train, X_test, y_test, df_filtered = create_train_test_set(ticker, matrix_features_sector, lags,additional_data,days,nb_predict_days)\n",
    "        \n",
    "        print(\"in module model_Train_Save, print model\")\n",
    "        print(MODEL)\n",
    "        print(\"in module model_Train_Save, print ticker\")\n",
    "        print(ticker)\n",
    "        MODEL.fit(X_train, y_train)\n",
    "        # save the model to disk\n",
    "        filename = ticker + '_model.sav'\n",
    "        joblib.dump(MODEL, filename)\n",
    "        upload_file_to_bucket(filename)\n",
    "        temp_pred = model_predict(MODEL, ticker, X_test, y_test)\n",
    "\n",
    "        predictions = predictions.append(temp_pred, ignore_index=True)  # this is to store in the master pandas list\n",
    "\n",
    "    # add binary buy=1 and sell=0\n",
    "    df_lagged = add_buy_sell_to_prediction(predictions,ticker_list_for_models)\n",
    "    df_lagged  # lag_lagged is a DF containing predictions + buy and Sell label\n",
    "\n",
    "    ticker = \"*all*\"\n",
    "    accuracy = balanced_accuracy(ticker, df_lagged)\n",
    "    print(f\"Accuracy score for {ticker} is {accuracy}.\")\n",
    "\n",
    "    accuracy = []\n",
    "    for ticker in ticker_list_for_models:\n",
    "        accuracy.append([balanced_accuracy(ticker, df_lagged), ticker])\n",
    "    DF_accuracy = pd.DataFrame(accuracy, columns=[\"Blc accuracy\", \"Ticker\"])\n",
    "    DF_accuracy\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def balanced_accuracy(ticker, predict):\n",
    "    # put *all* to have global accuracy score for all the predictions\n",
    "    # or put a ticker name\n",
    "    if ticker == '*all*':\n",
    "        return balanced_accuracy_score(predict['y_testb'], predict['y_predb'])\n",
    "    else:\n",
    "        return balanced_accuracy_score(predict['y_testb'][predict['ticker'] == ticker],\n",
    "                                       predict['y_predb'][predict['ticker'] == ticker])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def model_predict(MODEL, ticker, X, y):\n",
    "    print('X test shape in module model_predict',X.shape)\n",
    "    y_pred = MODEL.predict(X)\n",
    "    temp_pred = predictions_compile(y, y_pred, ticker)  # this is to store temporary ytest, ypredict, ticker\n",
    "    return temp_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def predictions_compile(y_test, y_pred, ticker):\n",
    "    print(\"module predictions compile\")\n",
    "    print(\"y_test_shape in module predictions compile\",y_test.shape)\n",
    "    print(\"y_pred_shape in module predictions compile\",y_pred.shape)\n",
    "   \n",
    "    print (\"y_test in module predictions compile\", y_test)\n",
    "    print (\"y_pred in module predictions compile\",y_pred)\n",
    "    print (\"ticker in module predictions compile\",ticker)\n",
    "    # this allow to create a dataframe of y_test, y_predict for a given ticker\n",
    "    predict_df = pd.DataFrame({'y_test': y_test, 'y_pred': y_pred})\n",
    "    predict_df['ticker'] = ticker\n",
    "    print (\"predict_df in module predictions compile\",predict_df)\n",
    "    return predict_df\n",
    "    # we need to have an initial empty dataframe to store the predictions\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def add_buy_sell_to_prediction(predictions, ticker_list_for_models):\n",
    "    # this section is to calculate the label=buy or sell\n",
    "    # it is adding colum y_testb and y_predictb to data frame predictions\n",
    "    # buy=1\n",
    "    # sell=0\n",
    "\n",
    "    df_lagged = pd.DataFrame()\n",
    "    for ticker in ticker_list_for_models:\n",
    "        df = predictions[predictions['ticker'] == ticker]\n",
    "        df_lagged_ticker = df.copy()\n",
    "        for window in range(1, 1 + 1):\n",
    "            shifted = df.shift(window)\n",
    "            shifted.columns = [x + \"_lag\" + str(window) for x in df.columns]\n",
    "\n",
    "            df_lagged_ticker = pd.concat((df_lagged_ticker, shifted), axis=1)\n",
    "            df_lagged_ticker = df_lagged_ticker.dropna()\n",
    "            df_lagged_ticker['y_testb'] = np.floor(df_lagged_ticker['y_test'] / df_lagged_ticker['y_test_lag1']).astype(\n",
    "                int)\n",
    "            df_lagged_ticker['y_predb'] = np.floor(df_lagged_ticker['y_pred'] / df_lagged_ticker['y_pred_lag1']).astype(\n",
    "                int)\n",
    "            # *** using map function to decide buy or sell\n",
    "            category = {1: \"Buy\", 0: \"Sell\"}\n",
    "            df_lagged_ticker['y_recommend'] = df_lagged_ticker['y_predb'].map(category)\n",
    "            df_lagged_ticker['daily return %'] = (\n",
    "                        (df_lagged_ticker['y_test'] / df_lagged_ticker['y_test_lag1']) - 1).where \\\n",
    "                (df_lagged_ticker['y_predb'].shift() == 1, \\\n",
    "                 (((df_lagged_ticker['y_test_lag1'] / df_lagged_ticker['y_test']) - 1)))\n",
    "            df_lagged_ticker['daily return %'] = df_lagged_ticker['daily return %'] * 100\n",
    "        df_lagged = df_lagged.append(df_lagged_ticker)\n",
    "\n",
    "    return df_lagged\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {},
   "outputs": [],
   "source": [
    "# functions to predict model and calculate accuracy\n",
    "\n",
    "import os\n",
    "# import bamboolib as bam\n",
    "import os.path\n",
    "\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_ticker(ticker_list_for_models):\n",
    "    ticker_list_for_models = ticker_list_for_models.split('-')\n",
    "    # note when we use flask we will use /ticker/aapl-nflx-cdw\n",
    "    # so we need to split\n",
    "\n",
    "    # print(f\"Example long_test: {long_test}\")\n",
    "    # print(f\"Example short_test: {short_test}\")\n",
    "    # print(f\"Example not_working: {not_working}\")\n",
    "    # ticker_list_for_models=input_ticker\n",
    "\n",
    "    # initialisation model\n",
    "    # how many years for the model\n",
    "    years = 6\n",
    "    lags = 30  # how many days of lags we need of this model, this is like an hyperparameter for us\n",
    "    version, additional_data, regressor, model,bucket = read_config_file()  # other data needed\n",
    "    print(version)\n",
    "    print(regressor)\n",
    "    print(model)\n",
    "    print(additional_data)\n",
    "    print(bucket)\n",
    "\n",
    "    nb_predict_days = 30  # size of test data in number of days\n",
    "\n",
    "    # if we don't have model for a ticker in list, retrain model and save\n",
    "    check = True\n",
    "    for ticker in ticker_list_for_models:\n",
    "        check=delete_then_get_model_from_bucket(ticker + \"_model.sav\") #this download model from bucket. Model will be trained if model does not exist\n",
    "\n",
    "        if not (os.path.exists(ticker + \"_model.sav\")):\n",
    "            check = False\n",
    "            break  # this allow to continue and not go through the list if file not exist\n",
    "\n",
    "    if check == False:\n",
    "        #Retrain all for the select list of tickers\n",
    "        print(f\"Training model for at least one ticker in {ticker_list_for_models}\")\n",
    "        Model_Train_Save(ticker_list_for_models, years, lags, additional_data, nb_predict_days)\n",
    "    else:\n",
    "        print(f\"no model Training is needed for  {ticker_list_for_models}\")\n",
    "        ## get variable for start the prediction\n",
    "        ##just in case, we fetch 2 time the lags so that we don't have issue when lagging\n",
    "\n",
    "    yesterday, start_date, train_date_start, train_date_last, test_date_start, test_date_last, days = initialize_data(\n",
    "        lags * 2, lags, nb_predict_days)\n",
    "\n",
    "    ### validation if we have everything\n",
    "    print(f\"Input ->years {years}\")\n",
    "    print(f\"Input ->ticker list {ticker_list_for_models}\")\n",
    "    print(f\"Input->lags {lags}\")\n",
    "    print(f\"Input ->predict days {nb_predict_days}\")\n",
    "    print(f\"Period->yesterday {yesterday}\")\n",
    "    print(f\"period->train_date_start {train_date_start}\")\n",
    "    print(f\"Period->train_date_last {train_date_last}\")\n",
    "    print(f\"Period->test_date_start {test_date_start}\")\n",
    "    print(f\"Period->test_date_last {test_date_last}\")\n",
    "\n",
    "    print(f\"This is the tickers for our model {ticker_list_for_models}\")\n",
    "    print(f\"This is the additional data  we add to the tickers for the model {additional_data}\")\n",
    "\n",
    "    #df_tomorrow = preprocessing(ticker_list_for_models, additional_data, lags * 2)  # add additional features\n",
    "    #df_tomorrow.shape\n",
    "\n",
    "    # to store predictions\n",
    "    Predictions = pd.DataFrame()\n",
    "\n",
    "    for ticker in ticker_list_for_models:\n",
    "        # load the saved model for the ticker\n",
    "        filename = ticker + \"_model.sav\"\n",
    "        loaded_model = joblib.load(filename)\n",
    "        sector=get_ticker_sector(ticker)\n",
    "        additional_data=read_config_file()[1]\n",
    "        additional_data=generate_enhanced_data(sector,ticker)\n",
    "        print(additional_data)\n",
    "        df_tomorrow = preprocessing(ticker, additional_data, lags * 2)  # add additional features\n",
    "        print(\"predict df_tomorrow shape before creating predict set\")\n",
    "        print(df_tomorrow.shape)\n",
    "\n",
    "        X_test, y_test, df_filtered = create_predict_set(ticker, df_tomorrow, lags, nb_predict_days,\n",
    "                                                         additional_data)  # this is X and y\n",
    "\n",
    "        print(\"X_test shape in module predict_ticker before calling model_predict and after creating predict_set\",X_test.shape)\n",
    "\n",
    "\n",
    "        temp_pred = model_predict(loaded_model, ticker, X_test, y_test)\n",
    "        print(\"temp_pred shape in module predict_ticker after calling model_predict\",temp_pred.shape)\n",
    "\n",
    "        print(temp_pred)\n",
    "        # ===\n",
    "        temp_pred.drop(labels=[0], inplace=True)\n",
    "        temp_pred.to_csv('temp_pred.csv')\n",
    "\n",
    "        from datetime import timedelta\n",
    "\n",
    "        df_filtered2 = pd.DataFrame(df_filtered[ticker])\n",
    "\n",
    "        df_filtered2 = df_filtered2.reset_index()\n",
    "\n",
    "        # Deleted 1 row in df_filtered2\n",
    "        df_filtered2.drop(labels=[0], inplace=True)\n",
    "\n",
    "        # Renamed columns Date\n",
    "        df_filtered2.rename(columns={'index': 'Date'}, inplace=True)\n",
    "\n",
    "        df_filtered2['Predicted for'] = df_filtered2['Date'] + timedelta(days=1)\n",
    "        # df_filtered2['Prediction for']=df_filtered2['Date']\n",
    "        df_filtered2 = df_filtered2[['Date', 'Predicted for']]\n",
    "\n",
    "        # Step: Copy a dataframe column\n",
    "\n",
    "        temp_pred2 = pd.concat([temp_pred, df_filtered2], axis=1)\n",
    "\n",
    "        # ===\n",
    "\n",
    "        Predictions = Predictions.append(temp_pred2, ignore_index=True)  # this is to store in the master pandas list\n",
    "\n",
    "        # print(f\"temp_pred: \\n{temp_pred}\")\n",
    "        # print(f\"temp_pred2: \\n{temp_pred2}\")\n",
    "        # print(f\"temp_pred2: \\n{temp_pred2}\")\n",
    "        # print(f\"df_filtered2: \\n{df_filtered2}\")\n",
    "\n",
    "        # print(f\"Predictions:\\n {Predictions}\")\n",
    "\n",
    "    # adding binary buy or sell to predictions dataframe\n",
    "    Predictions = add_buy_sell_to_prediction(Predictions,ticker_list_for_models)\n",
    "    Predictions.to_csv('predictions.csv')\n",
    "\n",
    "    ticker = \"*all*\"\n",
    "    accuracy_all= balanced_accuracy(ticker, Predictions)\n",
    "    print(f\"Accuracy score for {ticker} is {accuracy_all}.\")\n",
    "    # provide a data frame of the accuracies\n",
    "\n",
    "    avg_return = [0]  # render for view html need a first element to be 0\n",
    "    for ticker in ticker_list_for_models:\n",
    "        ticker_return = round(Predictions['daily return %'][Predictions['ticker'] == ticker].mean(), 2)\n",
    "        avg_return.append(ticker_return)\n",
    "\n",
    "    DF_Recommendations = []\n",
    "    for ticker in ticker_list_for_models:\n",
    "        DF_Recommendations.append([ticker, \"\", \"\", \"\", balanced_accuracy(ticker, Predictions)])\n",
    "        Recommendations = pd.DataFrame(DF_Recommendations,\n",
    "                                       columns=[\"Ticker\", 'Predicted for', 'Predicted', \"Recommended\", \"Accuracy\"])\n",
    "\n",
    "    # ********************************************************\n",
    "    # suggestion - DF_accuracy change to DF_accuracy_recommendation\n",
    "    # suggestion - resultat change as follow: Date, Observed Value, Date Prediction, Predicted Value,Recommendation\n",
    "    # suggestion - now date observed value should be change to NA\n",
    "    # ********************************************************\n",
    "\n",
    "    for ticker in ticker_list_for_models:\n",
    "        # results = Predictions[['y_test', 'y_pred', 'ticker', 'y_predb','y_recommend']][Predictions['ticker'] == ticker]\n",
    "        results = Predictions[['y_test', 'y_pred', 'ticker', 'y_predb', 'y_recommend','daily return %']][Predictions['ticker'] == ticker]\n",
    "        # date_predict=yesterday + timedelta(1)\n",
    "        date_predict = yesterday\n",
    "        date_predict = date_predict.strftime(\"%Y/%m/%d\")\n",
    "        ticker_predicted = results.iloc[-1]['y_pred']  # this is the last row containing result\n",
    "        ticker_recommend = results.iloc[-1]['y_recommend']  # this is the last row containing result\n",
    "        Recommendations.loc[Recommendations['Ticker'] == ticker, 'Predicted for'] = date_predict  # to change content of a cell\n",
    "        Recommendations.loc[Recommendations['Ticker'] == ticker, 'Predicted'] = ticker_predicted\n",
    "        Recommendations.loc[Recommendations['Ticker'] == ticker, 'Recommended'] = ticker_recommend\n",
    "\n",
    "        # print(f\"Prediction for {yesterday+timedelta(1)} -- ticker: {ticker} {'**resultat**'}\\n {resultat.tail()}\\n\\n\")\n",
    "\n",
    "    Results = Predictions[['ticker', 'Date', 'y_test', 'Predicted for', 'y_pred', 'y_recommend','daily return %']]\n",
    "    Results = Results.rename(\n",
    "        columns={'ticker': 'Ticker', 'y_test': 'Observed', 'y_pred': 'Predicted', 'y_recommend': 'Recommended','daily return %':'Daily return %'})\n",
    "\n",
    "    return Results, Recommendations,avg_return\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import os.path\n",
    "import numpy as np\n",
    "from yahoo_fin import stock_info as si\n",
    "#from flask import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sp500():\n",
    "    SP500_tickers = si.tickers_sp500()\n",
    "    delete_blob(\"SP500.json\")\n",
    "    SP500_list = read_create_write_SP500(SP500_tickers, \"SP500.json\")\n",
    "    return f\"JSON was generated  successfully- try again with /ticker/xxx or /details/xxx (where xxx is your list of tickers separated by '-'.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get_sp500()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def check_ticker(ticker):\n",
    "    ticker = ticker.upper()\n",
    "    version=read_config_file()[0]\n",
    "    model_html=read_config_file()[3]\n",
    "    filename_json=\"SP500.json\"\n",
    "    check=delete_then_get_model_from_bucket(filename_json)  # delete local file and copy file from bucket to have fresh one\n",
    "    #file_exists = os.path.exists(\"SP500.json\") # no longer required with blob check above\n",
    "    if check==True:  #file json exist\n",
    "        SP500_list = read_list(filename_json)\n",
    "        SP500_list = np.array(SP500_list)  # we need an array\n",
    "        validation=all([([x] in SP500_list[:,:1]) for x in ticker.split(\"-\")])\n",
    "        # all allow to check if a list o bolean is tru or false . all([true, false, true....etc])\n",
    "        # X in SP50_list, etc.... .... will check if x is in my SP500 list. here all row and column 0\n",
    "        # for x in is selecting each at a time\n",
    "\n",
    "        if validation==True:\n",
    "            Results, Recommendations,avg_return = predict_ticker(ticker)\n",
    "            return Results, Recommendations,avg_return\n",
    "            \n",
    "\n",
    "        else:\n",
    "            return f\"Incorrect ticker, please fix or select another.\"\n",
    "    else:\n",
    "        return f\"JSON does not exists - Please generate JSON  with /get_SP500\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "old file SP500.json deleted locally\n",
      "\n",
      "new file   SP500.json downloaded\n",
      "\n",
      "old file AAPL_model.sav deleted locally\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1\n",
      "*\n",
      "Lasso(alpha=1.0)\n",
      "['^tnx', '^GSPC', 'CL=F']\n",
      "stock-363101\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "file AAPL_model.sav not found in bucket\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model for at least one ticker in ['AAPL']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "old file SP500.json deleted locally\n",
      "\n",
      "new file   SP500.json downloaded\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in module model_Train_Save, additional_data\n",
      "['ACN', 'ADBE', 'ADI', 'ADSK', 'AKAM', 'AMAT', 'AMD', 'ANET', 'ANSS', 'APH', 'AVGO', 'BR', 'CDAY', 'CDNS', 'CDW', 'CRM', 'CSCO', 'CTSH', 'DXC', 'ENPH', 'EPAM', 'FFIV', 'FIS', 'FISV', 'FLT', 'FTNT', 'FTV', 'GLW', 'GRMN', 'HPE', 'HPQ', 'IBM', 'INTC', 'INTU', 'IT', 'JKHY', 'JNPR', 'KEYS', 'KLAC', 'LDOS', 'LRCX', 'MCHP', 'MPWR', 'MSFT', 'MSI', 'MU', 'NLOK', 'NOW', 'NTAP', 'NVDA', 'NXPI', 'ON', 'ORCL', 'PAYC', 'PTC', 'QCOM', 'QRVO', 'SEDG', 'SNPS', 'STX', 'SWKS', 'TDY', 'TEL', 'TER', 'TRMB', 'TXN', 'TYL', 'VRSN', 'WDC', 'ZBRA', '^tnx', '^GSPC', 'CL=F']\n",
      "preprocing module ticker AAPL\n",
      "preprocessing module additional data ['ACN', 'ADBE', 'ADI', 'ADSK', 'AKAM', 'AMAT', 'AMD', 'ANET', 'ANSS', 'APH', 'AVGO', 'BR', 'CDAY', 'CDNS', 'CDW', 'CRM', 'CSCO', 'CTSH', 'DXC', 'ENPH', 'EPAM', 'FFIV', 'FIS', 'FISV', 'FLT', 'FTNT', 'FTV', 'GLW', 'GRMN', 'HPE', 'HPQ', 'IBM', 'INTC', 'INTU', 'IT', 'JKHY', 'JNPR', 'KEYS', 'KLAC', 'LDOS', 'LRCX', 'MCHP', 'MPWR', 'MSFT', 'MSI', 'MU', 'NLOK', 'NOW', 'NTAP', 'NVDA', 'NXPI', 'ON', 'ORCL', 'PAYC', 'PTC', 'QCOM', 'QRVO', 'SEDG', 'SNPS', 'STX', 'SWKS', 'TDY', 'TEL', 'TER', 'TRMB', 'TXN', 'TYL', 'VRSN', 'WDC', 'ZBRA', '^tnx', '^GSPC', 'CL=F']\n",
      "preprocessing module days 2160\n",
      "data in module get_yf_dataframe ['AAPL', 'ACN', 'ADBE', 'ADI', 'ADSK', 'AKAM', 'AMAT', 'AMD', 'ANET', 'ANSS', 'APH', 'AVGO', 'BR', 'CDAY', 'CDNS', 'CDW', 'CRM', 'CSCO', 'CTSH', 'DXC', 'ENPH', 'EPAM', 'FFIV', 'FIS', 'FISV', 'FLT', 'FTNT', 'FTV', 'GLW', 'GRMN', 'HPE', 'HPQ', 'IBM', 'INTC', 'INTU', 'IT', 'JKHY', 'JNPR', 'KEYS', 'KLAC', 'LDOS', 'LRCX', 'MCHP', 'MPWR', 'MSFT', 'MSI', 'MU', 'NLOK', 'NOW', 'NTAP', 'NVDA', 'NXPI', 'ON', 'ORCL', 'PAYC', 'PTC', 'QCOM', 'QRVO', 'SEDG', 'SNPS', 'STX', 'SWKS', 'TDY', 'TEL', 'TER', 'TRMB', 'TXN', 'TYL', 'VRSN', 'WDC', 'ZBRA', '^tnx', '^GSPC', 'CL=F']\n",
      "nbdays in  in module get_yf_dataframe 2160\n",
      "in module model_Train_Save, print model\n",
      "Lasso()\n",
      "in module model_Train_Save, print ticker\n",
      "AAPL\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/steve/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.840e+03, tolerance: 2.246e+02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "uploading AAPL_model.sav to stock-363101_01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X test shape in module model_predict (21, 2293)\n",
      "module predictions compile\n",
      "y_test_shape in module predictions compile (21,)\n",
      "y_pred_shape in module predictions compile (21,)\n",
      "y_test in module predictions compile [140.08999634 140.41999817 138.97999573 138.33999634 142.99000549\n",
      " 138.38000488 142.41000366 143.75       143.86000061 143.38999939\n",
      " 147.27000427 149.44999695 152.33999634 149.3500061  144.80000305\n",
      " 155.74000549 153.33999634 150.6499939  145.02999878 138.88000488\n",
      " 138.38000488]\n",
      "y_pred in module predictions compile [140.13060595 136.90215196 136.78820365 136.89686095 139.38079571\n",
      " 136.89298972 138.76349067 140.71993066 141.6422268  141.03762502\n",
      " 144.15348825 146.38112869 148.20290914 147.97154008 146.19043319\n",
      " 147.4723568  150.15629089 149.80157929 144.82901367 140.13140879\n",
      " 140.73245478]\n",
      "ticker in module predictions compile AAPL\n",
      "predict_df in module predictions compile         y_test      y_pred ticker\n",
      "0   140.089996  140.130606   AAPL\n",
      "1   140.419998  136.902152   AAPL\n",
      "2   138.979996  136.788204   AAPL\n",
      "3   138.339996  136.896861   AAPL\n",
      "4   142.990005  139.380796   AAPL\n",
      "5   138.380005  136.892990   AAPL\n",
      "6   142.410004  138.763491   AAPL\n",
      "7   143.750000  140.719931   AAPL\n",
      "8   143.860001  141.642227   AAPL\n",
      "9   143.389999  141.037625   AAPL\n",
      "10  147.270004  144.153488   AAPL\n",
      "11  149.449997  146.381129   AAPL\n",
      "12  152.339996  148.202909   AAPL\n",
      "13  149.350006  147.971540   AAPL\n",
      "14  144.800003  146.190433   AAPL\n",
      "15  155.740005  147.472357   AAPL\n",
      "16  153.339996  150.156291   AAPL\n",
      "17  150.649994  149.801579   AAPL\n",
      "18  145.029999  144.829014   AAPL\n",
      "19  138.880005  140.131409   AAPL\n",
      "20  138.380005  140.732455   AAPL\n",
      "Accuracy score for *all* is 0.8080808080808081.\n",
      "Input ->years 6\n",
      "Input ->ticker list ['AAPL']\n",
      "Input->lags 30\n",
      "Input ->predict days 30\n",
      "Period->yesterday 2022-11-06 18:23:18.818316\n",
      "period->train_date_start 2022-09-07\n",
      "Period->train_date_last 2022-10-06\n",
      "Period->test_date_start 2022-10-07\n",
      "Period->test_date_last 2022-11-06\n",
      "This is the tickers for our model ['AAPL']\n",
      "This is the additional data  we add to the tickers for the model ['^tnx', '^GSPC', 'CL=F']\n",
      "['ACN', 'ADBE', 'ADI', 'ADSK', 'AKAM', 'AMAT', 'AMD', 'ANET', 'ANSS', 'APH', 'AVGO', 'BR', 'CDAY', 'CDNS', 'CDW', 'CRM', 'CSCO', 'CTSH', 'DXC', 'ENPH', 'EPAM', 'FFIV', 'FIS', 'FISV', 'FLT', 'FTNT', 'FTV', 'GLW', 'GRMN', 'HPE', 'HPQ', 'IBM', 'INTC', 'INTU', 'IT', 'JKHY', 'JNPR', 'KEYS', 'KLAC', 'LDOS', 'LRCX', 'MCHP', 'MPWR', 'MSFT', 'MSI', 'MU', 'NLOK', 'NOW', 'NTAP', 'NVDA', 'NXPI', 'ON', 'ORCL', 'PAYC', 'PTC', 'QCOM', 'QRVO', 'SEDG', 'SNPS', 'STX', 'SWKS', 'TDY', 'TEL', 'TER', 'TRMB', 'TXN', 'TYL', 'VRSN', 'WDC', 'ZBRA', '^tnx', '^GSPC', 'CL=F']\n",
      "preprocing module ticker AAPL\n",
      "preprocessing module additional data ['ACN', 'ADBE', 'ADI', 'ADSK', 'AKAM', 'AMAT', 'AMD', 'ANET', 'ANSS', 'APH', 'AVGO', 'BR', 'CDAY', 'CDNS', 'CDW', 'CRM', 'CSCO', 'CTSH', 'DXC', 'ENPH', 'EPAM', 'FFIV', 'FIS', 'FISV', 'FLT', 'FTNT', 'FTV', 'GLW', 'GRMN', 'HPE', 'HPQ', 'IBM', 'INTC', 'INTU', 'IT', 'JKHY', 'JNPR', 'KEYS', 'KLAC', 'LDOS', 'LRCX', 'MCHP', 'MPWR', 'MSFT', 'MSI', 'MU', 'NLOK', 'NOW', 'NTAP', 'NVDA', 'NXPI', 'ON', 'ORCL', 'PAYC', 'PTC', 'QCOM', 'QRVO', 'SEDG', 'SNPS', 'STX', 'SWKS', 'TDY', 'TEL', 'TER', 'TRMB', 'TXN', 'TYL', 'VRSN', 'WDC', 'ZBRA', '^tnx', '^GSPC', 'CL=F']\n",
      "preprocessing module days 60\n",
      "data in module get_yf_dataframe ['AAPL', 'ACN', 'ADBE', 'ADI', 'ADSK', 'AKAM', 'AMAT', 'AMD', 'ANET', 'ANSS', 'APH', 'AVGO', 'BR', 'CDAY', 'CDNS', 'CDW', 'CRM', 'CSCO', 'CTSH', 'DXC', 'ENPH', 'EPAM', 'FFIV', 'FIS', 'FISV', 'FLT', 'FTNT', 'FTV', 'GLW', 'GRMN', 'HPE', 'HPQ', 'IBM', 'INTC', 'INTU', 'IT', 'JKHY', 'JNPR', 'KEYS', 'KLAC', 'LDOS', 'LRCX', 'MCHP', 'MPWR', 'MSFT', 'MSI', 'MU', 'NLOK', 'NOW', 'NTAP', 'NVDA', 'NXPI', 'ON', 'ORCL', 'PAYC', 'PTC', 'QCOM', 'QRVO', 'SEDG', 'SNPS', 'STX', 'SWKS', 'TDY', 'TEL', 'TER', 'TRMB', 'TXN', 'TYL', 'VRSN', 'WDC', 'ZBRA', '^tnx', '^GSPC', 'CL=F']\n",
      "nbdays in  in module get_yf_dataframe 60\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1736497/945662832.py:103: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  predictions = predictions.append(temp_pred, ignore_index=True)  # this is to store in the master pandas list\n",
      "/tmp/ipykernel_1736497/548588589.py:29: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df_lagged = df_lagged.append(df_lagged_ticker)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predict df_tomorrow shape before creating predict set\n",
      "(44, 74)\n",
      "features shape in module create_predict_set\n",
      "(44, 74)\n",
      "df=features+additional data in module create_predict_set\n",
      "(44, 74)\n",
      "df_lagged of features+additional data before dropna in module create_predict_set\n",
      "(44, 2294)\n",
      "                  AAPL         ACN        ADBE         ADI        ADSK  \\\n",
      "2022-09-06  154.529999  283.459991  368.299988  148.229996  198.179993   \n",
      "2022-09-07  155.960007  286.760010  379.720001  150.699997  206.190002   \n",
      "2022-09-08  154.460007  287.959991  383.630005  152.130005  209.820007   \n",
      "2022-09-09  157.369995  290.549988  394.779999  154.179993  211.679993   \n",
      "2022-09-12  163.429993  295.260010  396.359985  155.649994  215.160004   \n",
      "2022-09-13  153.839996  281.519989  368.390015  148.250000  208.339996   \n",
      "2022-09-14  155.309998  278.529999  371.519989  150.250000  208.520004   \n",
      "2022-09-15  152.369995  273.859985  309.130005  147.869995  201.300003   \n",
      "2022-09-16  150.699997  272.679993  299.500000  149.309998  194.970001   \n",
      "2022-09-19  154.479996  274.980011  296.059998  149.610001  196.889999   \n",
      "2022-09-20  156.899994  270.239990  291.059998  149.779999  194.970001   \n",
      "2022-09-21  153.720001  265.420013  286.299988  148.440002  192.419998   \n",
      "2022-09-22  152.740005  262.320007  287.059998  145.339996  187.149994   \n",
      "2022-09-23  150.429993  259.980011  284.559998  141.919998  184.559998   \n",
      "2022-09-26  150.770004  257.540009  276.959991  140.820007  183.990005   \n",
      "2022-09-27  151.759995  256.339996  277.570007  141.809998  187.960007   \n",
      "2022-09-28  149.839996  261.929993  281.399994  144.580002  190.979996   \n",
      "2022-09-29  142.479996  258.269989  278.250000  141.990005  189.460007   \n",
      "2022-09-30  138.199997  257.299988  275.200012  139.339996  186.800003   \n",
      "2022-10-03  142.449997  264.890015  285.239990  145.130005  192.460007   \n",
      "2022-10-04  146.100006  274.309998  294.970001  150.850006  199.990005   \n",
      "2022-10-05  146.399994  274.339996  297.380005  151.889999  204.529999   \n",
      "2022-10-06  145.429993  269.470001  298.410004  150.970001  205.869995   \n",
      "2022-10-07  140.089996  259.709991  288.769989  144.919998  194.740005   \n",
      "2022-10-10  140.419998  257.850006  285.720001  140.899994  191.029999   \n",
      "2022-10-11  138.979996  252.979996  284.829987  138.800003  191.029999   \n",
      "2022-10-12  138.339996  250.070007  286.149994  138.589996  193.639999   \n",
      "2022-10-13  142.990005  257.459991  294.739990  142.750000  193.850006   \n",
      "2022-10-14  138.380005  252.720001  287.940002  136.729996  189.809998   \n",
      "2022-10-17  142.410004  262.220001  293.500000  139.119995  198.699997   \n",
      "2022-10-18  143.750000  264.049988  292.980011  141.100006  200.699997   \n",
      "2022-10-19  143.860001  264.059998  299.829987  141.330002  197.020004   \n",
      "2022-10-20  143.389999  261.779999  302.380005  142.080002  197.830002   \n",
      "2022-10-21  147.270004  269.570007  306.369995  146.589996  201.389999   \n",
      "2022-10-24  149.449997  275.309998  316.220001  144.529999  207.089996   \n",
      "2022-10-25  152.339996  280.609985  323.790009  146.369995  215.720001   \n",
      "2022-10-26  149.350006  279.869995  320.480011  141.380005  214.559998   \n",
      "2022-10-27  144.800003  278.839996  318.649994  140.679993  210.149994   \n",
      "2022-10-28  155.740005  287.779999  325.679993  144.880005  216.389999   \n",
      "2022-10-31  153.339996  283.899994  318.500000  142.619995  214.300003   \n",
      "2022-11-01  150.649994  281.470001  316.019989  144.699997  214.050003   \n",
      "2022-11-02  145.029999  272.450012  301.220001  141.240005  199.380005   \n",
      "2022-11-03  138.880005  256.880005  285.929993  138.020004  194.220001   \n",
      "2022-11-04  138.380005  261.160004  285.750000  144.289993  193.690002   \n",
      "\n",
      "                 AKAM       AMAT        AMD        ANET        ANSS  ...  \\\n",
      "2022-09-06  88.650002  90.290001  78.720001  117.550003  241.259995  ...   \n",
      "2022-09-07  90.290001  91.940002  79.610001  120.269997  249.770004  ...   \n",
      "2022-09-08  90.220001  93.790001  82.779999  122.779999  254.880005  ...   \n",
      "2022-09-09  91.690002  96.510002  85.449997  124.410004  258.799988  ...   \n",
      "2022-09-12  93.110001  96.300003  84.639999  124.750000  261.420013  ...   \n",
      "2022-09-13  89.660004  90.389999  77.029999  119.919998  248.759995  ...   \n",
      "2022-09-14  89.389999  90.639999  77.449997  122.260002  247.000000  ...   \n",
      "2022-09-15  88.239998  88.919998  76.660004  116.940002  241.449997  ...   \n",
      "2022-09-16  87.160004  88.870003  76.510002  115.730003  240.740005  ...   \n",
      "2022-09-19  88.559998  89.720001  76.769997  114.940002  241.429993  ...   \n",
      "2022-09-20  85.930000  88.120003  75.250000  114.070000  239.350006  ...   \n",
      "2022-09-21  83.800003  87.089996  74.480003  113.820000  235.419998  ...   \n",
      "2022-09-22  82.089996  85.040001  69.500000  112.550003  232.039993  ...   \n",
      "2022-09-23  81.110001  84.290001  67.959999  109.970001  229.539993  ...   \n",
      "2022-09-26  80.709999  82.940002  66.300003  109.099998  229.720001  ...   \n",
      "2022-09-27  80.919998  84.150002  67.169998  110.919998  227.570007  ...   \n",
      "2022-09-28  82.250000  86.000000  68.360001  116.690002  232.190002  ...   \n",
      "2022-09-29  80.500000  84.419998  64.139999  114.750000  227.529999  ...   \n",
      "2022-09-30  80.320000  81.930000  63.360001  112.889999  221.699997  ...   \n",
      "2022-10-03  83.839996  86.250000  66.110001  115.830002  227.970001  ...   \n",
      "2022-10-04  87.160004  89.410004  67.900002  120.809998  233.210007  ...   \n",
      "2022-10-05  85.379997  89.220001  67.940002  121.349998  233.839996  ...   \n",
      "2022-10-06  84.440002  88.120003  67.849998  121.900002  232.229996  ...   \n",
      "2022-10-07  82.080002  82.599998  58.439999  116.410004  218.300003  ...   \n",
      "2022-10-10  79.879997  79.190002  57.810001  109.480003  209.110001  ...   \n",
      "2022-10-11  78.059998  76.300003  57.630001  107.050003  200.330002  ...   \n",
      "2022-10-12  78.410004  76.010002  57.849998  103.650002  201.949997  ...   \n",
      "2022-10-13  80.459999  79.419998  58.939999  103.910004  205.669998  ...   \n",
      "2022-10-14  79.980003  74.820000  55.939999  100.370003  203.210007  ...   \n",
      "2022-10-17  82.540001  74.410004  57.959999  104.559998  211.800003  ...   \n",
      "2022-10-18  84.379997  75.230003  57.919998  106.279999  216.949997  ...   \n",
      "2022-10-19  82.940002  77.260002  57.230000  105.110001  214.190002  ...   \n",
      "2022-10-20  84.160004  78.660004  57.770000  105.639999  210.509995  ...   \n",
      "2022-10-21  85.879997  82.419998  58.820000  110.519997  214.149994  ...   \n",
      "2022-10-24  86.389999  84.940002  58.700001  110.739998  216.070007  ...   \n",
      "2022-10-25  88.449997  87.529999  61.470001  112.629997  219.500000  ...   \n",
      "2022-10-26  86.760002  88.139999  59.730000  108.970001  218.160004  ...   \n",
      "2022-10-27  87.580002  86.540001  58.599998  119.129997  216.479996  ...   \n",
      "2022-10-28  89.209999  89.720001  62.009998  121.470001  220.889999  ...   \n",
      "2022-10-31  88.330002  88.290001  60.060001  120.860001  221.160004  ...   \n",
      "2022-11-01  88.099998  89.790001  59.660000  127.709999  219.240005  ...   \n",
      "2022-11-02  85.410004  87.760002  58.630001  125.040001  209.449997  ...   \n",
      "2022-11-03  83.360001  86.300003  60.110001  122.250000  212.059998  ...   \n",
      "2022-11-04  84.099998  91.699997  62.189999  131.070007  213.910004  ...   \n",
      "\n",
      "            TER_lag30  TRMB_lag30   TXN_lag30   TYL_lag30  VRSN_lag30  \\\n",
      "2022-09-06        NaN         NaN         NaN         NaN         NaN   \n",
      "2022-09-07        NaN         NaN         NaN         NaN         NaN   \n",
      "2022-09-08        NaN         NaN         NaN         NaN         NaN   \n",
      "2022-09-09        NaN         NaN         NaN         NaN         NaN   \n",
      "2022-09-12        NaN         NaN         NaN         NaN         NaN   \n",
      "2022-09-13        NaN         NaN         NaN         NaN         NaN   \n",
      "2022-09-14        NaN         NaN         NaN         NaN         NaN   \n",
      "2022-09-15        NaN         NaN         NaN         NaN         NaN   \n",
      "2022-09-16        NaN         NaN         NaN         NaN         NaN   \n",
      "2022-09-19        NaN         NaN         NaN         NaN         NaN   \n",
      "2022-09-20        NaN         NaN         NaN         NaN         NaN   \n",
      "2022-09-21        NaN         NaN         NaN         NaN         NaN   \n",
      "2022-09-22        NaN         NaN         NaN         NaN         NaN   \n",
      "2022-09-23        NaN         NaN         NaN         NaN         NaN   \n",
      "2022-09-26        NaN         NaN         NaN         NaN         NaN   \n",
      "2022-09-27        NaN         NaN         NaN         NaN         NaN   \n",
      "2022-09-28        NaN         NaN         NaN         NaN         NaN   \n",
      "2022-09-29        NaN         NaN         NaN         NaN         NaN   \n",
      "2022-09-30        NaN         NaN         NaN         NaN         NaN   \n",
      "2022-10-03        NaN         NaN         NaN         NaN         NaN   \n",
      "2022-10-04        NaN         NaN         NaN         NaN         NaN   \n",
      "2022-10-05        NaN         NaN         NaN         NaN         NaN   \n",
      "2022-10-06        NaN         NaN         NaN         NaN         NaN   \n",
      "2022-10-07        NaN         NaN         NaN         NaN         NaN   \n",
      "2022-10-10        NaN         NaN         NaN         NaN         NaN   \n",
      "2022-10-11        NaN         NaN         NaN         NaN         NaN   \n",
      "2022-10-12        NaN         NaN         NaN         NaN         NaN   \n",
      "2022-10-13        NaN         NaN         NaN         NaN         NaN   \n",
      "2022-10-14        NaN         NaN         NaN         NaN         NaN   \n",
      "2022-10-17        NaN         NaN         NaN         NaN         NaN   \n",
      "2022-10-18  82.919998   60.220001  163.100006  363.589996  182.289993   \n",
      "2022-10-19  85.019997   62.290001  165.820007  373.709991  186.529999   \n",
      "2022-10-20  86.139999   61.389999  168.410004  380.239990  187.330002   \n",
      "2022-10-21  88.839996   63.080002  170.740005  385.190002  190.559998   \n",
      "2022-10-24  87.379997   64.430000  170.580002  386.859985  189.039993   \n",
      "2022-10-25  83.160004   61.680000  162.649994  364.559998  178.750000   \n",
      "2022-10-26  83.639999   61.220001  165.259995  368.540009  178.339996   \n",
      "2022-10-27  82.349998   59.419998  162.669998  362.760010  174.619995   \n",
      "2022-10-28  82.099998   58.680000  165.259995  360.980011  175.029999   \n",
      "2022-10-31  82.489998   58.919998  166.250000  362.200012  176.270004   \n",
      "2022-11-01  80.839996   58.459999  166.059998  353.859985  174.729996   \n",
      "2022-11-02  80.970001   57.369999  163.300003  351.299988  176.720001   \n",
      "2022-11-03  79.790001   56.790001  162.619995  342.489990  174.639999   \n",
      "2022-11-04  79.190002   56.380001  161.289993  341.119995  173.699997   \n",
      "\n",
      "            WDC_lag30  ZBRA_lag30  ^tnx_lag30  ^GSPC_lag30  CL=F_lag30  \n",
      "2022-09-06        NaN         NaN         NaN          NaN         NaN  \n",
      "2022-09-07        NaN         NaN         NaN          NaN         NaN  \n",
      "2022-09-08        NaN         NaN         NaN          NaN         NaN  \n",
      "2022-09-09        NaN         NaN         NaN          NaN         NaN  \n",
      "2022-09-12        NaN         NaN         NaN          NaN         NaN  \n",
      "2022-09-13        NaN         NaN         NaN          NaN         NaN  \n",
      "2022-09-14        NaN         NaN         NaN          NaN         NaN  \n",
      "2022-09-15        NaN         NaN         NaN          NaN         NaN  \n",
      "2022-09-16        NaN         NaN         NaN          NaN         NaN  \n",
      "2022-09-19        NaN         NaN         NaN          NaN         NaN  \n",
      "2022-09-20        NaN         NaN         NaN          NaN         NaN  \n",
      "2022-09-21        NaN         NaN         NaN          NaN         NaN  \n",
      "2022-09-22        NaN         NaN         NaN          NaN         NaN  \n",
      "2022-09-23        NaN         NaN         NaN          NaN         NaN  \n",
      "2022-09-26        NaN         NaN         NaN          NaN         NaN  \n",
      "2022-09-27        NaN         NaN         NaN          NaN         NaN  \n",
      "2022-09-28        NaN         NaN         NaN          NaN         NaN  \n",
      "2022-09-29        NaN         NaN         NaN          NaN         NaN  \n",
      "2022-09-30        NaN         NaN         NaN          NaN         NaN  \n",
      "2022-10-03        NaN         NaN         NaN          NaN         NaN  \n",
      "2022-10-04        NaN         NaN         NaN          NaN         NaN  \n",
      "2022-10-05        NaN         NaN         NaN          NaN         NaN  \n",
      "2022-10-06        NaN         NaN         NaN          NaN         NaN  \n",
      "2022-10-07        NaN         NaN         NaN          NaN         NaN  \n",
      "2022-10-10        NaN         NaN         NaN          NaN         NaN  \n",
      "2022-10-11        NaN         NaN         NaN          NaN         NaN  \n",
      "2022-10-12        NaN         NaN         NaN          NaN         NaN  \n",
      "2022-10-13        NaN         NaN         NaN          NaN         NaN  \n",
      "2022-10-14        NaN         NaN         NaN          NaN         NaN  \n",
      "2022-10-17        NaN         NaN         NaN          NaN         NaN  \n",
      "2022-10-18  40.970001  289.929993       3.340  3908.189941   86.879997  \n",
      "2022-10-19  41.450001  295.579987       3.265  3979.870117   81.940002  \n",
      "2022-10-20  42.410000  298.299988       3.292  4006.179932   83.540001  \n",
      "2022-10-21  43.799999  307.839996       3.321  4067.360107   86.790001  \n",
      "2022-10-24  43.270000  311.850006       3.362  4110.410156   87.779999  \n",
      "2022-10-25  39.320000  293.500000       3.422  3932.689941   87.309998  \n",
      "2022-10-26  38.330002  295.970001       3.412  3946.010010   88.480003  \n",
      "2022-10-27  37.770000  296.200012       3.459  3901.350098   85.099998  \n",
      "2022-10-28  37.220001  288.519989       3.448  3873.330078   85.110001  \n",
      "2022-10-31  36.619999  291.209991       3.490  3899.889893   85.730003  \n",
      "2022-11-01  35.540001  285.649994       3.571  3855.929932   84.449997  \n",
      "2022-11-02  34.779999  285.079987       3.510  3789.929932   82.940002  \n",
      "2022-11-03  33.820000  272.940002       3.708  3757.989990   83.489998  \n",
      "2022-11-04  33.840000  268.040009       3.697  3693.229980   78.739998  \n",
      "\n",
      "[44 rows x 2294 columns]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_lagged of features+additional data after dropna and before sort in module create_predict_set\n",
      "(14, 2294)\n",
      "df_lagged of features+additional data after dropna in module create_predict_set\n",
      "(14, 2294)\n",
      "                  AAPL   AAPL_lag1  AAPL_lag10  AAPL_lag11  AAPL_lag12  \\\n",
      "2022-10-18  143.750000  142.410004  146.100006  142.449997  138.199997   \n",
      "2022-10-19  143.860001  143.750000  146.399994  146.100006  142.449997   \n",
      "2022-10-20  143.389999  143.860001  145.429993  146.399994  146.100006   \n",
      "2022-10-21  147.270004  143.389999  140.089996  145.429993  146.399994   \n",
      "2022-10-24  149.449997  147.270004  140.419998  140.089996  145.429993   \n",
      "2022-10-25  152.339996  149.449997  138.979996  140.419998  140.089996   \n",
      "2022-10-26  149.350006  152.339996  138.339996  138.979996  140.419998   \n",
      "2022-10-27  144.800003  149.350006  142.990005  138.339996  138.979996   \n",
      "2022-10-28  155.740005  144.800003  138.380005  142.990005  138.339996   \n",
      "2022-10-31  153.339996  155.740005  142.410004  138.380005  142.990005   \n",
      "2022-11-01  150.649994  153.339996  143.750000  142.410004  138.380005   \n",
      "2022-11-02  145.029999  150.649994  143.860001  143.750000  142.410004   \n",
      "2022-11-03  138.880005  145.029999  143.389999  143.860001  143.750000   \n",
      "2022-11-04  138.380005  138.880005  147.270004  143.389999  143.860001   \n",
      "\n",
      "            AAPL_lag13  AAPL_lag14  AAPL_lag15  AAPL_lag16  AAPL_lag17  ...  \\\n",
      "2022-10-18  142.479996  149.839996  151.759995  150.770004  150.429993  ...   \n",
      "2022-10-19  138.199997  142.479996  149.839996  151.759995  150.770004  ...   \n",
      "2022-10-20  142.449997  138.199997  142.479996  149.839996  151.759995  ...   \n",
      "2022-10-21  146.100006  142.449997  138.199997  142.479996  149.839996  ...   \n",
      "2022-10-24  146.399994  146.100006  142.449997  138.199997  142.479996  ...   \n",
      "2022-10-25  145.429993  146.399994  146.100006  142.449997  138.199997  ...   \n",
      "2022-10-26  140.089996  145.429993  146.399994  146.100006  142.449997  ...   \n",
      "2022-10-27  140.419998  140.089996  145.429993  146.399994  146.100006  ...   \n",
      "2022-10-28  138.979996  140.419998  140.089996  145.429993  146.399994  ...   \n",
      "2022-10-31  138.339996  138.979996  140.419998  140.089996  145.429993  ...   \n",
      "2022-11-01  142.990005  138.339996  138.979996  140.419998  140.089996  ...   \n",
      "2022-11-02  138.380005  142.990005  138.339996  138.979996  140.419998  ...   \n",
      "2022-11-03  142.410004  138.380005  142.990005  138.339996  138.979996  ...   \n",
      "2022-11-04  143.750000  142.410004  138.380005  142.990005  138.339996  ...   \n",
      "\n",
      "            ^tnx_lag28  ^tnx_lag29  ^tnx_lag3  ^tnx_lag30  ^tnx_lag4  \\\n",
      "2022-10-18       3.292       3.265      3.952       3.340      3.902   \n",
      "2022-10-19       3.321       3.292      4.010       3.265      3.952   \n",
      "2022-10-20       3.362       3.321      4.015       3.292      4.010   \n",
      "2022-10-21       3.422       3.362      3.998       3.321      4.015   \n",
      "2022-10-24       3.412       3.422      4.127       3.362      3.998   \n",
      "2022-10-25       3.459       3.412      4.226       3.422      4.127   \n",
      "2022-10-26       3.448       3.459      4.213       3.412      4.226   \n",
      "2022-10-27       3.490       3.448      4.234       3.459      4.213   \n",
      "2022-10-28       3.571       3.490      4.108       3.448      4.234   \n",
      "2022-10-31       3.510       3.571      4.015       3.490      4.108   \n",
      "2022-11-01       3.708       3.510      3.937       3.571      4.015   \n",
      "2022-11-02       3.697       3.708      4.010       3.510      3.937   \n",
      "2022-11-03       3.878       3.697      4.077       3.708      4.010   \n",
      "2022-11-04       3.964       3.878      4.052       3.697      4.077   \n",
      "\n",
      "            ^tnx_lag5  ^tnx_lag6  ^tnx_lag7  ^tnx_lag8  ^tnx_lag9  \n",
      "2022-10-18      3.939      3.888      3.883      3.826      3.759  \n",
      "2022-10-19      3.902      3.939      3.888      3.883      3.826  \n",
      "2022-10-20      3.952      3.902      3.939      3.888      3.883  \n",
      "2022-10-21      4.010      3.952      3.902      3.939      3.888  \n",
      "2022-10-24      4.015      4.010      3.952      3.902      3.939  \n",
      "2022-10-25      3.998      4.015      4.010      3.952      3.902  \n",
      "2022-10-26      4.127      3.998      4.015      4.010      3.952  \n",
      "2022-10-27      4.226      4.127      3.998      4.015      4.010  \n",
      "2022-10-28      4.213      4.226      4.127      3.998      4.015  \n",
      "2022-10-31      4.234      4.213      4.226      4.127      3.998  \n",
      "2022-11-01      4.108      4.234      4.213      4.226      4.127  \n",
      "2022-11-02      4.015      4.108      4.234      4.213      4.226  \n",
      "2022-11-03      3.937      4.015      4.108      4.234      4.213  \n",
      "2022-11-04      4.010      3.937      4.015      4.108      4.234  \n",
      "\n",
      "[14 rows x 2294 columns]\n",
      "df_filtered which is only start to last date of predict  in module create_predict_set\n",
      "(14, 2294)\n",
      "X_test shape in module predict_ticker before calling model_predict and after creating predict_set (14, 2293)\n",
      "X test shape in module model_predict (14, 2293)\n",
      "module predictions compile\n",
      "y_test_shape in module predictions compile (14,)\n",
      "y_pred_shape in module predictions compile (14,)\n",
      "y_test in module predictions compile [143.75       143.86000061 143.38999939 147.27000427 149.44999695\n",
      " 152.33999634 149.3500061  144.80000305 155.74000549 153.33999634\n",
      " 150.6499939  145.02999878 138.88000488 138.38000488]\n",
      "y_pred in module predictions compile [171.23160682 176.17610291 182.32965438 187.46399533 190.08138754\n",
      " 192.25103696 197.64116496 200.81290326 198.5731554  199.64833684\n",
      " 202.7735152  196.67676769 194.30021188 185.36997589]\n",
      "ticker in module predictions compile AAPL\n",
      "predict_df in module predictions compile         y_test      y_pred ticker\n",
      "0   143.750000  171.231607   AAPL\n",
      "1   143.860001  176.176103   AAPL\n",
      "2   143.389999  182.329654   AAPL\n",
      "3   147.270004  187.463995   AAPL\n",
      "4   149.449997  190.081388   AAPL\n",
      "5   152.339996  192.251037   AAPL\n",
      "6   149.350006  197.641165   AAPL\n",
      "7   144.800003  200.812903   AAPL\n",
      "8   155.740005  198.573155   AAPL\n",
      "9   153.339996  199.648337   AAPL\n",
      "10  150.649994  202.773515   AAPL\n",
      "11  145.029999  196.676768   AAPL\n",
      "12  138.880005  194.300212   AAPL\n",
      "13  138.380005  185.369976   AAPL\n",
      "temp_pred shape in module predict_ticker after calling model_predict (14, 3)\n",
      "        y_test      y_pred ticker\n",
      "0   143.750000  171.231607   AAPL\n",
      "1   143.860001  176.176103   AAPL\n",
      "2   143.389999  182.329654   AAPL\n",
      "3   147.270004  187.463995   AAPL\n",
      "4   149.449997  190.081388   AAPL\n",
      "5   152.339996  192.251037   AAPL\n",
      "6   149.350006  197.641165   AAPL\n",
      "7   144.800003  200.812903   AAPL\n",
      "8   155.740005  198.573155   AAPL\n",
      "9   153.339996  199.648337   AAPL\n",
      "10  150.649994  202.773515   AAPL\n",
      "11  145.029999  196.676768   AAPL\n",
      "12  138.880005  194.300212   AAPL\n",
      "13  138.380005  185.369976   AAPL\n",
      "Accuracy score for *all* is 0.5625.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1736497/3961471144.py:113: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  Predictions = Predictions.append(temp_pred2, ignore_index=True)  # this is to store in the master pandas list\n",
      "/tmp/ipykernel_1736497/548588589.py:29: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df_lagged = df_lagged.append(df_lagged_ticker)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(   Ticker       Date    Observed Predicted for   Predicted Recommended  \\\n",
       " 1    AAPL 2022-10-20  143.389999    2022-10-21  182.329654         Buy   \n",
       " 2    AAPL 2022-10-21  147.270004    2022-10-22  187.463995         Buy   \n",
       " 3    AAPL 2022-10-24  149.449997    2022-10-25  190.081388         Buy   \n",
       " 4    AAPL 2022-10-25  152.339996    2022-10-26  192.251037         Buy   \n",
       " 5    AAPL 2022-10-26  149.350006    2022-10-27  197.641165         Buy   \n",
       " 6    AAPL 2022-10-27  144.800003    2022-10-28  200.812903         Buy   \n",
       " 7    AAPL 2022-10-28  155.740005    2022-10-29  198.573155        Sell   \n",
       " 8    AAPL 2022-10-31  153.339996    2022-11-01  199.648337         Buy   \n",
       " 9    AAPL 2022-11-01  150.649994    2022-11-02  202.773515         Buy   \n",
       " 10   AAPL 2022-11-02  145.029999    2022-11-03  196.676768        Sell   \n",
       " 11   AAPL 2022-11-03  138.880005    2022-11-04  194.300212        Sell   \n",
       " 12   AAPL 2022-11-04  138.380005    2022-11-05  185.369976        Sell   \n",
       " \n",
       "     Daily return %  \n",
       " 1         0.327778  \n",
       " 2         2.705910  \n",
       " 3         1.480269  \n",
       " 4         1.933757  \n",
       " 5        -1.962709  \n",
       " 6        -3.046537  \n",
       " 7         7.555250  \n",
       " 8         1.565155  \n",
       " 9        -1.754273  \n",
       " 10       -3.730498  \n",
       " 11        4.428279  \n",
       " 12        0.361324  ,\n",
       "   Ticker Predicted for   Predicted Recommended  Accuracy\n",
       " 0   AAPL    2022/11/06  185.369976        Sell    0.5625,\n",
       " [0, 0.82])"
      ]
     },
     "execution_count": 371,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ticker='aapl'\n",
    "check_ticker(ticker)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## New section to see enhanced date and preprocessed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [],
   "source": [
    "ticker='KO'\n",
    "sector=get_ticker_sector(ticker)\n",
    "additional_data=read_config_file()[1]\n",
    "additional_data=generate_enhanced_data(sector,ticker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=preprocessing(ticker,additional_data,5*360)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv('data_sector.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
