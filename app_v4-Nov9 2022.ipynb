{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: yahoo_fin in /home/steve/.local/lib/python3.10/site-packages (0.8.9.1)\n",
      "Requirement already satisfied: requests-html in /home/steve/.local/lib/python3.10/site-packages (from yahoo_fin) (0.10.0)\n",
      "Requirement already satisfied: pandas in /home/steve/.local/lib/python3.10/site-packages (from yahoo_fin) (1.4.2)\n",
      "Requirement already satisfied: feedparser in /home/steve/.local/lib/python3.10/site-packages (from yahoo_fin) (6.0.10)\n",
      "Requirement already satisfied: requests in /usr/lib/python3/dist-packages (from yahoo_fin) (2.25.1)\n",
      "Requirement already satisfied: sgmllib3k in /home/steve/.local/lib/python3.10/site-packages (from feedparser->yahoo_fin) (1.0.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /home/steve/.local/lib/python3.10/site-packages (from pandas->yahoo_fin) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/lib/python3/dist-packages (from pandas->yahoo_fin) (2022.1)\n",
      "Requirement already satisfied: numpy>=1.21.0 in /home/steve/.local/lib/python3.10/site-packages (from pandas->yahoo_fin) (1.22.3)\n",
      "Requirement already satisfied: pyquery in /home/steve/.local/lib/python3.10/site-packages (from requests-html->yahoo_fin) (1.4.3)\n",
      "Requirement already satisfied: pyppeteer>=0.0.14 in /home/steve/.local/lib/python3.10/site-packages (from requests-html->yahoo_fin) (1.0.2)\n",
      "Requirement already satisfied: fake-useragent in /home/steve/.local/lib/python3.10/site-packages (from requests-html->yahoo_fin) (0.1.11)\n",
      "Requirement already satisfied: w3lib in /home/steve/.local/lib/python3.10/site-packages (from requests-html->yahoo_fin) (2.0.1)\n",
      "Requirement already satisfied: parse in /home/steve/.local/lib/python3.10/site-packages (from requests-html->yahoo_fin) (1.19.0)\n",
      "Requirement already satisfied: bs4 in /home/steve/.local/lib/python3.10/site-packages (from requests-html->yahoo_fin) (0.0.1)\n",
      "Requirement already satisfied: certifi>=2021 in /home/steve/.local/lib/python3.10/site-packages (from pyppeteer>=0.0.14->requests-html->yahoo_fin) (2022.9.14)\n",
      "Requirement already satisfied: websockets<11.0,>=10.0 in /home/steve/.local/lib/python3.10/site-packages (from pyppeteer>=0.0.14->requests-html->yahoo_fin) (10.3)\n",
      "Requirement already satisfied: appdirs<2.0.0,>=1.4.3 in /home/steve/.local/lib/python3.10/site-packages (from pyppeteer>=0.0.14->requests-html->yahoo_fin) (1.4.4)\n",
      "Requirement already satisfied: importlib-metadata>=1.4 in /usr/lib/python3/dist-packages (from pyppeteer>=0.0.14->requests-html->yahoo_fin) (4.6.4)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.42.1 in /home/steve/.local/lib/python3.10/site-packages (from pyppeteer>=0.0.14->requests-html->yahoo_fin) (4.64.0)\n",
      "Requirement already satisfied: pyee<9.0.0,>=8.1.0 in /home/steve/.local/lib/python3.10/site-packages (from pyppeteer>=0.0.14->requests-html->yahoo_fin) (8.2.2)\n",
      "Requirement already satisfied: urllib3<2.0.0,>=1.25.8 in /usr/lib/python3/dist-packages (from pyppeteer>=0.0.14->requests-html->yahoo_fin) (1.26.5)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.1->pandas->yahoo_fin) (1.16.0)\n",
      "Requirement already satisfied: beautifulsoup4 in /home/steve/.local/lib/python3.10/site-packages (from bs4->requests-html->yahoo_fin) (4.11.1)\n",
      "Requirement already satisfied: cssselect>0.7.9 in /home/steve/.local/lib/python3.10/site-packages (from pyquery->requests-html->yahoo_fin) (1.1.0)\n",
      "Requirement already satisfied: lxml>=2.1 in /home/steve/.local/lib/python3.10/site-packages (from pyquery->requests-html->yahoo_fin) (4.9.1)\n",
      "Requirement already satisfied: soupsieve>1.2 in /home/steve/.local/lib/python3.10/site-packages (from beautifulsoup4->bs4->requests-html->yahoo_fin) (2.3.2.post1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install yahoo_fin\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: sklearn in /home/steve/.local/lib/python3.10/site-packages (0.0)\n",
      "Requirement already satisfied: scikit-learn in /home/steve/.local/lib/python3.10/site-packages (from sklearn) (1.1.3)\n",
      "Requirement already satisfied: numpy>=1.17.3 in /home/steve/.local/lib/python3.10/site-packages (from scikit-learn->sklearn) (1.22.3)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/steve/.local/lib/python3.10/site-packages (from scikit-learn->sklearn) (3.1.0)\n",
      "Requirement already satisfied: scipy>=1.3.2 in /home/steve/.local/lib/python3.10/site-packages (from scikit-learn->sklearn) (1.9.1)\n",
      "Requirement already satisfied: joblib>=1.0.0 in /home/steve/.local/lib/python3.10/site-packages (from scikit-learn->sklearn) (1.2.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#functions to deal with creating and extracting features\n",
    "\n",
    "#import bamboolib as bam\n",
    "\n",
    "# import bamboolib as bam\n",
    "# import bamboolib as bam\n",
    "from datetime import datetime, timedelta\n",
    "#from VBTT2_IO.IO import  read_config_file\n",
    "\n",
    "# import bamboolib as bam\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from yahoo_fin import stock_info as si\n",
    "\n",
    "\n",
    "def get_yf_dataframe(data, nbdays):\n",
    "    yesterday = datetime.now()  # we want data up to yesterday\n",
    "    start_date = yesterday - timedelta(days=nbdays)  # we run the model using data for nbdays\n",
    "    df_res = pd.DataFrame()\n",
    "    #print(\"data in module get_yf_dataframe\", data)\n",
    "    #print(\"nbdays in  in module get_yf_dataframe\", nbdays)\n",
    "    for ticker in data:\n",
    "        df_tmp = si.get_data(ticker, start_date, yesterday)\n",
    "        df_res[ticker] = df_tmp['close']\n",
    "    return df_res\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(ticker, additional_data, days):\n",
    "    #this function return a matrix of features augmented with fix data for number of days\n",
    "    print (\"preprocing module ticker\",ticker)\n",
    "    print(\"preprocessing module additional data\",additional_data)\n",
    "    print(\"preprocessing module days\", days)\n",
    "    tickers_in_sector_extended = np.concatenate((ticker, additional_data), axis=None)\n",
    "    tickers_in_sector_extended = tickers_in_sector_extended.tolist()\n",
    "    matrix_features_sector = get_yf_dataframe(tickers_in_sector_extended, days)\n",
    "    # import pandas as pd; import numpy as np\n",
    "    # matrix_features_sector = matrix_features_sector.reset_index()\n",
    "    matrix_features_sector = matrix_features_sector.reindex(sorted(matrix_features_sector.columns), axis=1)\n",
    "\n",
    "    ###################################\n",
    "    ##### saving and reading features - can help increase processing time\n",
    "    ##### to evaluate on future version\n",
    "    ####################################\n",
    "    # matrix_features_sector.to_csv(\"matrix_features_sector.csv\")\n",
    "    # matrix_features_sector=pd.read_csv(\"matrix_features_sector.csv\")\n",
    "\n",
    "    return matrix_features_sector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_train_test_set(ticker, features, lags,additional_data,days,nb_predict_days):\n",
    "    # creating train and test set\n",
    "    yesterday, start_date, train_date_start, train_date_last, test_date_start, test_date_last, days = initialize_data(\n",
    "        days,\n",
    "        lags,\n",
    "        nb_predict_days)\n",
    "\n",
    "    df = features[[ticker] + additional_data]\n",
    "    df_ticker = df[[ticker]]\n",
    "    df_lagged=pd.DataFrame()\n",
    "    for window in range(1, lags + 1):\n",
    "        shifted = df.shift(window)\n",
    "        shifted.columns = [x + \"_lag\" + str(window) for x in df.columns]\n",
    "\n",
    "        df_lagged = pd.concat((df_lagged, shifted), axis=1)\n",
    "    #df_lagged = df_lagged.fillna(method='ffill')\n",
    "    df_lagged= df_lagged.interpolate(method='linear')\n",
    "    df_lagged=pd.concat((df_ticker,df_lagged),axis=1)\n",
    "    df_lagged = df_lagged.dropna()\n",
    "    df_lagged = df_lagged.reindex(sorted(df_lagged.columns), axis=1)\n",
    "\n",
    "    # print(df_lagged)\n",
    "    # df_lagged[ticker+\"_2labels\"]=np.floor(df_lagged[ticker]/df_lagged[ticker+\"_lag1\"]).astype(int)\n",
    "\n",
    "    # train_set\n",
    "    df_filtered = df_lagged.loc[:train_date_last]\n",
    "    #X_train=df_filtered.drop(columns=[ticker, ticker+\"_2labels\"])\n",
    "    X_train = df_filtered.drop(columns=[ticker])\n",
    "    # y_train=df_filtered[ticker+\"_2labels\"]\n",
    "    y_train = df_filtered[ticker]\n",
    "\n",
    "    # test set\n",
    "    df_filtered = df_lagged.loc[test_date_start:test_date_last]\n",
    "    # X_test=df_filtered.drop(columns=[ticker, ticker+\"_2labels\"])\n",
    "    X_test = df_filtered.drop(columns=[ticker])\n",
    "    # y_test=df_filtered[ticker+\"_2labels\"]\n",
    "    y_test = df_filtered[ticker]\n",
    "\n",
    "    # we convert to numpy array\n",
    "    X_train = X_train.to_numpy()\n",
    "    y_train = y_train.to_numpy()\n",
    "    X_test = X_test.to_numpy()\n",
    "    y_test = y_test.to_numpy()\n",
    "    return X_train, y_train, X_test, y_test, df_filtered\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "def initialize_data(days, lags, nb_predict_days):\n",
    "\n",
    "    from datetime import datetime, timedelta\n",
    "\n",
    "\n",
    "\n",
    "    # define main date in models for defining training and test sets dates.\n",
    "    #yesterday = datetime.now() - timedelta(1)\n",
    "    yesterday=datetime.now()\n",
    "    start_date = yesterday - timedelta(days=days)\n",
    "    train_date_start = start_date.strftime(\"%Y-%m-%d\")\n",
    "    train_date_last = yesterday - timedelta(days=nb_predict_days + 1)  # nombre de jours a predire\n",
    "    train_date_last = train_date_last.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "    test_date_start = yesterday - timedelta(days=nb_predict_days)\n",
    "    test_date_start = test_date_start.strftime(\"%Y-%m-%d\")\n",
    "    test_date_last = yesterday.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "    return yesterday, start_date, train_date_start, train_date_last, test_date_start, test_date_last,days\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def create_predict_set(ticker ,features ,lags, nb_predict_days, additional_data):\n",
    "\n",
    "    yesterday, start_date, train_date_start, train_date_last, test_date_start, test_date_last, days = initialize_data(lags * 2, lags, nb_predict_days)\n",
    "    #  List of features X_train, y_train, X_test,y_test\n",
    "    print(\"features shape in module create_predict_set\")\n",
    "    print(features.shape)\n",
    "\n",
    "    df =features[[ticker ] +additional_data]\n",
    "    print('df=features+additional data in module create_predict_set')\n",
    "    print(df)\n",
    "    \n",
    "    #duplicate last row of features to use in the lag - this is to add tomorrow date in the predict set\n",
    "    df_last=df.iloc[-1:]  #we get last date\n",
    "    print(df_last.index[-1])\n",
    "    print(type(df_last.index[-1]))\n",
    "    #last_date=datetime.fromtimestamp(df_last.index[-1]) #index is of type timestamp therefore convert to datetime\n",
    "    last_date=df_last.index[-1] #index is of type timestamp therefore convert to datetime\n",
    "    df=df.append(df_last)  #we append last date to end\n",
    "    #df.index.array[-1] = last_date+timedelta(1) #we change to tomorrows date \n",
    "    #- Note this does not work especially during weekend. better use below\n",
    "    \n",
    "    date_predict=YF_datetime()\n",
    "    date_predict = date_predict.strftime(\"%Y/%m/%d\")\n",
    "    df.index.array[-1] = date_predict\n",
    "    \n",
    "    \n",
    "    \n",
    "    #df.set_axis((df.index[:-1].union([last_date],sort=False)),axis=0)\n",
    "    \n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    #df_lagged =df.copy()\n",
    "    df_ticker=df[[ticker]]\n",
    "    df_lagged=pd.DataFrame()\n",
    "    print(\"ici df lagged\", df_lagged)\n",
    "    for window in range(1, lags + 1):\n",
    "        shifted = df.shift(window)\n",
    "        shifted.columns = [x + \"_lag\" + str(window) for x in df.columns]\n",
    "\n",
    "        df_lagged = pd.concat((df_lagged, shifted), axis=1)\n",
    "    print('df_lagged of features+additional data before dropna in module create_predict_set')\n",
    "    print(df_lagged.shape)\n",
    "    print(df_lagged)\n",
    "    #df_lagged = df_lagged.fillna(method='ffill')\n",
    "    df_lagged= df_lagged.interpolate(method='linear')\n",
    "    df_lagged=pd.concat((df_ticker,df_lagged),axis=1)\n",
    "    df_lagged = df_lagged.dropna()\n",
    "    print('df_lagged of features+additional data after dropna and before sort in module create_predict_set')\n",
    "    print(df_lagged.shape)\n",
    "    df_lagged =df_lagged.reindex(sorted(df_lagged.columns), axis=1)\n",
    "    df_lagged.to_csv('df_lagged3.csv')\n",
    "    print('df_lagged of features+additional data after dropna in module create_predict_set')\n",
    "    print(df_lagged.shape)\n",
    "    print(df_lagged)\n",
    "    # test set\n",
    "    df_filtered = df_lagged.loc[test_date_start:test_date_last]\n",
    "    \n",
    "    \n",
    "    print('df_filtered which is only start to last date of predict  in module create_predict_set')\n",
    "    print(df_filtered.shape)\n",
    "    \n",
    "    X_test =df_filtered.drop(columns=[ticker])\n",
    "    y_test =df_filtered[ticker]\n",
    "    # we convert to numpy array\n",
    "    X_test =X_test.to_numpy()\n",
    "    y_test= y_test.to_numpy()\n",
    "    return X_test ,y_test ,df_filtered\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import bamboolib as bam\n",
    "import os\n",
    "import os.path\n",
    "\n",
    "import numpy as np\n",
    "from yahoo_fin import stock_info as si\n",
    "\n",
    "#from VBTT2_IO.IO import write_list, read_list,delete_then_get_model_from_bucket,read_config_file\n",
    "import time\n",
    "\n",
    "\n",
    "\n",
    "#functions to fetch and process SP500 data\n",
    "\n",
    "def read_create_write_SP500(SP500_tickers,filename_json):\n",
    "    # This create the complete master list of all tickers in SP500, their sectors, their industries\n",
    "    # Check if SP500 json file exist first otherwise process SP500\n",
    "\n",
    "    check=delete_then_get_model_from_bucket(filename_json) #delete local file and copy file from bucket to have fresh one\n",
    "    #file_exists = os.path.exists(filename_json) #no longer required with blob check above\n",
    "\n",
    "    if check==True:\n",
    "        SP500_list = read_list(filename_json)\n",
    "        SP500_list = np.array(SP500_list)  # we need an array\n",
    "    else:\n",
    "        SP500_list = read_write_SP500(SP500_tickers,filename_json)  # this will extract\n",
    "\n",
    "    return SP500_list\n",
    "\n",
    "\n",
    "def get_ticker_sector(ticker):\n",
    "    # version 2\n",
    "    # prerequisites is to import Yahoo_finance.stock_info\n",
    "    # probleme avec cette methode c'est que industry est ligne 18 ou 19 *\n",
    "    # prerequisites 2 is to import pandas as pd; import numpy as np\n",
    "    filename_json=\"SP500.json\"\n",
    "    file_exists = os.path.exists(filename_json)\n",
    "    if file_exists:  # file json exist\n",
    "        SP500_list = read_list(filename_json)\n",
    "        SP500_list = np.array(SP500_list)  # we need an array\n",
    "        #now we should extract the sector which is column 2 for in each item of this 2D array\n",
    "        ticker_sector=SP500_list[:, 1:2][SP500_list[0:, 0:1] == ticker].tolist()\n",
    "\n",
    "        return ticker_sector[0]\n",
    "    else:\n",
    "        df = si.get_company_info(ticker)  # a utiliser pour trouver le secteur\n",
    "        df = df.reset_index()\n",
    "        sector = df.loc[df['Breakdown'].isin(['sector'])]  # from bamboolib to extract sector\n",
    "        sector = sector.iloc[0, 1]  # this is to extract just the value\n",
    "        industry = df.loc[df['Breakdown'].isin(['industry'])]\n",
    "        industry = industry.iloc[0, 1]\n",
    "        ticker_sector = []\n",
    "        ticker_sector.append([ticker, sector, industry])\n",
    "        return ticker_sector[0]\n",
    "\n",
    "\n",
    "def read_write_SP500(tickers_list,filename_json):\n",
    "    # Initialisation of SP500 data - find sector, industry for all tickers in SP500\n",
    "\n",
    "    SP500_list = []\n",
    "    for ticker in tickers_list:\n",
    "        print(\"ticker in module read_write_SP500\",ticker)\n",
    "        SP500_list.append(get_ticker_sector(ticker))\n",
    "        if len(SP500_list) % 30 == 0:\n",
    "            time.sleep(30)\n",
    "            # for some reason processing SP500 one shot is failing. so sleep of 20 seconds for each 50 tickers\n",
    "    SP500_list = np.array(SP500_list)\n",
    "    # Save in a file to reduce processing time next time\n",
    "    write_list(SP500_list.tolist(), filename_json)  # this function work if it is a list\n",
    "    return SP500_list  # this returns the array, not the list\n",
    "\n",
    "\n",
    "def get_all_tickers_sector(sector):\n",
    "    # prerequisites list is an array of ticker, sector and industry\n",
    "    filename_json = \"SP500.json\"\n",
    "    SP500_list = read_list(filename_json)\n",
    "    SP500_list = np.array(SP500_list)  # we need an array\n",
    "\n",
    "\n",
    "    sub_list_tickers = np.array(SP500_list)\n",
    "    fltr = np.asarray([sector])\n",
    "    result = sub_list_tickers[np.in1d(sub_list_tickers[:, 1], fltr)]\n",
    "    return result[:, 0:1]\n",
    "    # on veut extraire les tickers donc toutes les rangees (0:0) et la colonne 0 donc (0:1)\n",
    "\n",
    "\n",
    "def get_all_tickers_industry(industry):\n",
    "    # prerequisites list is an array of ticker, sector and industry\n",
    "    filename_json = \"SP500.json\"\n",
    "    SP500_list = read_list(filename_json)\n",
    "    SP500_list = np.array(SP500_list)  # we need an array\n",
    "\n",
    "    sub_list_tickers = np.array(SP500_list)\n",
    "    fltr = np.asarray([industry])\n",
    "    result = sub_list_tickers[np.in1d(sub_list_tickers[:, 2], fltr)]\n",
    "    return result[:, 0:1]\n",
    "    # on veut extraire les tickers donc toutes les rangees (0:0) et la colonne 0 donc (0:1)\n",
    "\n",
    "\n",
    "def generate_enhanced_data(sector,ticker):\n",
    "    additional_data = read_config_file()[1]\n",
    "    sector_list=get_all_tickers_sector(sector)\n",
    "    additional_data = np.concatenate((sector_list, additional_data), axis=None)\n",
    "    additional_data=additional_data.tolist()\n",
    "    additional_data.remove(ticker)\n",
    "    return additional_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: google.api.core in /home/steve/.local/lib/python3.10/site-packages (2.10.2)\n",
      "Requirement already satisfied: google-auth<3.0dev,>=1.25.0 in /home/steve/.local/lib/python3.10/site-packages (from google.api.core) (2.14.0)\n",
      "Requirement already satisfied: requests<3.0.0dev,>=2.18.0 in /usr/lib/python3/dist-packages (from google.api.core) (2.25.1)\n",
      "Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5 in /home/steve/.local/lib/python3.10/site-packages (from google.api.core) (4.21.9)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.56.2 in /home/steve/.local/lib/python3.10/site-packages (from google.api.core) (1.56.4)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /home/steve/.local/lib/python3.10/site-packages (from google-auth<3.0dev,>=1.25.0->google.api.core) (5.2.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /home/steve/.local/lib/python3.10/site-packages (from google-auth<3.0dev,>=1.25.0->google.api.core) (4.9)\n",
      "Requirement already satisfied: six>=1.9.0 in /usr/lib/python3/dist-packages (from google-auth<3.0dev,>=1.25.0->google.api.core) (1.16.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /home/steve/.local/lib/python3.10/site-packages (from google-auth<3.0dev,>=1.25.0->google.api.core) (0.2.8)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /home/steve/.local/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth<3.0dev,>=1.25.0->google.api.core) (0.4.8)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install google.api.core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: google-cloud-storage in /home/steve/.local/lib/python3.10/site-packages (2.5.0)\n",
      "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5 in /home/steve/.local/lib/python3.10/site-packages (from google-cloud-storage) (2.10.2)\n",
      "Requirement already satisfied: google-resumable-media>=2.3.2 in /home/steve/.local/lib/python3.10/site-packages (from google-cloud-storage) (2.4.0)\n",
      "Requirement already satisfied: google-auth<3.0dev,>=1.25.0 in /home/steve/.local/lib/python3.10/site-packages (from google-cloud-storage) (2.14.0)\n",
      "Requirement already satisfied: requests<3.0.0dev,>=2.18.0 in /usr/lib/python3/dist-packages (from google-cloud-storage) (2.25.1)\n",
      "Requirement already satisfied: google-cloud-core<3.0dev,>=2.3.0 in /home/steve/.local/lib/python3.10/site-packages (from google-cloud-storage) (2.3.2)\n",
      "Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5 in /home/steve/.local/lib/python3.10/site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-storage) (4.21.9)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.56.2 in /home/steve/.local/lib/python3.10/site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-storage) (1.56.4)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /home/steve/.local/lib/python3.10/site-packages (from google-auth<3.0dev,>=1.25.0->google-cloud-storage) (4.9)\n",
      "Requirement already satisfied: six>=1.9.0 in /usr/lib/python3/dist-packages (from google-auth<3.0dev,>=1.25.0->google-cloud-storage) (1.16.0)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /home/steve/.local/lib/python3.10/site-packages (from google-auth<3.0dev,>=1.25.0->google-cloud-storage) (5.2.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /home/steve/.local/lib/python3.10/site-packages (from google-auth<3.0dev,>=1.25.0->google-cloud-storage) (0.2.8)\n",
      "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /home/steve/.local/lib/python3.10/site-packages (from google-resumable-media>=2.3.2->google-cloud-storage) (1.5.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /home/steve/.local/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth<3.0dev,>=1.25.0->google-cloud-storage) (0.4.8)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install google-cloud-storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.api_core.exceptions import NotFound"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from google.cloud import storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import bamboolib as bam\n",
    "\n",
    "# Input Output functions to save and read files\n",
    "\n",
    "#import bamboolib as bam\n",
    "import json\n",
    "import configparser\n",
    "import logging\n",
    "import joblib\n",
    "\n",
    "import os\n",
    "import os.path\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_list(a_list, filename_json):\n",
    "    print(\"Started writing list data into a json file\")\n",
    "    with open(filename_json, \"w\") as fp:\n",
    "        json.dump(a_list, fp)\n",
    "        print(\"Done writing data into .json file\")\n",
    "    upload_file_to_bucket(filename_json)\n",
    "\n",
    "\n",
    "# Read list to memory\n",
    "def read_list(filename_json):\n",
    "    # for reading also binary mode is important\n",
    "    with open(filename_json, 'rb') as fp:\n",
    "        n_list = json.load(fp)\n",
    "        return n_list\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def read_config_file():\n",
    "    # return element that is in configfile. exemple additional data\n",
    "    config = configparser.ConfigParser()\n",
    "    config.read('config.ini')\n",
    "    version = config['DEFAULT']['version']\n",
    "    additional_data = config['DEFAULT']['additional_data'].split(',')\n",
    "    regressor = config['DEFAULT']['regressor']\n",
    "    model = config['DEFAULT']['model']\n",
    "    root_bucket=config['DEFAULT']['gcloud_root_bucket']\n",
    "\n",
    "    # ^TNX reasury yield is the annual return investors can expect from holding a U.S. government security with a given\n",
    "    # ^GSPC tracks the performance of the stocks of 500 large-cap companies in the US\"\n",
    "    # CL=F crude oil pricesi.get_data(result[0][0])\n",
    "\n",
    "    return version, additional_data,regressor, model, root_bucket\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#file existimport logging\n",
    "\n",
    "\n",
    "def create_bucket(bucket_name):\n",
    "    log = logging.getLogger()\n",
    "\n",
    "    storage_client = storage.Client()\n",
    "    if bucket_name not in [x.name for x in storage_client.list_buckets()]:\n",
    "        bucket = storage_client.create_bucket(bucket_name)\n",
    "\n",
    "        log.info(\"Bucket {} created\".format(bucket.name))\n",
    "    else:\n",
    "        log.info(\"Bucket {} already exists\".format(bucket_name))\n",
    "\n",
    "\n",
    "\n",
    "def get_bucket_name():\n",
    "    root_bucket = read_config_file()[4]\n",
    "    version = read_config_file()[0]\n",
    "    bucket_name=f'{root_bucket}_{version.replace(\".\", \"\")}'\n",
    "    create_bucket(bucket_name)\n",
    "    return bucket_name\n",
    "\n",
    "\n",
    "def upload_file_to_bucket(model_file_name):\n",
    "    bucket_name=get_bucket_name()\n",
    "    log = logging.getLogger()\n",
    "    log.warning(f'uploading {model_file_name} to {bucket_name}')\n",
    "    storage_client = storage.Client()\n",
    "    bucket=storage_client.bucket(bucket_name)\n",
    "    blob = bucket.blob(model_file_name)\n",
    "    blob.upload_from_filename(model_file_name)\n",
    "\n",
    "\n",
    "def delete_blob(blob_name):\n",
    "    log = logging.getLogger()\n",
    "    bucket_name = get_bucket_name()\n",
    "    storage_client = storage.Client()\n",
    "    bucket = storage_client.bucket(bucket_name)\n",
    "    blob = bucket.blob(blob_name)\n",
    "    blob.delete()\n",
    "    log.warning(f'old blob {blob_name} deleted from {bucket_name}\\n')\n",
    "\n",
    "\n",
    "def delete_then_get_model_from_bucket(model_filename):\n",
    "    log = logging.getLogger()\n",
    "    if (os.path.exists(model_filename)):\n",
    "        os.remove(model_filename)\n",
    "        log.warning(f'old file {model_filename} deleted locally\\n')\n",
    "\n",
    "    bucket_name = get_bucket_name()\n",
    "    storage_client = storage.Client()\n",
    "    bucket = storage_client.bucket(bucket_name)\n",
    "    blob=bucket.blob(model_filename)\n",
    "\n",
    "    try:\n",
    "        blob.download_to_filename(model_filename)\n",
    "        log.warning(f'new file   {model_filename} downloaded\\n')\n",
    "        return True #file download ok therefore retrain not required  or fileexists locally\n",
    "\n",
    "    except NotFound as e:\n",
    "        log.warning(f'file {model_filename} not found in bucket\\n')\n",
    "        return False  #retrain required or file does not exist locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "from yahoo_fin import stock_info as si\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Model_Train_Save(ticker_list_for_models, years, lags, additional_data, nb_predict_days):\n",
    "    # Functions for model training and algorythm data and import\n",
    "\n",
    "    # import the regressors\n",
    "    version, additional_data, regressor, MODEL,bucket=read_config_file()\n",
    "   \n",
    "\n",
    "    if MODEL=='DecisionTree()':\n",
    "        from sklearn.tree import DecisionTreeRegressor\n",
    "        MODEL = DecisionTreeRegressor()\n",
    "    elif MODEL=='LinearRegression()':\n",
    "        from sklearn.linear_model import LinearRegression\n",
    "        MODEL = LinearRegression()\n",
    "    elif MODEL=='svm.SVR()':\n",
    "        from sklearn import svm\n",
    "        MODEL = svm.SVR()\n",
    "    elif MODEL==\"DecisionTree(max_depth=5)\":\n",
    "        from sklearn.tree import DecisionTreeRegressor\n",
    "        MODEL = DecisionTreeRegressor(max_depth=5)\n",
    "    elif MODEL==\"Ridge(alpha=1.0)\":\n",
    "        from sklearn.linear_model import Ridge\n",
    "        MODEL=Ridge(alpha=1.0)\n",
    "    elif MODEL==\"Lasso(alpha=1.0)\":\n",
    "        from sklearn.linear_model import Lasso\n",
    "        MODEL=Lasso(alpha=1.0)\n",
    "    elif MODEL==\"xgboost\":\n",
    "        import xgboost as xgb\n",
    "        MODEL= xgb.XGBClassifier()\n",
    "    elif MODEL==\"RandomForestRegressor(n_estimators=100)\":   \n",
    "        from sklearn.ensemble import RandomForestRegressor\n",
    "        MODEL=RandomForestRegressor(n_estimators=100)\n",
    "    else:\n",
    "        #MODEL=='LinearRegression'\n",
    "        from sklearn.linear_model import LinearRegression\n",
    "        MODEL = LinearRegression()\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    #### Initialisation of variables and data\n",
    "    # the timeframe for training and test sets and predict\n",
    "    days = 360 * years  # Nunber of days in the model\n",
    "    yesterday, start_date, train_date_start, train_date_last, test_date_start, test_date_last, days = initialize_data(\n",
    "        days,\n",
    "        lags,\n",
    "        nb_predict_days)\n",
    "    SP500_tickers = si.tickers_sp500()  # get list of tickers\n",
    "\n",
    "    # print(f\"SP500_tickers = {SP500_tickers}\")\n",
    "\n",
    "\n",
    "\n",
    "    # read master list of ticker, sector, industries or if not found, create it and save it#\n",
    "    SP500_list = read_create_write_SP500(SP500_tickers, \"SP500.json\")\n",
    "\n",
    "    ### validation if we have everything\n",
    "    print(f\"Model Input ->years {years}\")\n",
    "    print(f\"Model Input ->ticker list {ticker_list_for_models}\")\n",
    "    print(f\"Model Input->lags {lags}\")\n",
    "    print(f\"Model Input ->predict days {nb_predict_days}\")\n",
    "    print(f\"Model Period->yesterday {yesterday}\")\n",
    "    print(f\"Model period->train_date_start {train_date_start}\")\n",
    "    print(f\"Model Period->train_date_last {train_date_last}\")\n",
    "    print(f\"Model Period->test_date_start {test_date_start}\")\n",
    "    print(f\"Model Period->test_date_last {test_date_last}\")\n",
    "\n",
    "    print(f\"Model This is the tickers for our model {ticker_list_for_models}\")\n",
    "    print(f\"Model This is the additional data  we add to the tickers for the model {additional_data}\")\n",
    "    print(f\"Model VALIDATE - This is the number of training days of the train dataset {days}\")\n",
    "\n",
    "    # Get features\n",
    "    #matrix_features_sector = preprocessing(ticker_list_for_models, additional_data, days)\n",
    "\n",
    "    #### Run models for all tickers selected in input  and predict\n",
    "\n",
    "    predictions = pd.DataFrame()  # to store predictions\n",
    "\n",
    "    for ticker in ticker_list_for_models:\n",
    "        sector = get_ticker_sector(ticker)\n",
    "        #additional_data = read_config_file()[1]\n",
    "        additional_data = generate_enhanced_data(sector,ticker)\n",
    "        print(\"in module model_Train_Save, additional_data\")\n",
    "        print(additional_data)\n",
    "\n",
    "        matrix_features_sector = preprocessing(ticker, additional_data, days)\n",
    "\n",
    "        X_train, y_train, X_test, y_test, df_filtered = create_train_test_set(ticker, matrix_features_sector, lags,additional_data,days,nb_predict_days)\n",
    "        \n",
    "        print(\"in module model_Train_Save, print model\")\n",
    "        print(MODEL)\n",
    "        print(\"in module model_Train_Save, print ticker\")\n",
    "        print(ticker)\n",
    "        MODEL.fit(X_train, y_train)\n",
    "        # save the model to disk\n",
    "        filename = ticker + '_model.sav'\n",
    "        joblib.dump(MODEL, filename)\n",
    "        upload_file_to_bucket(filename)\n",
    "        temp_pred = model_predict(MODEL, ticker, X_test, y_test)\n",
    "\n",
    "        predictions = predictions.append(temp_pred, ignore_index=True)  # this is to store in the master pandas list\n",
    "\n",
    "    # add binary buy=1 and sell=0\n",
    "    df_lagged = add_buy_sell_to_prediction(predictions,ticker_list_for_models)\n",
    "    df_lagged  # lag_lagged is a DF containing predictions + buy and Sell label\n",
    "\n",
    "    ticker = \"*all*\"\n",
    "    accuracy = balanced_accuracy(ticker, df_lagged)\n",
    "    print(f\"Accuracy score for {ticker} is {accuracy}.\")\n",
    "\n",
    "    accuracy = []\n",
    "    for ticker in ticker_list_for_models:\n",
    "        accuracy.append([balanced_accuracy(ticker, df_lagged), ticker])\n",
    "    DF_accuracy = pd.DataFrame(accuracy, columns=[\"Blc accuracy\", \"Ticker\"])\n",
    "    DF_accuracy\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def balanced_accuracy(ticker, predict):\n",
    "    # put *all* to have global accuracy score for all the predictions\n",
    "    # or put a ticker name\n",
    "    if ticker == '*all*':\n",
    "        return balanced_accuracy_score(predict['y_testb'], predict['y_predb'])\n",
    "    else:\n",
    "        return balanced_accuracy_score(predict['y_testb'][predict['ticker'] == ticker],\n",
    "                                       predict['y_predb'][predict['ticker'] == ticker])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def model_predict(MODEL, ticker, X, y):\n",
    "    print('X test shape in module model_predict',X.shape)\n",
    "    y_pred = MODEL.predict(X)\n",
    "    temp_pred = predictions_compile(y, y_pred, ticker)  # this is to store temporary ytest, ypredict, ticker\n",
    "    return temp_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def predictions_compile(y_test, y_pred, ticker):\n",
    "    print(\"module predictions compile\")\n",
    "    print(\"y_test_shape in module predictions compile\",y_test.shape)\n",
    "    print(\"y_pred_shape in module predictions compile\",y_pred.shape)\n",
    "   \n",
    "    print (\"y_test in module predictions compile\", y_test)\n",
    "    print (\"y_pred in module predictions compile\",y_pred)\n",
    "    print (\"ticker in module predictions compile\",ticker)\n",
    "    # this allow to create a dataframe of y_test, y_predict for a given ticker\n",
    "    predict_df = pd.DataFrame({'y_test': y_test, 'y_pred': y_pred})\n",
    "    predict_df['ticker'] = ticker\n",
    "    print (\"predict_df in module predictions compile\",predict_df)\n",
    "    return predict_df\n",
    "    # we need to have an initial empty dataframe to store the predictions\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def add_buy_sell_to_prediction(predictions, ticker_list_for_models):\n",
    "    # this section is to calculate the label=buy or sell\n",
    "    # it is adding colum y_testb and y_predictb to data frame predictions\n",
    "    # buy=1\n",
    "    # sell=0\n",
    "\n",
    "    df_lagged = pd.DataFrame()\n",
    "    for ticker in ticker_list_for_models:\n",
    "        df = predictions[predictions['ticker'] == ticker]\n",
    "        df_lagged_ticker = df.copy()\n",
    "        for window in range(1, 1 + 1):\n",
    "            shifted = df.shift(window)\n",
    "            shifted.columns = [x + \"_lag\" + str(window) for x in df.columns]\n",
    "\n",
    "            df_lagged_ticker = pd.concat((df_lagged_ticker, shifted), axis=1)\n",
    "            df_lagged_ticker = df_lagged_ticker.dropna()\n",
    "            df_lagged_ticker['y_testb'] = np.floor(df_lagged_ticker['y_test'] / df_lagged_ticker['y_test_lag1']).astype(\n",
    "                int)\n",
    "            df_lagged_ticker['y_predb'] = np.floor(df_lagged_ticker['y_pred'] / df_lagged_ticker['y_test_lag1']).astype(\n",
    "                int)\n",
    "            # *** using map function to decide buy or sell\n",
    "            category = {1: \"Buy\", 0: \"Sell\"}\n",
    "            df_lagged_ticker['y_recommend'] = df_lagged_ticker['y_predb'].map(category)\n",
    "            df_lagged_ticker['daily return %'] = (\n",
    "                        (df_lagged_ticker['y_test'] / df_lagged_ticker['y_test_lag1']) - 1).where \\\n",
    "                (df_lagged_ticker['y_predb'] == 1, \\\n",
    "                 (((df_lagged_ticker['y_test_lag1'] / df_lagged_ticker['y_test']) - 1)))\n",
    "            df_lagged_ticker['daily return %'] = df_lagged_ticker['daily return %'] * 100\n",
    "        df_lagged = df_lagged.append(df_lagged_ticker)\n",
    "\n",
    "    return df_lagged\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "# functions to predict model and calculate accuracy\n",
    "\n",
    "import os\n",
    "# import bamboolib as bam\n",
    "import os.path\n",
    "\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "def YF_datetime():\n",
    "    date_predict=datetime.now()\n",
    "    if date_predict.hour>=17:\n",
    "        date_predict=date_predict+timedelta(1)\n",
    "    return date_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_ticker(ticker_list_for_models):\n",
    "    ticker_list_for_models = ticker_list_for_models.split('-')\n",
    "    # note when we use flask we will use /ticker/aapl-nflx-cdw\n",
    "    # so we need to split\n",
    "\n",
    "    # print(f\"Example long_test: {long_test}\")\n",
    "    # print(f\"Example short_test: {short_test}\")\n",
    "    # print(f\"Example not_working: {not_working}\")\n",
    "    # ticker_list_for_models=input_ticker\n",
    "\n",
    "    # initialisation model\n",
    "    # how many years for the model\n",
    "    years = 6\n",
    "    lags = 30  # how many days of lags we need of this model, this is like an hyperparameter for us\n",
    "    version, additional_data, regressor, model,bucket = read_config_file()  # other data needed\n",
    "    print(version)\n",
    "    print(regressor)\n",
    "    print(model)\n",
    "    print(additional_data)\n",
    "    print(bucket)\n",
    "\n",
    "    nb_predict_days = 15  # size of test data in number of days\n",
    "\n",
    "    # if we don't have model for a ticker in list, retrain model and save\n",
    "    check = True\n",
    "    for ticker in ticker_list_for_models:\n",
    "        check=delete_then_get_model_from_bucket(ticker + \"_model.sav\") #this download model from bucket. Model will be trained if model does not exist\n",
    "\n",
    "        if not (os.path.exists(ticker + \"_model.sav\")):\n",
    "            check = False\n",
    "            break  # this allow to continue and not go through the list if file not exist\n",
    "\n",
    "    if check == False:\n",
    "        #Retrain all for the select list of tickers\n",
    "        print(f\"Training model for at least one ticker in {ticker_list_for_models}\")\n",
    "        Model_Train_Save(ticker_list_for_models, years, lags, additional_data, nb_predict_days)\n",
    "    else:\n",
    "        print(f\"no model Training is needed for  {ticker_list_for_models}\")\n",
    "        ## get variable for start the prediction\n",
    "        ##just in case, we fetch 2 time the lags so that we don't have issue when lagging\n",
    "\n",
    "    yesterday, start_date, train_date_start, train_date_last, test_date_start, test_date_last, days = initialize_data(\n",
    "        lags * 2.5, lags, nb_predict_days)\n",
    "\n",
    "    ### validation if we have everything\n",
    "    print(f\"Input ->years {years}\")\n",
    "    print(f\"Input ->ticker list {ticker_list_for_models}\")\n",
    "    print(f\"Input->lags {lags}\")\n",
    "    print(f\"Input ->predict days {nb_predict_days}\")\n",
    "    print(f\"Period->yesterday {yesterday}\")\n",
    "    print(f\"period->train_date_start {train_date_start}\")\n",
    "    print(f\"Period->train_date_last {train_date_last}\")\n",
    "    print(f\"Period->test_date_start {test_date_start}\")\n",
    "    print(f\"Period->test_date_last {test_date_last}\")\n",
    "\n",
    "    print(f\"This is the tickers for our model {ticker_list_for_models}\")\n",
    "    print(f\"This is the additional data  we add to the tickers for the model {additional_data}\")\n",
    "\n",
    "    #df_tomorrow = preprocessing(ticker_list_for_models, additional_data, lags * 2)  # add additional features\n",
    "    #df_tomorrow.shape\n",
    "\n",
    "    # to store predictions\n",
    "    Predictions = pd.DataFrame()\n",
    "\n",
    "    for ticker in ticker_list_for_models:\n",
    "        # load the saved model for the ticker\n",
    "        filename = ticker + \"_model.sav\"\n",
    "        loaded_model = joblib.load(filename)\n",
    "        sector=get_ticker_sector(ticker)\n",
    "        additional_data=read_config_file()[1]\n",
    "        additional_data=generate_enhanced_data(sector,ticker)\n",
    "        print(additional_data)\n",
    "        df_tomorrow = preprocessing(ticker, additional_data, lags * 2.5)  # add additional features\n",
    "        print(\"predict df_tomorrow shape before creating predict set\")\n",
    "        print(df_tomorrow.shape)\n",
    "\n",
    "        X_test, y_test, df_filtered = create_predict_set(ticker, df_tomorrow, lags, nb_predict_days,\n",
    "                                                         additional_data)  # this is X and y\n",
    "\n",
    "        print(\"X_test shape in module predict_ticker before calling model_predict and after creating predict_set\",X_test.shape)\n",
    "\n",
    "\n",
    "        temp_pred = model_predict(loaded_model, ticker, X_test, y_test)\n",
    "        print(\"temp_pred shape in module predict_ticker after calling model_predict\",temp_pred.shape)\n",
    "\n",
    "        print(temp_pred)\n",
    "        # ===\n",
    "        temp_pred.drop(labels=[0], inplace=True)\n",
    "        temp_pred.to_csv('temp_pred.csv')\n",
    "\n",
    "        from datetime import timedelta\n",
    "\n",
    "        df_filtered2 = pd.DataFrame(df_filtered[ticker])\n",
    "\n",
    "        df_filtered2 = df_filtered2.reset_index()\n",
    "\n",
    "        # Deleted 1 row in df_filtered2\n",
    "        df_filtered2.drop(labels=[0], inplace=True)\n",
    "\n",
    "        # Renamed columns Date\n",
    "        df_filtered2.rename(columns={'index': 'Date'}, inplace=True)\n",
    "\n",
    "        df_filtered2['Predicted for'] = df_filtered2['Date'] \n",
    "        # df_filtered2['Prediction for']=df_filtered2['Date']\n",
    "        df_filtered2 = df_filtered2[['Date', 'Predicted for']]\n",
    "\n",
    "        # Step: Copy a dataframe column\n",
    "\n",
    "        temp_pred2 = pd.concat([temp_pred, df_filtered2], axis=1)\n",
    "\n",
    "        # ===\n",
    "\n",
    "        Predictions = Predictions.append(temp_pred2, ignore_index=True)  # this is to store in the master pandas list\n",
    "\n",
    "        # print(f\"temp_pred: \\n{temp_pred}\")\n",
    "        # print(f\"temp_pred2: \\n{temp_pred2}\")\n",
    "        # print(f\"temp_pred2: \\n{temp_pred2}\")\n",
    "        # print(f\"df_filtered2: \\n{df_filtered2}\")\n",
    "\n",
    "        # print(f\"Predictions:\\n {Predictions}\")\n",
    "\n",
    "    # adding binary buy or sell to predictions dataframe\n",
    "    Predictions = add_buy_sell_to_prediction(Predictions,ticker_list_for_models)\n",
    "    Predictions.to_csv('predictions.csv')\n",
    "\n",
    "    #ticker = \"*all*\"\n",
    "    #accuracy_all= balanced_accuracy(ticker, Predictions)\n",
    "    #print(f\"Accuracy score for {ticker} is {accuracy_all}.\")\n",
    "    # provide a data frame of the accuracies\n",
    "\n",
    "    avg_return = [0]  # render for view html need a first element to be 0\n",
    "    for ticker in ticker_list_for_models:\n",
    "        ticker_return = round(Predictions['daily return %'][Predictions['ticker'] == ticker].iloc[:-1].mean(), 2)\n",
    "        avg_return.append(ticker_return)\n",
    "\n",
    "    DF_Recommendations = []\n",
    "    for ticker in ticker_list_for_models:\n",
    "        DF_Recommendations.append([ticker, \"\", \"\", \"\", balanced_accuracy(ticker, Predictions.iloc[:-1])])\n",
    "        Recommendations = pd.DataFrame(DF_Recommendations,\n",
    "                                       columns=[\"Ticker\", 'Predicted for', 'Predicted', \"Recommended\", \"Accuracy\"])\n",
    "        print(Recommendations)\n",
    "\n",
    "    # ********************************************************\n",
    "    # suggestion - DF_accuracy change to DF_accuracy_recommendation\n",
    "    # suggestion - resultat change as follow: Date, Observed Value, Date Prediction, Predicted Value,Recommendation\n",
    "    # suggestion - now date observed value should be change to NA\n",
    "    # ********************************************************\n",
    "\n",
    "    for ticker in ticker_list_for_models:\n",
    "        # results = Predictions[['y_test', 'y_pred', 'ticker', 'y_predb','y_recommend']][Predictions['ticker'] == ticker]\n",
    "        results = Predictions[['y_test', 'y_pred', 'ticker', 'y_predb', 'y_recommend','daily return %']][Predictions['ticker'] == ticker]\n",
    "        date_predict=YF_datetime() #can be confusing but yesterday is datetime.now. good thing is to take latest in results +1\n",
    "        date_predict = date_predict.strftime(\"%Y/%m/%d\")\n",
    "        ticker_predicted = results.iloc[-1]['y_pred']  # this is the last row containing result\n",
    "        ticker_recommend = results.iloc[-1]['y_recommend']  # this is the last row containing result\n",
    "        Recommendations.loc[Recommendations['Ticker'] == ticker, 'Predicted for'] = date_predict  # to change content of a cell\n",
    "        Recommendations.loc[Recommendations['Ticker'] == ticker, 'Predicted'] = ticker_predicted\n",
    "        Recommendations.loc[Recommendations['Ticker'] == ticker, 'Recommended'] = ticker_recommend\n",
    "\n",
    "        Predictions.loc[(Predictions['Date']==date_predict) & (Predictions['ticker']==ticker),'y_test']=np.nan\n",
    "        Predictions.loc[(Predictions['Date']==date_predict) & (Predictions['ticker']==ticker),'daily return %']=np.nan\n",
    "           \n",
    "\n",
    "        # print(f\"Prediction for {yesterday+timedelta(1)} -- ticker: {ticker} {'**resultat**'}\\n {resultat.tail()}\\n\\n\")\n",
    "\n",
    "    Results = Predictions[['ticker', 'Date', 'y_test', 'Predicted for', 'y_pred', 'y_recommend','daily return %']]\n",
    "    Results = Results.rename(\n",
    "        columns={'ticker': 'Ticker', 'y_test': 'Observed', 'y_pred': 'Predicted', 'y_recommend': 'Recommended','daily return %':'Daily return %'})\n",
    "\n",
    "\n",
    "    return Results, Recommendations,avg_return\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import os.path\n",
    "import numpy as np\n",
    "from yahoo_fin import stock_info as si\n",
    "#from flask import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sp500():\n",
    "    SP500_tickers = si.tickers_sp500()\n",
    "    delete_blob(\"SP500.json\")\n",
    "    SP500_list = read_create_write_SP500(SP500_tickers, \"SP500.json\")\n",
    "    return f\"JSON was generated  successfully- try again with /ticker/xxx or /details/xxx (where xxx is your list of tickers separated by '-'.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get_sp500()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def check_ticker(ticker):\n",
    "    ticker = ticker.upper()\n",
    "    version=read_config_file()[0]\n",
    "    model_html=read_config_file()[3]\n",
    "    filename_json=\"SP500.json\"\n",
    "    check=delete_then_get_model_from_bucket(filename_json)  # delete local file and copy file from bucket to have fresh one\n",
    "    #file_exists = os.path.exists(\"SP500.json\") # no longer required with blob check above\n",
    "    if check==True:  #file json exist\n",
    "        SP500_list = read_list(filename_json)\n",
    "        SP500_list = np.array(SP500_list)  # we need an array\n",
    "        validation=all([([x] in SP500_list[:,:1]) for x in ticker.split(\"-\")])\n",
    "        # all allow to check if a list o bolean is tru or false . all([true, false, true....etc])\n",
    "        # X in SP50_list, etc.... .... will check if x is in my SP500 list. here all row and column 0\n",
    "        # for x in is selecting each at a time\n",
    "\n",
    "        if validation==True:\n",
    "            Results, Recommendations,avg_return = predict_ticker(ticker)\n",
    "            print(\"avg return\",avg_return)\n",
    "            return Results, Recommendations,avg_return\n",
    "            \n",
    "\n",
    "        else:\n",
    "            return f\"Incorrect ticker, please fix or select another.\"\n",
    "    else:\n",
    "        return f\"JSON does not exists - Please generate JSON  with /get_SP500\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "old file SP500.json deleted locally\n",
      "\n",
      "new file   SP500.json downloaded\n",
      "\n",
      "old file AAPL_model.sav deleted locally\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1\n",
      "*\n",
      "Lasso(alpha=1.0)\n",
      "['^tnx', '^GSPC', 'CL=F']\n",
      "stock-363101\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "file AAPL_model.sav not found in bucket\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model for at least one ticker in ['AAPL']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "old file SP500.json deleted locally\n",
      "\n",
      "new file   SP500.json downloaded\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Input ->years 6\n",
      "Model Input ->ticker list ['AAPL']\n",
      "Model Input->lags 30\n",
      "Model Input ->predict days 15\n",
      "Model Period->yesterday 2022-11-19 09:40:26.240297\n",
      "Model period->train_date_start 2016-12-20\n",
      "Model Period->train_date_last 2022-11-03\n",
      "Model Period->test_date_start 2022-11-04\n",
      "Model Period->test_date_last 2022-11-19\n",
      "Model This is the tickers for our model ['AAPL']\n",
      "Model This is the additional data  we add to the tickers for the model ['^tnx', '^GSPC', 'CL=F']\n",
      "Model VALIDATE - This is the number of training days of the train dataset 2160\n",
      "in module model_Train_Save, additional_data\n",
      "['ACN', 'ADBE', 'ADI', 'ADSK', 'AKAM', 'AMAT', 'AMD', 'ANET', 'ANSS', 'APH', 'AVGO', 'BR', 'CDAY', 'CDNS', 'CDW', 'CRM', 'CSCO', 'CTSH', 'DXC', 'ENPH', 'EPAM', 'FFIV', 'FIS', 'FISV', 'FLT', 'FTNT', 'FTV', 'GLW', 'GRMN', 'HPE', 'HPQ', 'IBM', 'INTC', 'INTU', 'IT', 'JKHY', 'JNPR', 'KEYS', 'KLAC', 'LDOS', 'LRCX', 'MCHP', 'MPWR', 'MSFT', 'MSI', 'MU', 'NLOK', 'NOW', 'NTAP', 'NVDA', 'NXPI', 'ON', 'ORCL', 'PAYC', 'PTC', 'QCOM', 'QRVO', 'SEDG', 'SNPS', 'STX', 'SWKS', 'TDY', 'TEL', 'TER', 'TRMB', 'TXN', 'TYL', 'VRSN', 'WDC', 'ZBRA', '^tnx', '^GSPC', 'CL=F']\n",
      "preprocing module ticker AAPL\n",
      "preprocessing module additional data ['ACN', 'ADBE', 'ADI', 'ADSK', 'AKAM', 'AMAT', 'AMD', 'ANET', 'ANSS', 'APH', 'AVGO', 'BR', 'CDAY', 'CDNS', 'CDW', 'CRM', 'CSCO', 'CTSH', 'DXC', 'ENPH', 'EPAM', 'FFIV', 'FIS', 'FISV', 'FLT', 'FTNT', 'FTV', 'GLW', 'GRMN', 'HPE', 'HPQ', 'IBM', 'INTC', 'INTU', 'IT', 'JKHY', 'JNPR', 'KEYS', 'KLAC', 'LDOS', 'LRCX', 'MCHP', 'MPWR', 'MSFT', 'MSI', 'MU', 'NLOK', 'NOW', 'NTAP', 'NVDA', 'NXPI', 'ON', 'ORCL', 'PAYC', 'PTC', 'QCOM', 'QRVO', 'SEDG', 'SNPS', 'STX', 'SWKS', 'TDY', 'TEL', 'TER', 'TRMB', 'TXN', 'TYL', 'VRSN', 'WDC', 'ZBRA', '^tnx', '^GSPC', 'CL=F']\n",
      "preprocessing module days 2160\n",
      "in module model_Train_Save, print model\n",
      "Lasso()\n",
      "in module model_Train_Save, print ticker\n",
      "AAPL\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/steve/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.422e+03, tolerance: 2.285e+02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "uploading AAPL_model.sav to stock-363101_01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X test shape in module model_predict (11, 2220)\n",
      "module predictions compile\n",
      "y_test_shape in module predictions compile (11,)\n",
      "y_pred_shape in module predictions compile (11,)\n",
      "y_test in module predictions compile [138.38000488 138.91999817 139.5        134.86999512 146.86999512\n",
      " 149.69999695 148.27999878 150.03999329 148.78999329 150.72000122\n",
      " 151.28999329]\n",
      "y_pred in module predictions compile [139.42974625 138.83536893 139.56694879 139.95962195 136.54845913\n",
      " 146.49632198 149.39189803 148.08886487 149.68703976 149.20512087\n",
      " 150.21409064]\n",
      "ticker in module predictions compile AAPL\n",
      "predict_df in module predictions compile         y_test      y_pred ticker\n",
      "0   138.380005  139.429746   AAPL\n",
      "1   138.919998  138.835369   AAPL\n",
      "2   139.500000  139.566949   AAPL\n",
      "3   134.869995  139.959622   AAPL\n",
      "4   146.869995  136.548459   AAPL\n",
      "5   149.699997  146.496322   AAPL\n",
      "6   148.279999  149.391898   AAPL\n",
      "7   150.039993  148.088865   AAPL\n",
      "8   148.789993  149.687040   AAPL\n",
      "9   150.720001  149.205121   AAPL\n",
      "10  151.289993  150.214091   AAPL\n",
      "Accuracy score for *all* is 0.6190476190476191.\n",
      "Input ->years 6\n",
      "Input ->ticker list ['AAPL']\n",
      "Input->lags 30\n",
      "Input ->predict days 15\n",
      "Period->yesterday 2022-11-19 09:40:58.181907\n",
      "period->train_date_start 2022-09-05\n",
      "Period->train_date_last 2022-11-03\n",
      "Period->test_date_start 2022-11-04\n",
      "Period->test_date_last 2022-11-19\n",
      "This is the tickers for our model ['AAPL']\n",
      "This is the additional data  we add to the tickers for the model ['^tnx', '^GSPC', 'CL=F']\n",
      "['ACN', 'ADBE', 'ADI', 'ADSK', 'AKAM', 'AMAT', 'AMD', 'ANET', 'ANSS', 'APH', 'AVGO', 'BR', 'CDAY', 'CDNS', 'CDW', 'CRM', 'CSCO', 'CTSH', 'DXC', 'ENPH', 'EPAM', 'FFIV', 'FIS', 'FISV', 'FLT', 'FTNT', 'FTV', 'GLW', 'GRMN', 'HPE', 'HPQ', 'IBM', 'INTC', 'INTU', 'IT', 'JKHY', 'JNPR', 'KEYS', 'KLAC', 'LDOS', 'LRCX', 'MCHP', 'MPWR', 'MSFT', 'MSI', 'MU', 'NLOK', 'NOW', 'NTAP', 'NVDA', 'NXPI', 'ON', 'ORCL', 'PAYC', 'PTC', 'QCOM', 'QRVO', 'SEDG', 'SNPS', 'STX', 'SWKS', 'TDY', 'TEL', 'TER', 'TRMB', 'TXN', 'TYL', 'VRSN', 'WDC', 'ZBRA', '^tnx', '^GSPC', 'CL=F']\n",
      "preprocing module ticker AAPL\n",
      "preprocessing module additional data ['ACN', 'ADBE', 'ADI', 'ADSK', 'AKAM', 'AMAT', 'AMD', 'ANET', 'ANSS', 'APH', 'AVGO', 'BR', 'CDAY', 'CDNS', 'CDW', 'CRM', 'CSCO', 'CTSH', 'DXC', 'ENPH', 'EPAM', 'FFIV', 'FIS', 'FISV', 'FLT', 'FTNT', 'FTV', 'GLW', 'GRMN', 'HPE', 'HPQ', 'IBM', 'INTC', 'INTU', 'IT', 'JKHY', 'JNPR', 'KEYS', 'KLAC', 'LDOS', 'LRCX', 'MCHP', 'MPWR', 'MSFT', 'MSI', 'MU', 'NLOK', 'NOW', 'NTAP', 'NVDA', 'NXPI', 'ON', 'ORCL', 'PAYC', 'PTC', 'QCOM', 'QRVO', 'SEDG', 'SNPS', 'STX', 'SWKS', 'TDY', 'TEL', 'TER', 'TRMB', 'TXN', 'TYL', 'VRSN', 'WDC', 'ZBRA', '^tnx', '^GSPC', 'CL=F']\n",
      "preprocessing module days 75.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_192499/3849885330.py:103: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  predictions = predictions.append(temp_pred, ignore_index=True)  # this is to store in the master pandas list\n",
      "/tmp/ipykernel_192499/832641991.py:29: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df_lagged = df_lagged.append(df_lagged_ticker)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predict df_tomorrow shape before creating predict set\n",
      "(54, 74)\n",
      "features shape in module create_predict_set\n",
      "(54, 74)\n",
      "df=features+additional data in module create_predict_set\n",
      "                  AAPL         ACN        ADBE         ADI        ADSK  \\\n",
      "2022-09-06  154.529999  283.459991  368.299988  148.229996  198.179993   \n",
      "2022-09-07  155.960007  286.760010  379.720001  150.699997  206.190002   \n",
      "2022-09-08  154.460007  287.959991  383.630005  152.130005  209.820007   \n",
      "2022-09-09  157.369995  290.549988  394.779999  154.179993  211.679993   \n",
      "2022-09-12  163.429993  295.260010  396.359985  155.649994  215.160004   \n",
      "2022-09-13  153.839996  281.519989  368.390015  148.250000  208.339996   \n",
      "2022-09-14  155.309998  278.529999  371.519989  150.250000  208.520004   \n",
      "2022-09-15  152.369995  273.859985  309.130005  147.869995  201.300003   \n",
      "2022-09-16  150.699997  272.679993  299.500000  149.309998  194.970001   \n",
      "2022-09-19  154.479996  274.980011  296.059998  149.610001  196.889999   \n",
      "2022-09-20  156.899994  270.239990  291.059998  149.779999  194.970001   \n",
      "2022-09-21  153.720001  265.420013  286.299988  148.440002  192.419998   \n",
      "2022-09-22  152.740005  262.320007  287.059998  145.339996  187.149994   \n",
      "2022-09-23  150.429993  259.980011  284.559998  141.919998  184.559998   \n",
      "2022-09-26  150.770004  257.540009  276.959991  140.820007  183.990005   \n",
      "2022-09-27  151.759995  256.339996  277.570007  141.809998  187.960007   \n",
      "2022-09-28  149.839996  261.929993  281.399994  144.580002  190.979996   \n",
      "2022-09-29  142.479996  258.269989  278.250000  141.990005  189.460007   \n",
      "2022-09-30  138.199997  257.299988  275.200012  139.339996  186.800003   \n",
      "2022-10-03  142.449997  264.890015  285.239990  145.130005  192.460007   \n",
      "2022-10-04  146.100006  274.309998  294.970001  150.850006  199.990005   \n",
      "2022-10-05  146.399994  274.339996  297.380005  151.889999  204.529999   \n",
      "2022-10-06  145.429993  269.470001  298.410004  150.970001  205.869995   \n",
      "2022-10-07  140.089996  259.709991  288.769989  144.919998  194.740005   \n",
      "2022-10-10  140.419998  257.850006  285.720001  140.899994  191.029999   \n",
      "2022-10-11  138.979996  252.979996  284.829987  138.800003  191.029999   \n",
      "2022-10-12  138.339996  250.070007  286.149994  138.589996  193.639999   \n",
      "2022-10-13  142.990005  257.459991  294.739990  142.750000  193.850006   \n",
      "2022-10-14  138.380005  252.720001  287.940002  136.729996  189.809998   \n",
      "2022-10-17  142.410004  262.220001  293.500000  139.119995  198.699997   \n",
      "2022-10-18  143.750000  264.049988  292.980011  141.100006  200.699997   \n",
      "2022-10-19  143.860001  264.059998  299.829987  141.330002  197.020004   \n",
      "2022-10-20  143.389999  261.779999  302.380005  142.080002  197.830002   \n",
      "2022-10-21  147.270004  269.570007  306.369995  146.589996  201.389999   \n",
      "2022-10-24  149.449997  275.309998  316.220001  144.529999  207.089996   \n",
      "2022-10-25  152.339996  280.609985  323.790009  146.369995  215.720001   \n",
      "2022-10-26  149.350006  279.869995  320.480011  141.380005  214.559998   \n",
      "2022-10-27  144.800003  278.839996  318.649994  140.679993  210.149994   \n",
      "2022-10-28  155.740005  287.779999  325.679993  144.880005  216.389999   \n",
      "2022-10-31  153.339996  283.899994  318.500000  142.619995  214.300003   \n",
      "2022-11-01  150.649994  281.470001  316.019989  144.699997  214.050003   \n",
      "2022-11-02  145.029999  272.450012  301.220001  141.240005  199.380005   \n",
      "2022-11-03  138.880005  256.880005  285.929993  138.020004  194.220001   \n",
      "2022-11-04  138.380005  261.160004  285.750000  144.289993  193.690002   \n",
      "2022-11-07  138.919998  269.070007  299.540009  148.929993  194.880005   \n",
      "2022-11-08  139.500000  269.029999  302.170013  151.039993  199.009995   \n",
      "2022-11-09  134.869995  266.440002  298.869995  148.229996  194.610001   \n",
      "2022-11-10  146.869995  287.019989  329.950012  160.369995  222.960007   \n",
      "2022-11-11  149.699997  290.089996  341.149994  164.059998  228.800003   \n",
      "2022-11-14  148.279999  288.910004  340.369995  161.259995  223.309998   \n",
      "2022-11-15  150.039993  292.649994  345.959991  165.169998  232.300003   \n",
      "2022-11-16  148.789993  291.510010  338.410004  161.270004  221.139999   \n",
      "2022-11-17  150.720001  287.140015  337.829987  161.470001  214.679993   \n",
      "2022-11-18  151.289993  286.500000  330.859985  161.850006  210.369995   \n",
      "\n",
      "                 AKAM        AMAT        AMD        ANET        ANSS  ...  \\\n",
      "2022-09-06  88.650002   90.290001  78.720001  117.550003  241.259995  ...   \n",
      "2022-09-07  90.290001   91.940002  79.610001  120.269997  249.770004  ...   \n",
      "2022-09-08  90.220001   93.790001  82.779999  122.779999  254.880005  ...   \n",
      "2022-09-09  91.690002   96.510002  85.449997  124.410004  258.799988  ...   \n",
      "2022-09-12  93.110001   96.300003  84.639999  124.750000  261.420013  ...   \n",
      "2022-09-13  89.660004   90.389999  77.029999  119.919998  248.759995  ...   \n",
      "2022-09-14  89.389999   90.639999  77.449997  122.260002  247.000000  ...   \n",
      "2022-09-15  88.239998   88.919998  76.660004  116.940002  241.449997  ...   \n",
      "2022-09-16  87.160004   88.870003  76.510002  115.730003  240.740005  ...   \n",
      "2022-09-19  88.559998   89.720001  76.769997  114.940002  241.429993  ...   \n",
      "2022-09-20  85.930000   88.120003  75.250000  114.070000  239.350006  ...   \n",
      "2022-09-21  83.800003   87.089996  74.480003  113.820000  235.419998  ...   \n",
      "2022-09-22  82.089996   85.040001  69.500000  112.550003  232.039993  ...   \n",
      "2022-09-23  81.110001   84.290001  67.959999  109.970001  229.539993  ...   \n",
      "2022-09-26  80.709999   82.940002  66.300003  109.099998  229.720001  ...   \n",
      "2022-09-27  80.919998   84.150002  67.169998  110.919998  227.570007  ...   \n",
      "2022-09-28  82.250000   86.000000  68.360001  116.690002  232.190002  ...   \n",
      "2022-09-29  80.500000   84.419998  64.139999  114.750000  227.529999  ...   \n",
      "2022-09-30  80.320000   81.930000  63.360001  112.889999  221.699997  ...   \n",
      "2022-10-03  83.839996   86.250000  66.110001  115.830002  227.970001  ...   \n",
      "2022-10-04  87.160004   89.410004  67.900002  120.809998  233.210007  ...   \n",
      "2022-10-05  85.379997   89.220001  67.940002  121.349998  233.839996  ...   \n",
      "2022-10-06  84.440002   88.120003  67.849998  121.900002  232.229996  ...   \n",
      "2022-10-07  82.080002   82.599998  58.439999  116.410004  218.300003  ...   \n",
      "2022-10-10  79.879997   79.190002  57.810001  109.480003  209.110001  ...   \n",
      "2022-10-11  78.059998   76.300003  57.630001  107.050003  200.330002  ...   \n",
      "2022-10-12  78.410004   76.010002  57.849998  103.650002  201.949997  ...   \n",
      "2022-10-13  80.459999   79.419998  58.939999  103.910004  205.669998  ...   \n",
      "2022-10-14  79.980003   74.820000  55.939999  100.370003  203.210007  ...   \n",
      "2022-10-17  82.540001   74.410004  57.959999  104.559998  211.800003  ...   \n",
      "2022-10-18  84.379997   75.230003  57.919998  106.279999  216.949997  ...   \n",
      "2022-10-19  82.940002   77.260002  57.230000  105.110001  214.190002  ...   \n",
      "2022-10-20  84.160004   78.660004  57.770000  105.639999  210.509995  ...   \n",
      "2022-10-21  85.879997   82.419998  58.820000  110.519997  214.149994  ...   \n",
      "2022-10-24  86.389999   84.940002  58.700001  110.739998  216.070007  ...   \n",
      "2022-10-25  88.449997   87.529999  61.470001  112.629997  219.500000  ...   \n",
      "2022-10-26  86.760002   88.139999  59.730000  108.970001  218.160004  ...   \n",
      "2022-10-27  87.580002   86.540001  58.599998  119.129997  216.479996  ...   \n",
      "2022-10-28  89.209999   89.720001  62.009998  121.470001  220.889999  ...   \n",
      "2022-10-31  88.330002   88.290001  60.060001  120.860001  221.160004  ...   \n",
      "2022-11-01  88.099998   89.790001  59.660000  127.709999  219.240005  ...   \n",
      "2022-11-02  85.410004   87.760002  58.630001  125.040001  209.449997  ...   \n",
      "2022-11-03  83.360001   86.300003  60.110001  122.250000  212.059998  ...   \n",
      "2022-11-04  84.099998   91.699997  62.189999  131.070007  213.910004  ...   \n",
      "2022-11-07  83.860001   95.040001  63.080002  130.580002  221.389999  ...   \n",
      "2022-11-08  83.889999   97.459999  63.849998  129.110001  228.600006  ...   \n",
      "2022-11-09  89.080002   94.379997  59.919998  122.919998  225.600006  ...   \n",
      "2022-11-10  92.919998  104.790001  68.470001  126.980003  249.190002  ...   \n",
      "2022-11-11  93.349998  110.529999  72.370003  128.550003  255.710007  ...   \n",
      "2022-11-14  91.529999  107.610001  73.529999  131.199997  251.850006  ...   \n",
      "2022-11-15  92.459999  110.459999  76.370003  132.649994  254.210007  ...   \n",
      "2022-11-16  90.820000  104.220001  72.699997  130.690002  252.919998  ...   \n",
      "2022-11-17  90.790001  104.449997  73.900002  132.059998  243.919998  ...   \n",
      "2022-11-18  90.949997  104.699997  73.570000  135.479996  243.759995  ...   \n",
      "\n",
      "                  TER       TRMB         TXN         TYL        VRSN  \\\n",
      "2022-09-06  82.919998  60.220001  163.100006  363.589996  182.289993   \n",
      "2022-09-07  85.019997  62.290001  165.820007  373.709991  186.529999   \n",
      "2022-09-08  86.139999  61.389999  168.410004  380.239990  187.330002   \n",
      "2022-09-09  88.839996  63.080002  170.740005  385.190002  190.559998   \n",
      "2022-09-12  87.379997  64.430000  170.580002  386.859985  189.039993   \n",
      "2022-09-13  83.160004  61.680000  162.649994  364.559998  178.750000   \n",
      "2022-09-14  83.639999  61.220001  165.259995  368.540009  178.339996   \n",
      "2022-09-15  82.349998  59.419998  162.669998  362.760010  174.619995   \n",
      "2022-09-16  82.099998  58.680000  165.259995  360.980011  175.029999   \n",
      "2022-09-19  82.489998  58.919998  166.250000  362.200012  176.270004   \n",
      "2022-09-20  80.839996  58.459999  166.059998  353.859985  174.729996   \n",
      "2022-09-21  80.970001  57.369999  163.300003  351.299988  176.720001   \n",
      "2022-09-22  79.790001  56.790001  162.619995  342.489990  174.639999   \n",
      "2022-09-23  79.190002  56.380001  161.289993  341.119995  173.699997   \n",
      "2022-09-26  77.209999  55.439999  160.460007  342.750000  173.380005   \n",
      "2022-09-27  78.370003  55.250000  160.710007  341.750000  172.529999   \n",
      "2022-09-28  78.949997  56.919998  162.800003  347.079987  177.289993   \n",
      "2022-09-29  77.169998  55.259998  158.449997  347.700012  176.169998   \n",
      "2022-09-30  75.150002  54.270000  154.779999  347.500000  173.699997   \n",
      "2022-10-03  78.690002  56.220001  159.839996  349.200012  179.300003   \n",
      "2022-10-04  81.669998  58.790001  165.149994  362.890015  183.460007   \n",
      "2022-10-05  81.949997  57.610001  167.800003  367.269989  183.869995   \n",
      "2022-10-06  82.010002  57.730000  166.539993  366.000000  182.080002   \n",
      "2022-10-07  77.330002  54.610001  159.279999  352.950012  177.860001   \n",
      "2022-10-10  76.110001  53.820000  156.789993  343.540009  176.990005   \n",
      "2022-10-11  74.019997  52.490002  153.449997  329.760010  174.889999   \n",
      "2022-10-12  72.000000  52.000000  151.550003  328.149994  175.679993   \n",
      "2022-10-13  75.110001  53.480000  154.339996  324.750000  176.850006   \n",
      "2022-10-14  71.370003  52.299999  148.339996  315.829987  174.080002   \n",
      "2022-10-17  71.510002  54.500000  150.990005  326.250000  179.550003   \n",
      "2022-10-18  72.779999  55.759998  151.509995  339.390015  182.889999   \n",
      "2022-10-19  73.180000  54.830002  152.649994  336.369995  182.100006   \n",
      "2022-10-20  73.949997  53.490002  153.720001  334.559998  178.669998   \n",
      "2022-10-21  77.269997  55.880001  159.720001  335.950012  179.149994   \n",
      "2022-10-24  77.459999  56.099998  161.649994  337.980011  182.199997   \n",
      "2022-10-25  79.650002  58.150002  162.160004  346.059998  187.820007   \n",
      "2022-10-26  82.250000  58.029999  157.869995  343.420013  184.990005   \n",
      "2022-10-27  79.860001  58.240002  156.759995  333.529999  185.789993   \n",
      "2022-10-28  83.349998  60.240002  161.360001  330.500000  203.369995   \n",
      "2022-10-31  81.349998  60.160000  160.630005  323.329987  200.460007   \n",
      "2022-11-01  82.220001  59.889999  162.899994  314.239990  196.970001   \n",
      "2022-11-02  79.440002  53.650002  158.490005  296.559998  186.610001   \n",
      "2022-11-03  80.089996  52.060001  156.520004  292.630005  180.190002   \n",
      "2022-11-04  84.720001  53.790001  162.649994  287.929993  176.690002   \n",
      "2022-11-07  85.540001  57.200001  165.690002  289.470001  177.880005   \n",
      "2022-11-08  86.449997  54.650002  168.110001  296.140015  180.729996   \n",
      "2022-11-09  84.370003  54.049999  164.990005  292.019989  178.320007   \n",
      "2022-11-10  93.330002  59.160000  174.690002  309.709991  191.119995   \n",
      "2022-11-11  96.309998  62.000000  179.490005  324.609985  192.389999   \n",
      "2022-11-14  94.139999  59.299999  177.440002  320.350006  190.449997   \n",
      "2022-11-15  96.589996  59.830002  177.570007  337.200012  195.529999   \n",
      "2022-11-16  91.480003  58.680000  173.460007  335.640015  197.550003   \n",
      "2022-11-17  92.510002  57.770000  175.360001  320.549988  195.550003   \n",
      "2022-11-18  92.900002  57.990002  175.179993  313.450012  194.250000   \n",
      "\n",
      "                  WDC        ZBRA   ^tnx        ^GSPC       CL=F  \n",
      "2022-09-06  40.970001  289.929993  3.340  3908.189941  86.879997  \n",
      "2022-09-07  41.450001  295.579987  3.265  3979.870117  81.940002  \n",
      "2022-09-08  42.410000  298.299988  3.292  4006.179932  83.540001  \n",
      "2022-09-09  43.799999  307.839996  3.321  4067.360107  86.790001  \n",
      "2022-09-12  43.270000  311.850006  3.362  4110.410156  87.779999  \n",
      "2022-09-13  39.320000  293.500000  3.422  3932.689941  87.309998  \n",
      "2022-09-14  38.330002  295.970001  3.412  3946.010010  88.480003  \n",
      "2022-09-15  37.770000  296.200012  3.459  3901.350098  85.099998  \n",
      "2022-09-16  37.220001  288.519989  3.448  3873.330078  85.110001  \n",
      "2022-09-19  36.619999  291.209991  3.490  3899.889893  85.730003  \n",
      "2022-09-20  35.540001  285.649994  3.571  3855.929932  84.449997  \n",
      "2022-09-21  34.779999  285.079987  3.510  3789.929932  82.940002  \n",
      "2022-09-22  33.820000  272.940002  3.708  3757.989990  83.489998  \n",
      "2022-09-23  33.840000  268.040009  3.697  3693.229980  78.739998  \n",
      "2022-09-26  32.400002  265.859985  3.878  3655.040039  76.709999  \n",
      "2022-09-27  32.720001  264.950012  3.964  3647.290039  78.500000  \n",
      "2022-09-28  33.150002  271.350006  3.705  3719.040039  82.150002  \n",
      "2022-09-29  32.180000  265.519989  3.747  3640.469971  81.230003  \n",
      "2022-09-30  32.549999  262.010010  3.804  3585.620117  79.489998  \n",
      "2022-10-03  34.049999  272.089996  3.651  3678.429932  83.629997  \n",
      "2022-10-04  36.459999  281.649994  3.617  3790.929932  86.519997  \n",
      "2022-10-05  37.169998  281.880005  3.759  3783.280029  87.760002  \n",
      "2022-10-06  37.060001  280.350006  3.826  3744.520020  88.449997  \n",
      "2022-10-07  35.740002  266.679993  3.883  3639.659912  92.639999  \n",
      "2022-10-10  34.660000  258.600006  3.888  3612.389893  91.129997  \n",
      "2022-10-11  34.820000  250.009995  3.939  3588.840088  89.349998  \n",
      "2022-10-12  34.279999  261.029999  3.902  3577.030029  87.269997  \n",
      "2022-10-13  35.099998  268.269989  3.952  3669.909912  89.110001  \n",
      "2022-10-14  33.720001  255.610001  4.010  3583.070068  85.610001  \n",
      "2022-10-17  33.889999  261.640015  4.015  3677.949951  85.459999  \n",
      "2022-10-18  33.610001  263.890015  3.998  3719.979980  82.820000  \n",
      "2022-10-19  33.360001  260.459991  4.127  3695.159912  85.550003  \n",
      "2022-10-20  33.470001  256.230011  4.226  3665.780029  85.980003  \n",
      "2022-10-21  34.860001  263.040009  4.213  3752.750000  85.050003  \n",
      "2022-10-24  34.919998  265.720001  4.234  3797.340088  84.580002  \n",
      "2022-10-25  35.290001  274.579987  4.108  3859.110107  85.320000  \n",
      "2022-10-26  35.279999  273.179993  4.015  3830.600098  87.910004  \n",
      "2022-10-27  34.340000  275.290009  3.937  3807.300049  89.080002  \n",
      "2022-10-28  35.500000  288.000000  4.010  3901.060059  87.900002  \n",
      "2022-10-31  34.369999  283.220001  4.077  3871.979980  86.529999  \n",
      "2022-11-01  35.700001  238.300003  4.052  3856.100098  88.370003  \n",
      "2022-11-02  34.580002  236.029999  4.059  3759.689941  90.000000  \n",
      "2022-11-03  33.709999  227.320007  4.124  3719.889893  88.169998  \n",
      "2022-11-04  35.439999  230.559998  4.156  3770.550049  92.610001  \n",
      "2022-11-07  36.619999  236.360001  4.214  3806.800049  91.790001  \n",
      "2022-11-08  36.520000  234.649994  4.126  3828.110107  88.910004  \n",
      "2022-11-09  34.349998  226.880005  4.151  3748.570068  85.830002  \n",
      "2022-11-10  37.750000  253.419998  3.829  3956.370117  86.470001  \n",
      "2022-11-11  39.750000  263.380005  3.813  3992.929932  88.959999  \n",
      "2022-11-14  39.599998  251.470001  3.865  3957.250000  85.870003  \n",
      "2022-11-15  39.459999  261.589996  3.799  3991.729980  86.919998  \n",
      "2022-11-16  36.419998  258.429993  3.692  3958.790039  85.589996  \n",
      "2022-11-17  36.529999  255.110001  3.775  3946.560059  81.639999  \n",
      "2022-11-18  36.860001  256.100006  3.818  3965.340088  80.080002  \n",
      "\n",
      "[54 rows x 74 columns]\n",
      "2022-11-18 00:00:00\n",
      "<class 'pandas._libs.tslibs.timestamps.Timestamp'>\n",
      "ici df lagged Empty DataFrame\n",
      "Columns: []\n",
      "Index: []\n",
      "df_lagged of features+additional data before dropna in module create_predict_set\n",
      "(55, 2220)\n",
      "             AAPL_lag1    ACN_lag1   ADBE_lag1    ADI_lag1   ADSK_lag1  \\\n",
      "2022-09-06         NaN         NaN         NaN         NaN         NaN   \n",
      "2022-09-07  154.529999  283.459991  368.299988  148.229996  198.179993   \n",
      "2022-09-08  155.960007  286.760010  379.720001  150.699997  206.190002   \n",
      "2022-09-09  154.460007  287.959991  383.630005  152.130005  209.820007   \n",
      "2022-09-12  157.369995  290.549988  394.779999  154.179993  211.679993   \n",
      "2022-09-13  163.429993  295.260010  396.359985  155.649994  215.160004   \n",
      "2022-09-14  153.839996  281.519989  368.390015  148.250000  208.339996   \n",
      "2022-09-15  155.309998  278.529999  371.519989  150.250000  208.520004   \n",
      "2022-09-16  152.369995  273.859985  309.130005  147.869995  201.300003   \n",
      "2022-09-19  150.699997  272.679993  299.500000  149.309998  194.970001   \n",
      "2022-09-20  154.479996  274.980011  296.059998  149.610001  196.889999   \n",
      "2022-09-21  156.899994  270.239990  291.059998  149.779999  194.970001   \n",
      "2022-09-22  153.720001  265.420013  286.299988  148.440002  192.419998   \n",
      "2022-09-23  152.740005  262.320007  287.059998  145.339996  187.149994   \n",
      "2022-09-26  150.429993  259.980011  284.559998  141.919998  184.559998   \n",
      "2022-09-27  150.770004  257.540009  276.959991  140.820007  183.990005   \n",
      "2022-09-28  151.759995  256.339996  277.570007  141.809998  187.960007   \n",
      "2022-09-29  149.839996  261.929993  281.399994  144.580002  190.979996   \n",
      "2022-09-30  142.479996  258.269989  278.250000  141.990005  189.460007   \n",
      "2022-10-03  138.199997  257.299988  275.200012  139.339996  186.800003   \n",
      "2022-10-04  142.449997  264.890015  285.239990  145.130005  192.460007   \n",
      "2022-10-05  146.100006  274.309998  294.970001  150.850006  199.990005   \n",
      "2022-10-06  146.399994  274.339996  297.380005  151.889999  204.529999   \n",
      "2022-10-07  145.429993  269.470001  298.410004  150.970001  205.869995   \n",
      "2022-10-10  140.089996  259.709991  288.769989  144.919998  194.740005   \n",
      "2022-10-11  140.419998  257.850006  285.720001  140.899994  191.029999   \n",
      "2022-10-12  138.979996  252.979996  284.829987  138.800003  191.029999   \n",
      "2022-10-13  138.339996  250.070007  286.149994  138.589996  193.639999   \n",
      "2022-10-14  142.990005  257.459991  294.739990  142.750000  193.850006   \n",
      "2022-10-17  138.380005  252.720001  287.940002  136.729996  189.809998   \n",
      "2022-10-18  142.410004  262.220001  293.500000  139.119995  198.699997   \n",
      "2022-10-19  143.750000  264.049988  292.980011  141.100006  200.699997   \n",
      "2022-10-20  143.860001  264.059998  299.829987  141.330002  197.020004   \n",
      "2022-10-21  143.389999  261.779999  302.380005  142.080002  197.830002   \n",
      "2022-10-24  147.270004  269.570007  306.369995  146.589996  201.389999   \n",
      "2022-10-25  149.449997  275.309998  316.220001  144.529999  207.089996   \n",
      "2022-10-26  152.339996  280.609985  323.790009  146.369995  215.720001   \n",
      "2022-10-27  149.350006  279.869995  320.480011  141.380005  214.559998   \n",
      "2022-10-28  144.800003  278.839996  318.649994  140.679993  210.149994   \n",
      "2022-10-31  155.740005  287.779999  325.679993  144.880005  216.389999   \n",
      "2022-11-01  153.339996  283.899994  318.500000  142.619995  214.300003   \n",
      "2022-11-02  150.649994  281.470001  316.019989  144.699997  214.050003   \n",
      "2022-11-03  145.029999  272.450012  301.220001  141.240005  199.380005   \n",
      "2022-11-04  138.880005  256.880005  285.929993  138.020004  194.220001   \n",
      "2022-11-07  138.380005  261.160004  285.750000  144.289993  193.690002   \n",
      "2022-11-08  138.919998  269.070007  299.540009  148.929993  194.880005   \n",
      "2022-11-09  139.500000  269.029999  302.170013  151.039993  199.009995   \n",
      "2022-11-10  134.869995  266.440002  298.869995  148.229996  194.610001   \n",
      "2022-11-11  146.869995  287.019989  329.950012  160.369995  222.960007   \n",
      "2022-11-14  149.699997  290.089996  341.149994  164.059998  228.800003   \n",
      "2022-11-15  148.279999  288.910004  340.369995  161.259995  223.309998   \n",
      "2022-11-16  150.039993  292.649994  345.959991  165.169998  232.300003   \n",
      "2022-11-17  148.789993  291.510010  338.410004  161.270004  221.139999   \n",
      "2022-11-18  150.720001  287.140015  337.829987  161.470001  214.679993   \n",
      "2022-11-19  151.289993  286.500000  330.859985  161.850006  210.369995   \n",
      "\n",
      "            AKAM_lag1   AMAT_lag1   AMD_lag1   ANET_lag1   ANSS_lag1  ...  \\\n",
      "2022-09-06        NaN         NaN        NaN         NaN         NaN  ...   \n",
      "2022-09-07  88.650002   90.290001  78.720001  117.550003  241.259995  ...   \n",
      "2022-09-08  90.290001   91.940002  79.610001  120.269997  249.770004  ...   \n",
      "2022-09-09  90.220001   93.790001  82.779999  122.779999  254.880005  ...   \n",
      "2022-09-12  91.690002   96.510002  85.449997  124.410004  258.799988  ...   \n",
      "2022-09-13  93.110001   96.300003  84.639999  124.750000  261.420013  ...   \n",
      "2022-09-14  89.660004   90.389999  77.029999  119.919998  248.759995  ...   \n",
      "2022-09-15  89.389999   90.639999  77.449997  122.260002  247.000000  ...   \n",
      "2022-09-16  88.239998   88.919998  76.660004  116.940002  241.449997  ...   \n",
      "2022-09-19  87.160004   88.870003  76.510002  115.730003  240.740005  ...   \n",
      "2022-09-20  88.559998   89.720001  76.769997  114.940002  241.429993  ...   \n",
      "2022-09-21  85.930000   88.120003  75.250000  114.070000  239.350006  ...   \n",
      "2022-09-22  83.800003   87.089996  74.480003  113.820000  235.419998  ...   \n",
      "2022-09-23  82.089996   85.040001  69.500000  112.550003  232.039993  ...   \n",
      "2022-09-26  81.110001   84.290001  67.959999  109.970001  229.539993  ...   \n",
      "2022-09-27  80.709999   82.940002  66.300003  109.099998  229.720001  ...   \n",
      "2022-09-28  80.919998   84.150002  67.169998  110.919998  227.570007  ...   \n",
      "2022-09-29  82.250000   86.000000  68.360001  116.690002  232.190002  ...   \n",
      "2022-09-30  80.500000   84.419998  64.139999  114.750000  227.529999  ...   \n",
      "2022-10-03  80.320000   81.930000  63.360001  112.889999  221.699997  ...   \n",
      "2022-10-04  83.839996   86.250000  66.110001  115.830002  227.970001  ...   \n",
      "2022-10-05  87.160004   89.410004  67.900002  120.809998  233.210007  ...   \n",
      "2022-10-06  85.379997   89.220001  67.940002  121.349998  233.839996  ...   \n",
      "2022-10-07  84.440002   88.120003  67.849998  121.900002  232.229996  ...   \n",
      "2022-10-10  82.080002   82.599998  58.439999  116.410004  218.300003  ...   \n",
      "2022-10-11  79.879997   79.190002  57.810001  109.480003  209.110001  ...   \n",
      "2022-10-12  78.059998   76.300003  57.630001  107.050003  200.330002  ...   \n",
      "2022-10-13  78.410004   76.010002  57.849998  103.650002  201.949997  ...   \n",
      "2022-10-14  80.459999   79.419998  58.939999  103.910004  205.669998  ...   \n",
      "2022-10-17  79.980003   74.820000  55.939999  100.370003  203.210007  ...   \n",
      "2022-10-18  82.540001   74.410004  57.959999  104.559998  211.800003  ...   \n",
      "2022-10-19  84.379997   75.230003  57.919998  106.279999  216.949997  ...   \n",
      "2022-10-20  82.940002   77.260002  57.230000  105.110001  214.190002  ...   \n",
      "2022-10-21  84.160004   78.660004  57.770000  105.639999  210.509995  ...   \n",
      "2022-10-24  85.879997   82.419998  58.820000  110.519997  214.149994  ...   \n",
      "2022-10-25  86.389999   84.940002  58.700001  110.739998  216.070007  ...   \n",
      "2022-10-26  88.449997   87.529999  61.470001  112.629997  219.500000  ...   \n",
      "2022-10-27  86.760002   88.139999  59.730000  108.970001  218.160004  ...   \n",
      "2022-10-28  87.580002   86.540001  58.599998  119.129997  216.479996  ...   \n",
      "2022-10-31  89.209999   89.720001  62.009998  121.470001  220.889999  ...   \n",
      "2022-11-01  88.330002   88.290001  60.060001  120.860001  221.160004  ...   \n",
      "2022-11-02  88.099998   89.790001  59.660000  127.709999  219.240005  ...   \n",
      "2022-11-03  85.410004   87.760002  58.630001  125.040001  209.449997  ...   \n",
      "2022-11-04  83.360001   86.300003  60.110001  122.250000  212.059998  ...   \n",
      "2022-11-07  84.099998   91.699997  62.189999  131.070007  213.910004  ...   \n",
      "2022-11-08  83.860001   95.040001  63.080002  130.580002  221.389999  ...   \n",
      "2022-11-09  83.889999   97.459999  63.849998  129.110001  228.600006  ...   \n",
      "2022-11-10  89.080002   94.379997  59.919998  122.919998  225.600006  ...   \n",
      "2022-11-11  92.919998  104.790001  68.470001  126.980003  249.190002  ...   \n",
      "2022-11-14  93.349998  110.529999  72.370003  128.550003  255.710007  ...   \n",
      "2022-11-15  91.529999  107.610001  73.529999  131.199997  251.850006  ...   \n",
      "2022-11-16  92.459999  110.459999  76.370003  132.649994  254.210007  ...   \n",
      "2022-11-17  90.820000  104.220001  72.699997  130.690002  252.919998  ...   \n",
      "2022-11-18  90.790001  104.449997  73.900002  132.059998  243.919998  ...   \n",
      "2022-11-19  90.949997  104.699997  73.570000  135.479996  243.759995  ...   \n",
      "\n",
      "            TER_lag30  TRMB_lag30   TXN_lag30   TYL_lag30  VRSN_lag30  \\\n",
      "2022-09-06        NaN         NaN         NaN         NaN         NaN   \n",
      "2022-09-07        NaN         NaN         NaN         NaN         NaN   \n",
      "2022-09-08        NaN         NaN         NaN         NaN         NaN   \n",
      "2022-09-09        NaN         NaN         NaN         NaN         NaN   \n",
      "2022-09-12        NaN         NaN         NaN         NaN         NaN   \n",
      "2022-09-13        NaN         NaN         NaN         NaN         NaN   \n",
      "2022-09-14        NaN         NaN         NaN         NaN         NaN   \n",
      "2022-09-15        NaN         NaN         NaN         NaN         NaN   \n",
      "2022-09-16        NaN         NaN         NaN         NaN         NaN   \n",
      "2022-09-19        NaN         NaN         NaN         NaN         NaN   \n",
      "2022-09-20        NaN         NaN         NaN         NaN         NaN   \n",
      "2022-09-21        NaN         NaN         NaN         NaN         NaN   \n",
      "2022-09-22        NaN         NaN         NaN         NaN         NaN   \n",
      "2022-09-23        NaN         NaN         NaN         NaN         NaN   \n",
      "2022-09-26        NaN         NaN         NaN         NaN         NaN   \n",
      "2022-09-27        NaN         NaN         NaN         NaN         NaN   \n",
      "2022-09-28        NaN         NaN         NaN         NaN         NaN   \n",
      "2022-09-29        NaN         NaN         NaN         NaN         NaN   \n",
      "2022-09-30        NaN         NaN         NaN         NaN         NaN   \n",
      "2022-10-03        NaN         NaN         NaN         NaN         NaN   \n",
      "2022-10-04        NaN         NaN         NaN         NaN         NaN   \n",
      "2022-10-05        NaN         NaN         NaN         NaN         NaN   \n",
      "2022-10-06        NaN         NaN         NaN         NaN         NaN   \n",
      "2022-10-07        NaN         NaN         NaN         NaN         NaN   \n",
      "2022-10-10        NaN         NaN         NaN         NaN         NaN   \n",
      "2022-10-11        NaN         NaN         NaN         NaN         NaN   \n",
      "2022-10-12        NaN         NaN         NaN         NaN         NaN   \n",
      "2022-10-13        NaN         NaN         NaN         NaN         NaN   \n",
      "2022-10-14        NaN         NaN         NaN         NaN         NaN   \n",
      "2022-10-17        NaN         NaN         NaN         NaN         NaN   \n",
      "2022-10-18  82.919998   60.220001  163.100006  363.589996  182.289993   \n",
      "2022-10-19  85.019997   62.290001  165.820007  373.709991  186.529999   \n",
      "2022-10-20  86.139999   61.389999  168.410004  380.239990  187.330002   \n",
      "2022-10-21  88.839996   63.080002  170.740005  385.190002  190.559998   \n",
      "2022-10-24  87.379997   64.430000  170.580002  386.859985  189.039993   \n",
      "2022-10-25  83.160004   61.680000  162.649994  364.559998  178.750000   \n",
      "2022-10-26  83.639999   61.220001  165.259995  368.540009  178.339996   \n",
      "2022-10-27  82.349998   59.419998  162.669998  362.760010  174.619995   \n",
      "2022-10-28  82.099998   58.680000  165.259995  360.980011  175.029999   \n",
      "2022-10-31  82.489998   58.919998  166.250000  362.200012  176.270004   \n",
      "2022-11-01  80.839996   58.459999  166.059998  353.859985  174.729996   \n",
      "2022-11-02  80.970001   57.369999  163.300003  351.299988  176.720001   \n",
      "2022-11-03  79.790001   56.790001  162.619995  342.489990  174.639999   \n",
      "2022-11-04  79.190002   56.380001  161.289993  341.119995  173.699997   \n",
      "2022-11-07  77.209999   55.439999  160.460007  342.750000  173.380005   \n",
      "2022-11-08  78.370003   55.250000  160.710007  341.750000  172.529999   \n",
      "2022-11-09  78.949997   56.919998  162.800003  347.079987  177.289993   \n",
      "2022-11-10  77.169998   55.259998  158.449997  347.700012  176.169998   \n",
      "2022-11-11  75.150002   54.270000  154.779999  347.500000  173.699997   \n",
      "2022-11-14  78.690002   56.220001  159.839996  349.200012  179.300003   \n",
      "2022-11-15  81.669998   58.790001  165.149994  362.890015  183.460007   \n",
      "2022-11-16  81.949997   57.610001  167.800003  367.269989  183.869995   \n",
      "2022-11-17  82.010002   57.730000  166.539993  366.000000  182.080002   \n",
      "2022-11-18  77.330002   54.610001  159.279999  352.950012  177.860001   \n",
      "2022-11-19  76.110001   53.820000  156.789993  343.540009  176.990005   \n",
      "\n",
      "            WDC_lag30  ZBRA_lag30  ^tnx_lag30  ^GSPC_lag30  CL=F_lag30  \n",
      "2022-09-06        NaN         NaN         NaN          NaN         NaN  \n",
      "2022-09-07        NaN         NaN         NaN          NaN         NaN  \n",
      "2022-09-08        NaN         NaN         NaN          NaN         NaN  \n",
      "2022-09-09        NaN         NaN         NaN          NaN         NaN  \n",
      "2022-09-12        NaN         NaN         NaN          NaN         NaN  \n",
      "2022-09-13        NaN         NaN         NaN          NaN         NaN  \n",
      "2022-09-14        NaN         NaN         NaN          NaN         NaN  \n",
      "2022-09-15        NaN         NaN         NaN          NaN         NaN  \n",
      "2022-09-16        NaN         NaN         NaN          NaN         NaN  \n",
      "2022-09-19        NaN         NaN         NaN          NaN         NaN  \n",
      "2022-09-20        NaN         NaN         NaN          NaN         NaN  \n",
      "2022-09-21        NaN         NaN         NaN          NaN         NaN  \n",
      "2022-09-22        NaN         NaN         NaN          NaN         NaN  \n",
      "2022-09-23        NaN         NaN         NaN          NaN         NaN  \n",
      "2022-09-26        NaN         NaN         NaN          NaN         NaN  \n",
      "2022-09-27        NaN         NaN         NaN          NaN         NaN  \n",
      "2022-09-28        NaN         NaN         NaN          NaN         NaN  \n",
      "2022-09-29        NaN         NaN         NaN          NaN         NaN  \n",
      "2022-09-30        NaN         NaN         NaN          NaN         NaN  \n",
      "2022-10-03        NaN         NaN         NaN          NaN         NaN  \n",
      "2022-10-04        NaN         NaN         NaN          NaN         NaN  \n",
      "2022-10-05        NaN         NaN         NaN          NaN         NaN  \n",
      "2022-10-06        NaN         NaN         NaN          NaN         NaN  \n",
      "2022-10-07        NaN         NaN         NaN          NaN         NaN  \n",
      "2022-10-10        NaN         NaN         NaN          NaN         NaN  \n",
      "2022-10-11        NaN         NaN         NaN          NaN         NaN  \n",
      "2022-10-12        NaN         NaN         NaN          NaN         NaN  \n",
      "2022-10-13        NaN         NaN         NaN          NaN         NaN  \n",
      "2022-10-14        NaN         NaN         NaN          NaN         NaN  \n",
      "2022-10-17        NaN         NaN         NaN          NaN         NaN  \n",
      "2022-10-18  40.970001  289.929993       3.340  3908.189941   86.879997  \n",
      "2022-10-19  41.450001  295.579987       3.265  3979.870117   81.940002  \n",
      "2022-10-20  42.410000  298.299988       3.292  4006.179932   83.540001  \n",
      "2022-10-21  43.799999  307.839996       3.321  4067.360107   86.790001  \n",
      "2022-10-24  43.270000  311.850006       3.362  4110.410156   87.779999  \n",
      "2022-10-25  39.320000  293.500000       3.422  3932.689941   87.309998  \n",
      "2022-10-26  38.330002  295.970001       3.412  3946.010010   88.480003  \n",
      "2022-10-27  37.770000  296.200012       3.459  3901.350098   85.099998  \n",
      "2022-10-28  37.220001  288.519989       3.448  3873.330078   85.110001  \n",
      "2022-10-31  36.619999  291.209991       3.490  3899.889893   85.730003  \n",
      "2022-11-01  35.540001  285.649994       3.571  3855.929932   84.449997  \n",
      "2022-11-02  34.779999  285.079987       3.510  3789.929932   82.940002  \n",
      "2022-11-03  33.820000  272.940002       3.708  3757.989990   83.489998  \n",
      "2022-11-04  33.840000  268.040009       3.697  3693.229980   78.739998  \n",
      "2022-11-07  32.400002  265.859985       3.878  3655.040039   76.709999  \n",
      "2022-11-08  32.720001  264.950012       3.964  3647.290039   78.500000  \n",
      "2022-11-09  33.150002  271.350006       3.705  3719.040039   82.150002  \n",
      "2022-11-10  32.180000  265.519989       3.747  3640.469971   81.230003  \n",
      "2022-11-11  32.549999  262.010010       3.804  3585.620117   79.489998  \n",
      "2022-11-14  34.049999  272.089996       3.651  3678.429932   83.629997  \n",
      "2022-11-15  36.459999  281.649994       3.617  3790.929932   86.519997  \n",
      "2022-11-16  37.169998  281.880005       3.759  3783.280029   87.760002  \n",
      "2022-11-17  37.060001  280.350006       3.826  3744.520020   88.449997  \n",
      "2022-11-18  35.740002  266.679993       3.883  3639.659912   92.639999  \n",
      "2022-11-19  34.660000  258.600006       3.888  3612.389893   91.129997  \n",
      "\n",
      "[55 rows x 2220 columns]\n",
      "df_lagged of features+additional data after dropna and before sort in module create_predict_set\n",
      "(25, 2221)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_192499/3311901794.py:18: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df=df.append(df_last)  #we append last date to end\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_lagged of features+additional data after dropna in module create_predict_set\n",
      "(25, 2221)\n",
      "                  AAPL   AAPL_lag1  AAPL_lag10  AAPL_lag11  AAPL_lag12  \\\n",
      "2022-10-18  143.750000  142.410004  146.100006  142.449997  138.199997   \n",
      "2022-10-19  143.860001  143.750000  146.399994  146.100006  142.449997   \n",
      "2022-10-20  143.389999  143.860001  145.429993  146.399994  146.100006   \n",
      "2022-10-21  147.270004  143.389999  140.089996  145.429993  146.399994   \n",
      "2022-10-24  149.449997  147.270004  140.419998  140.089996  145.429993   \n",
      "2022-10-25  152.339996  149.449997  138.979996  140.419998  140.089996   \n",
      "2022-10-26  149.350006  152.339996  138.339996  138.979996  140.419998   \n",
      "2022-10-27  144.800003  149.350006  142.990005  138.339996  138.979996   \n",
      "2022-10-28  155.740005  144.800003  138.380005  142.990005  138.339996   \n",
      "2022-10-31  153.339996  155.740005  142.410004  138.380005  142.990005   \n",
      "2022-11-01  150.649994  153.339996  143.750000  142.410004  138.380005   \n",
      "2022-11-02  145.029999  150.649994  143.860001  143.750000  142.410004   \n",
      "2022-11-03  138.880005  145.029999  143.389999  143.860001  143.750000   \n",
      "2022-11-04  138.380005  138.880005  147.270004  143.389999  143.860001   \n",
      "2022-11-07  138.919998  138.380005  149.449997  147.270004  143.389999   \n",
      "2022-11-08  139.500000  138.919998  152.339996  149.449997  147.270004   \n",
      "2022-11-09  134.869995  139.500000  149.350006  152.339996  149.449997   \n",
      "2022-11-10  146.869995  134.869995  144.800003  149.350006  152.339996   \n",
      "2022-11-11  149.699997  146.869995  155.740005  144.800003  149.350006   \n",
      "2022-11-14  148.279999  149.699997  153.339996  155.740005  144.800003   \n",
      "2022-11-15  150.039993  148.279999  150.649994  153.339996  155.740005   \n",
      "2022-11-16  148.789993  150.039993  145.029999  150.649994  153.339996   \n",
      "2022-11-17  150.720001  148.789993  138.880005  145.029999  150.649994   \n",
      "2022-11-18  151.289993  150.720001  138.380005  138.880005  145.029999   \n",
      "2022-11-19  151.289993  151.289993  138.919998  138.380005  138.880005   \n",
      "\n",
      "            AAPL_lag13  AAPL_lag14  AAPL_lag15  AAPL_lag16  AAPL_lag17  ...  \\\n",
      "2022-10-18  142.479996  149.839996  151.759995  150.770004  150.429993  ...   \n",
      "2022-10-19  138.199997  142.479996  149.839996  151.759995  150.770004  ...   \n",
      "2022-10-20  142.449997  138.199997  142.479996  149.839996  151.759995  ...   \n",
      "2022-10-21  146.100006  142.449997  138.199997  142.479996  149.839996  ...   \n",
      "2022-10-24  146.399994  146.100006  142.449997  138.199997  142.479996  ...   \n",
      "2022-10-25  145.429993  146.399994  146.100006  142.449997  138.199997  ...   \n",
      "2022-10-26  140.089996  145.429993  146.399994  146.100006  142.449997  ...   \n",
      "2022-10-27  140.419998  140.089996  145.429993  146.399994  146.100006  ...   \n",
      "2022-10-28  138.979996  140.419998  140.089996  145.429993  146.399994  ...   \n",
      "2022-10-31  138.339996  138.979996  140.419998  140.089996  145.429993  ...   \n",
      "2022-11-01  142.990005  138.339996  138.979996  140.419998  140.089996  ...   \n",
      "2022-11-02  138.380005  142.990005  138.339996  138.979996  140.419998  ...   \n",
      "2022-11-03  142.410004  138.380005  142.990005  138.339996  138.979996  ...   \n",
      "2022-11-04  143.750000  142.410004  138.380005  142.990005  138.339996  ...   \n",
      "2022-11-07  143.860001  143.750000  142.410004  138.380005  142.990005  ...   \n",
      "2022-11-08  143.389999  143.860001  143.750000  142.410004  138.380005  ...   \n",
      "2022-11-09  147.270004  143.389999  143.860001  143.750000  142.410004  ...   \n",
      "2022-11-10  149.449997  147.270004  143.389999  143.860001  143.750000  ...   \n",
      "2022-11-11  152.339996  149.449997  147.270004  143.389999  143.860001  ...   \n",
      "2022-11-14  149.350006  152.339996  149.449997  147.270004  143.389999  ...   \n",
      "2022-11-15  144.800003  149.350006  152.339996  149.449997  147.270004  ...   \n",
      "2022-11-16  155.740005  144.800003  149.350006  152.339996  149.449997  ...   \n",
      "2022-11-17  153.339996  155.740005  144.800003  149.350006  152.339996  ...   \n",
      "2022-11-18  150.649994  153.339996  155.740005  144.800003  149.350006  ...   \n",
      "2022-11-19  145.029999  150.649994  153.339996  155.740005  144.800003  ...   \n",
      "\n",
      "            ^tnx_lag28  ^tnx_lag29  ^tnx_lag3  ^tnx_lag30  ^tnx_lag4  \\\n",
      "2022-10-18       3.292       3.265      3.952       3.340      3.902   \n",
      "2022-10-19       3.321       3.292      4.010       3.265      3.952   \n",
      "2022-10-20       3.362       3.321      4.015       3.292      4.010   \n",
      "2022-10-21       3.422       3.362      3.998       3.321      4.015   \n",
      "2022-10-24       3.412       3.422      4.127       3.362      3.998   \n",
      "2022-10-25       3.459       3.412      4.226       3.422      4.127   \n",
      "2022-10-26       3.448       3.459      4.213       3.412      4.226   \n",
      "2022-10-27       3.490       3.448      4.234       3.459      4.213   \n",
      "2022-10-28       3.571       3.490      4.108       3.448      4.234   \n",
      "2022-10-31       3.510       3.571      4.015       3.490      4.108   \n",
      "2022-11-01       3.708       3.510      3.937       3.571      4.015   \n",
      "2022-11-02       3.697       3.708      4.010       3.510      3.937   \n",
      "2022-11-03       3.878       3.697      4.077       3.708      4.010   \n",
      "2022-11-04       3.964       3.878      4.052       3.697      4.077   \n",
      "2022-11-07       3.705       3.964      4.059       3.878      4.052   \n",
      "2022-11-08       3.747       3.705      4.124       3.964      4.059   \n",
      "2022-11-09       3.804       3.747      4.156       3.705      4.124   \n",
      "2022-11-10       3.651       3.804      4.214       3.747      4.156   \n",
      "2022-11-11       3.617       3.651      4.126       3.804      4.214   \n",
      "2022-11-14       3.759       3.617      4.151       3.651      4.126   \n",
      "2022-11-15       3.826       3.759      3.829       3.617      4.151   \n",
      "2022-11-16       3.883       3.826      3.813       3.759      3.829   \n",
      "2022-11-17       3.888       3.883      3.865       3.826      3.813   \n",
      "2022-11-18       3.939       3.888      3.799       3.883      3.865   \n",
      "2022-11-19       3.902       3.939      3.692       3.888      3.799   \n",
      "\n",
      "            ^tnx_lag5  ^tnx_lag6  ^tnx_lag7  ^tnx_lag8  ^tnx_lag9  \n",
      "2022-10-18      3.939      3.888      3.883      3.826      3.759  \n",
      "2022-10-19      3.902      3.939      3.888      3.883      3.826  \n",
      "2022-10-20      3.952      3.902      3.939      3.888      3.883  \n",
      "2022-10-21      4.010      3.952      3.902      3.939      3.888  \n",
      "2022-10-24      4.015      4.010      3.952      3.902      3.939  \n",
      "2022-10-25      3.998      4.015      4.010      3.952      3.902  \n",
      "2022-10-26      4.127      3.998      4.015      4.010      3.952  \n",
      "2022-10-27      4.226      4.127      3.998      4.015      4.010  \n",
      "2022-10-28      4.213      4.226      4.127      3.998      4.015  \n",
      "2022-10-31      4.234      4.213      4.226      4.127      3.998  \n",
      "2022-11-01      4.108      4.234      4.213      4.226      4.127  \n",
      "2022-11-02      4.015      4.108      4.234      4.213      4.226  \n",
      "2022-11-03      3.937      4.015      4.108      4.234      4.213  \n",
      "2022-11-04      4.010      3.937      4.015      4.108      4.234  \n",
      "2022-11-07      4.077      4.010      3.937      4.015      4.108  \n",
      "2022-11-08      4.052      4.077      4.010      3.937      4.015  \n",
      "2022-11-09      4.059      4.052      4.077      4.010      3.937  \n",
      "2022-11-10      4.124      4.059      4.052      4.077      4.010  \n",
      "2022-11-11      4.156      4.124      4.059      4.052      4.077  \n",
      "2022-11-14      4.214      4.156      4.124      4.059      4.052  \n",
      "2022-11-15      4.126      4.214      4.156      4.124      4.059  \n",
      "2022-11-16      4.151      4.126      4.214      4.156      4.124  \n",
      "2022-11-17      3.829      4.151      4.126      4.214      4.156  \n",
      "2022-11-18      3.813      3.829      4.151      4.126      4.214  \n",
      "2022-11-19      3.865      3.813      3.829      4.151      4.126  \n",
      "\n",
      "[25 rows x 2221 columns]\n",
      "df_filtered which is only start to last date of predict  in module create_predict_set\n",
      "(12, 2221)\n",
      "X_test shape in module predict_ticker before calling model_predict and after creating predict_set (12, 2220)\n",
      "X test shape in module model_predict (12, 2220)\n",
      "module predictions compile\n",
      "y_test_shape in module predictions compile (12,)\n",
      "y_pred_shape in module predictions compile (12,)\n",
      "y_test in module predictions compile [138.38000488 138.91999817 139.5        134.86999512 146.86999512\n",
      " 149.69999695 148.27999878 150.03999329 148.78999329 150.72000122\n",
      " 151.28999329 151.28999329]\n",
      "y_pred in module predictions compile [139.42974625 138.83536893 139.56694879 139.95962195 136.54845913\n",
      " 146.49632198 149.39189803 148.08886487 149.68703976 149.20512087\n",
      " 150.21409064 152.1143222 ]\n",
      "ticker in module predictions compile AAPL\n",
      "predict_df in module predictions compile         y_test      y_pred ticker\n",
      "0   138.380005  139.429746   AAPL\n",
      "1   138.919998  138.835369   AAPL\n",
      "2   139.500000  139.566949   AAPL\n",
      "3   134.869995  139.959622   AAPL\n",
      "4   146.869995  136.548459   AAPL\n",
      "5   149.699997  146.496322   AAPL\n",
      "6   148.279999  149.391898   AAPL\n",
      "7   150.039993  148.088865   AAPL\n",
      "8   148.789993  149.687040   AAPL\n",
      "9   150.720001  149.205121   AAPL\n",
      "10  151.289993  150.214091   AAPL\n",
      "11  151.289993  152.114322   AAPL\n",
      "temp_pred shape in module predict_ticker after calling model_predict (12, 3)\n",
      "        y_test      y_pred ticker\n",
      "0   138.380005  139.429746   AAPL\n",
      "1   138.919998  138.835369   AAPL\n",
      "2   139.500000  139.566949   AAPL\n",
      "3   134.869995  139.959622   AAPL\n",
      "4   146.869995  136.548459   AAPL\n",
      "5   149.699997  146.496322   AAPL\n",
      "6   148.279999  149.391898   AAPL\n",
      "7   150.039993  148.088865   AAPL\n",
      "8   148.789993  149.687040   AAPL\n",
      "9   150.720001  149.205121   AAPL\n",
      "10  151.289993  150.214091   AAPL\n",
      "11  151.289993  152.114322   AAPL\n",
      "  Ticker Predicted for Predicted Recommended  Accuracy\n",
      "0   AAPL                                      0.583333\n",
      "avg return [0, 0.63]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_192499/1685750918.py:113: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  Predictions = Predictions.append(temp_pred2, ignore_index=True)  # this is to store in the master pandas list\n",
      "/tmp/ipykernel_192499/832641991.py:29: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df_lagged = df_lagged.append(df_lagged_ticker)\n"
     ]
    }
   ],
   "source": [
    "ticker='aapl'\n",
    "results,recommendations,avg_return=check_ticker(ticker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Ticker       Date    Observed Predicted for   Predicted Recommended  \\\n",
      "1    AAPL 2022-11-08  139.500000    2022-11-08  139.566949         Buy   \n",
      "2    AAPL 2022-11-09  134.869995    2022-11-09  139.959622         Buy   \n",
      "3    AAPL 2022-11-10  146.869995    2022-11-10  136.548459         Buy   \n",
      "4    AAPL 2022-11-11  149.699997    2022-11-11  146.496322        Sell   \n",
      "5    AAPL 2022-11-14  148.279999    2022-11-14  149.391898        Sell   \n",
      "6    AAPL 2022-11-15  150.039993    2022-11-15  148.088865        Sell   \n",
      "7    AAPL 2022-11-16  148.789993    2022-11-16  149.687040        Sell   \n",
      "8    AAPL 2022-11-17  150.720001    2022-11-17  149.205121         Buy   \n",
      "9    AAPL 2022-11-18  151.289993    2022-11-18  150.214091        Sell   \n",
      "10   AAPL 2022-11-19         NaN    2022-11-19  152.114322         Buy   \n",
      "\n",
      "    Daily return %  \n",
      "1         0.417508  \n",
      "2        -3.319000  \n",
      "3         8.897457  \n",
      "4        -1.890449  \n",
      "5         0.957646  \n",
      "6        -1.173017  \n",
      "7         0.840110  \n",
      "8         1.297136  \n",
      "9        -0.376755  \n",
      "10             NaN  \n"
     ]
    }
   ],
   "source": [
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><div id=bf8f3a36-9f9c-4f51-8846-74ff8db44e9d style=\"display:none; background-color:#9D6CFF; color:white; width:200px; height:30px; padding-left:5px; border-radius:4px; flex-direction:row; justify-content:space-around; align-items:center;\" onmouseover=\"this.style.backgroundColor='#BA9BF8'\" onmouseout=\"this.style.backgroundColor='#9D6CFF'\" onclick=\"window.commands?.execute('create-mitosheet-from-dataframe-output');\">See Full Dataframe in Mito</div> <script> if (window.commands?.hasCommand('create-mitosheet-from-dataframe-output')) document.getElementById('bf8f3a36-9f9c-4f51-8846-74ff8db44e9d').style.display = 'flex' </script> <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Ticker</th>\n",
       "      <th>Predicted for</th>\n",
       "      <th>Predicted</th>\n",
       "      <th>Recommended</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>2022/11/19</td>\n",
       "      <td>152.114322</td>\n",
       "      <td>Buy</td>\n",
       "      <td>0.583333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table></div>"
      ],
      "text/plain": [
       "  Ticker Predicted for   Predicted Recommended  Accuracy\n",
       "0   AAPL    2022/11/19  152.114322         Buy  0.583333"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, '2022/11/08', '2022/11/18']"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "date_range=[0]\n",
    "date_range.append(results.iloc[0]['Date'].strftime(\"%Y/%m/%d\"))\n",
    "date_range.append(results.iloc[-2]['Date'].strftime(\"%Y/%m/%d\"))\n",
    "date_range"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## New section to see enhanced date and preprocessed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "ticker='GOOG'\n",
    "sector=get_ticker_sector(ticker)\n",
    "additional_data=read_config_file()[1]\n",
    "additional_data=generate_enhanced_data(sector,ticker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Communication Services'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['ATVI', 'Communication Services', 'Electronic Gaming & Multimedia'],\n",
       " ['CHTR', 'Communication Services', 'Telecom Services'],\n",
       " ['CMCSA', 'Communication Services', 'Telecom Services'],\n",
       " ['DIS', 'Communication Services', 'Entertainment'],\n",
       " ['DISH', 'Communication Services', 'Telecom Services'],\n",
       " ['EA', 'Communication Services', 'Electronic Gaming & Multimedia'],\n",
       " ['FOX', 'Communication Services', 'Entertainment'],\n",
       " ['FOXA', 'Communication Services', 'Entertainment'],\n",
       " ['GOOG', 'Communication Services', 'Internet Content & Information'],\n",
       " ['GOOGL', 'Communication Services', 'Internet Content & Information'],\n",
       " ['IPG', 'Communication Services', 'Advertising Agencies'],\n",
       " ['LUMN', 'Communication Services', 'Telecom Services'],\n",
       " ['LYV', 'Communication Services', 'Entertainment'],\n",
       " ['META', 'Communication Services', 'Internet Content & Information'],\n",
       " ['MTCH', 'Communication Services', 'Internet Content & Information'],\n",
       " ['NFLX', 'Communication Services', 'Entertainment'],\n",
       " ['NWS', 'Communication Services', 'Entertainment'],\n",
       " ['NWSA', 'Communication Services', 'Entertainment'],\n",
       " ['OMC', 'Communication Services', 'Advertising Agencies'],\n",
       " ['PARA', 'Communication Services', 'Entertainment'],\n",
       " ['T', 'Communication Services', 'Telecom Services'],\n",
       " ['TMUS', 'Communication Services', 'Telecom Services'],\n",
       " ['TTWO', 'Communication Services', 'Electronic Gaming & Multimedia'],\n",
       " ['TWTR', 'Communication Services', 'Internet Content & Information'],\n",
       " ['VZ', 'Communication Services', 'Telecom Services'],\n",
       " ['WBD', 'Communication Services', 'Entertainment']]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SP500_list = read_list(\"SP500.json\")\n",
    "sub_list_tickers = np.array(SP500_list)\n",
    "fltr = np.asarray(sector)\n",
    "sector = sub_list_tickers[np.in1d(sub_list_tickers[:, 1], fltr)].tolist()\n",
    "sector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "ticker='CDW'\n",
    "sector=get_ticker_sector(ticker)\n",
    "additional_data=read_config_file()[1]\n",
    "additional_data=generate_enhanced_data(sector,ticker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Technology'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['AAPL', 'Technology', 'Consumer Electronics'],\n",
       " ['ACN', 'Technology', 'Information Technology Services'],\n",
       " ['ADBE', 'Technology', 'SoftwareInfrastructure'],\n",
       " ['ADI', 'Technology', 'Semiconductors'],\n",
       " ['ADSK', 'Technology', 'SoftwareApplication'],\n",
       " ['AKAM', 'Technology', 'SoftwareInfrastructure'],\n",
       " ['AMAT', 'Technology', 'Semiconductor Equipment & Materials'],\n",
       " ['AMD', 'Technology', 'Semiconductors'],\n",
       " ['ANET', 'Technology', 'Computer Hardware'],\n",
       " ['ANSS', 'Technology', 'SoftwareApplication'],\n",
       " ['APH', 'Technology', 'Electronic Components'],\n",
       " ['AVGO', 'Technology', 'Semiconductors'],\n",
       " ['BR', 'Technology', 'Information Technology Services'],\n",
       " ['CDAY', 'Technology', 'SoftwareApplication'],\n",
       " ['CDNS', 'Technology', 'SoftwareApplication'],\n",
       " ['CDW', 'Technology', 'Information Technology Services'],\n",
       " ['CRM', 'Technology', 'SoftwareApplication'],\n",
       " ['CSCO', 'Technology', 'Communication Equipment'],\n",
       " ['CTSH', 'Technology', 'Information Technology Services'],\n",
       " ['DXC', 'Technology', 'Information Technology Services'],\n",
       " ['ENPH', 'Technology', 'Solar'],\n",
       " ['EPAM', 'Technology', 'Information Technology Services'],\n",
       " ['FFIV', 'Technology', 'SoftwareInfrastructure'],\n",
       " ['FIS', 'Technology', 'Information Technology Services'],\n",
       " ['FISV', 'Technology', 'Information Technology Services'],\n",
       " ['FLT', 'Technology', 'SoftwareInfrastructure'],\n",
       " ['FTNT', 'Technology', 'SoftwareInfrastructure'],\n",
       " ['FTV', 'Technology', 'Scientific & Technical Instruments'],\n",
       " ['GLW', 'Technology', 'Electronic Components'],\n",
       " ['GRMN', 'Technology', 'Scientific & Technical Instruments'],\n",
       " ['HPE', 'Technology', 'Communication Equipment'],\n",
       " ['HPQ', 'Technology', 'Computer Hardware'],\n",
       " ['IBM', 'Technology', 'Information Technology Services'],\n",
       " ['INTC', 'Technology', 'Semiconductors'],\n",
       " ['INTU', 'Technology', 'SoftwareApplication'],\n",
       " ['IT', 'Technology', 'Information Technology Services'],\n",
       " ['JKHY', 'Technology', 'Information Technology Services'],\n",
       " ['JNPR', 'Technology', 'Communication Equipment'],\n",
       " ['KEYS', 'Technology', 'Scientific & Technical Instruments'],\n",
       " ['KLAC', 'Technology', 'Semiconductor Equipment & Materials'],\n",
       " ['LDOS', 'Technology', 'Information Technology Services'],\n",
       " ['LRCX', 'Technology', 'Semiconductor Equipment & Materials'],\n",
       " ['MCHP', 'Technology', 'Semiconductors'],\n",
       " ['MPWR', 'Technology', 'Semiconductors'],\n",
       " ['MSFT', 'Technology', 'SoftwareInfrastructure'],\n",
       " ['MSI', 'Technology', 'Communication Equipment'],\n",
       " ['MU', 'Technology', 'Semiconductors'],\n",
       " ['NLOK', 'Technology', 'SoftwareInfrastructure'],\n",
       " ['NOW', 'Technology', 'SoftwareApplication'],\n",
       " ['NTAP', 'Technology', 'Computer Hardware'],\n",
       " ['NVDA', 'Technology', 'Semiconductors'],\n",
       " ['NXPI', 'Technology', 'Semiconductors'],\n",
       " ['ON', 'Technology', 'Semiconductors'],\n",
       " ['ORCL', 'Technology', 'SoftwareInfrastructure'],\n",
       " ['PAYC', 'Technology', 'SoftwareApplication'],\n",
       " ['PTC', 'Technology', 'SoftwareApplication'],\n",
       " ['QCOM', 'Technology', 'Semiconductors'],\n",
       " ['QRVO', 'Technology', 'Semiconductors'],\n",
       " ['SEDG', 'Technology', 'Solar'],\n",
       " ['SNPS', 'Technology', 'SoftwareInfrastructure'],\n",
       " ['STX', 'Technology', 'Computer Hardware'],\n",
       " ['SWKS', 'Technology', 'Semiconductors'],\n",
       " ['TDY', 'Technology', 'Scientific & Technical Instruments'],\n",
       " ['TEL', 'Technology', 'Electronic Components'],\n",
       " ['TER', 'Technology', 'Semiconductor Equipment & Materials'],\n",
       " ['TRMB', 'Technology', 'Scientific & Technical Instruments'],\n",
       " ['TXN', 'Technology', 'Semiconductors'],\n",
       " ['TYL', 'Technology', 'SoftwareApplication'],\n",
       " ['VRSN', 'Technology', 'SoftwareInfrastructure'],\n",
       " ['WDC', 'Technology', 'Computer Hardware'],\n",
       " ['ZBRA', 'Technology', 'Communication Equipment']]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SP500_list = read_list(\"SP500.json\")\n",
    "sub_list_tickers = np.array(SP500_list)\n",
    "fltr = np.asarray(sector)\n",
    "sector = sub_list_tickers[np.in1d(sub_list_tickers[:, 1], fltr)].tolist()\n",
    "sector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MODEL==\"Ridge(alpha=1.0)\":\n",
    "from sklearn.linear_model import Ridge\n",
    "MODEL=Ridge(alpha=1.0)\n",
    "        \n",
    "#MODEL==\"Lasso(alpha=1.0)\":\n",
    "from sklearn.linear_model import Lasso\n",
    "MODEL=Lasso(alpha=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv(\"df_lagged18nov.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df[['AAPL','AKAM']]\n",
    "df_ticker=df[['AAPL']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "lags=3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lagged = pd.DataFrame()\n",
    "for window in range(1, lags + 1):\n",
    "        shifted = df.shift(window)\n",
    "        shifted.columns = [x + \"_lag\" + str(window) for x in df.columns]\n",
    "\n",
    "        df_lagged = pd.concat((df_lagged, shifted), axis=1)\n",
    "#df_lagged = df_lagged.fillna(method='ffill')\n",
    "df_lagged= df_lagged.interpolate(method='linear')\n",
    "df_lagged=pd.concat((df_ticker,df_lagged),axis=1)\n",
    "df_lagged = df_lagged.dropna()\n",
    "df_lagged = df_lagged.reindex(sorted(df_lagged.columns), axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><div id=e889e75d-2c3d-422f-a0c3-de7704ee6aa3 style=\"display:none; background-color:#9D6CFF; color:white; width:200px; height:30px; padding-left:5px; border-radius:4px; flex-direction:row; justify-content:space-around; align-items:center;\" onmouseover=\"this.style.backgroundColor='#BA9BF8'\" onmouseout=\"this.style.backgroundColor='#9D6CFF'\" onclick=\"window.commands?.execute('create-mitosheet-from-dataframe-output');\">See Full Dataframe in Mito</div> <script> if (window.commands?.hasCommand('create-mitosheet-from-dataframe-output')) document.getElementById('e889e75d-2c3d-422f-a0c3-de7704ee6aa3').style.display = 'flex' </script> <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AAPL</th>\n",
       "      <th>AAPL_lag1</th>\n",
       "      <th>AAPL_lag2</th>\n",
       "      <th>AAPL_lag3</th>\n",
       "      <th>AKAM_lag1</th>\n",
       "      <th>AKAM_lag2</th>\n",
       "      <th>AKAM_lag3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>147.270004</td>\n",
       "      <td>143.389999</td>\n",
       "      <td>143.860001</td>\n",
       "      <td>143.750000</td>\n",
       "      <td>84.160004</td>\n",
       "      <td>82.940002</td>\n",
       "      <td>84.379997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>149.449997</td>\n",
       "      <td>147.270004</td>\n",
       "      <td>143.389999</td>\n",
       "      <td>143.860001</td>\n",
       "      <td>85.879997</td>\n",
       "      <td>84.160004</td>\n",
       "      <td>82.940002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>152.339996</td>\n",
       "      <td>149.449997</td>\n",
       "      <td>147.270004</td>\n",
       "      <td>143.389999</td>\n",
       "      <td>86.389999</td>\n",
       "      <td>85.879997</td>\n",
       "      <td>84.160004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>149.350006</td>\n",
       "      <td>152.339996</td>\n",
       "      <td>149.449997</td>\n",
       "      <td>147.270004</td>\n",
       "      <td>88.449997</td>\n",
       "      <td>86.389999</td>\n",
       "      <td>85.879997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>144.800003</td>\n",
       "      <td>149.350006</td>\n",
       "      <td>152.339996</td>\n",
       "      <td>149.449997</td>\n",
       "      <td>86.760002</td>\n",
       "      <td>88.449997</td>\n",
       "      <td>86.389999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>148.279999</td>\n",
       "      <td>149.699997</td>\n",
       "      <td>146.869995</td>\n",
       "      <td>134.869995</td>\n",
       "      <td>93.349998</td>\n",
       "      <td>92.919998</td>\n",
       "      <td>89.080002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>150.039993</td>\n",
       "      <td>148.279999</td>\n",
       "      <td>149.699997</td>\n",
       "      <td>146.869995</td>\n",
       "      <td>91.529999</td>\n",
       "      <td>93.349998</td>\n",
       "      <td>92.919998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>148.789993</td>\n",
       "      <td>150.039993</td>\n",
       "      <td>148.279999</td>\n",
       "      <td>149.699997</td>\n",
       "      <td>92.459999</td>\n",
       "      <td>91.529999</td>\n",
       "      <td>93.349998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>150.720001</td>\n",
       "      <td>148.789993</td>\n",
       "      <td>150.039993</td>\n",
       "      <td>148.279999</td>\n",
       "      <td>90.820000</td>\n",
       "      <td>92.459999</td>\n",
       "      <td>91.529999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>150.720001</td>\n",
       "      <td>150.720001</td>\n",
       "      <td>148.789993</td>\n",
       "      <td>150.039993</td>\n",
       "      <td>90.790001</td>\n",
       "      <td>90.820000</td>\n",
       "      <td>92.459999</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table></div>"
      ],
      "text/plain": [
       "          AAPL   AAPL_lag1   AAPL_lag2   AAPL_lag3  AKAM_lag1  AKAM_lag2  \\\n",
       "3   147.270004  143.389999  143.860001  143.750000  84.160004  82.940002   \n",
       "4   149.449997  147.270004  143.389999  143.860001  85.879997  84.160004   \n",
       "5   152.339996  149.449997  147.270004  143.389999  86.389999  85.879997   \n",
       "6   149.350006  152.339996  149.449997  147.270004  88.449997  86.389999   \n",
       "7   144.800003  149.350006  152.339996  149.449997  86.760002  88.449997   \n",
       "8   155.740005  144.800003  149.350006  152.339996  87.580002  86.760002   \n",
       "9   153.339996  155.740005  144.800003  149.350006  89.209999  87.580002   \n",
       "10  150.649994  153.339996  155.740005  144.800003  88.330002  89.209999   \n",
       "11  145.029999  150.649994  153.339996  155.740005  88.099998  88.330002   \n",
       "12  138.880005  145.029999  150.649994  153.339996  85.410004  88.099998   \n",
       "13  138.380005  138.880005  145.029999  150.649994  83.360001  85.410004   \n",
       "14  138.919998  138.380005  138.880005  145.029999  84.099998  83.360001   \n",
       "15  139.500000  138.919998  138.380005  138.880005  83.860001  84.099998   \n",
       "16  134.869995  139.500000  138.919998  138.380005  83.889999  83.860001   \n",
       "17  146.869995  134.869995  139.500000  138.919998  89.080002  83.889999   \n",
       "18  149.699997  146.869995  134.869995  139.500000  92.919998  89.080002   \n",
       "19  148.279999  149.699997  146.869995  134.869995  93.349998  92.919998   \n",
       "20  150.039993  148.279999  149.699997  146.869995  91.529999  93.349998   \n",
       "21  148.789993  150.039993  148.279999  149.699997  92.459999  91.529999   \n",
       "22  150.720001  148.789993  150.039993  148.279999  90.820000  92.459999   \n",
       "23  150.720001  150.720001  148.789993  150.039993  90.790001  90.820000   \n",
       "\n",
       "    AKAM_lag3  \n",
       "3   84.379997  \n",
       "4   82.940002  \n",
       "5   84.160004  \n",
       "6   85.879997  \n",
       "7   86.389999  \n",
       "8   88.449997  \n",
       "9   86.760002  \n",
       "10  87.580002  \n",
       "11  89.209999  \n",
       "12  88.330002  \n",
       "13  88.099998  \n",
       "14  85.410004  \n",
       "15  83.360001  \n",
       "16  84.099998  \n",
       "17  83.860001  \n",
       "18  83.889999  \n",
       "19  89.080002  \n",
       "20  92.919998  \n",
       "21  93.349998  \n",
       "22  91.529999  \n",
       "23  92.459999  "
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_lagged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
